{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__author__ = \"Theodora Chu, Josh Cohen, Jason Chen\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: stemming, take out stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info for creating VSM data\n",
    "vsmdata_home = \"vsmdata\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "equivalence_set = ['african american', 'african-american', 'black', 'african', 'nigger', 'nigga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_set2 = ['asian-american', 'african-american', 'black', 'african', 'asian', 'jewish', 'latino', 'mexican', 'russian', 'american']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input\n",
    "Takes in a text file and returns a list of ordered unigrams U. \n",
    "It should also consider stemming and other relevant pre-processing. Josh's note: parse \"African American\" as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseTextFile(filename):\n",
    "    text = open('cor-por-a/' + filename, 'r')\n",
    "    for i in range(0, 10):\n",
    "        print text.readline()\n",
    "    text_parse = text.read().split()\n",
    "    #print text_parse\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "#     print lancaster.stem('maximum') \n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return text_parse\n",
    "#     print porter.stem('maximum')    \n",
    "\n",
    "#parseTextFile('TomSawyer.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_seedword(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):                \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    return overallCorpus\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                #print article_file\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    overallCorpus += ' ';\n",
    "                    overallCorpus += article_text;\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i);\n",
    "    print 'hey!'\n",
    "    return overallCorpus;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_worddoc(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    file_list = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):      \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    return (overallCorpus, file_list)\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                file_list.append(filename)\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    for word in article_text:\n",
    "                        overallCorpus.append((word, filename))\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i)\n",
    "    return (overallCorpus, file_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "1. Parse U to create a word-word frequency matrix M, where each row represents a word and each entry x(i,j) represents the number of times word i co-occurs with word j.\n",
    "2. Convert M to a new matrix M’ with some sort of correlation operation. We could use PMI, Occai (see Josh’s paper), CSA, or some other correlation structure.\n",
    "3. Let row a represent the unigram “African American”. Take in that row, and output an ordered list of (this_unigram, correlation_score) pairs which represent the correlation score of this_unigram with the term “African American”\n",
    "4. Produce a list L of the top 100 correlated words with the term “African American”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns matrix object where mat_obj[0] refers to the seed_word matrix, where mat[1] refers\n",
    "# to the vocab list, where mat[2] refers to a frequency list,\n",
    "# where mat_obj[0][0] refers to the vector representing co-occurrence for first word in seed\n",
    "# set, and where mat_obj[0][len(seed_set)] refers to a vector of overall counts for each term \n",
    "def createSeedWordMatrixNYT(num_files):\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parse_NYT_articles_seedword(num_files)];\n",
    "    num_terms = len(u_vec);\n",
    "    print 'num terms in corpus: ' + str(num_terms);\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    print 'vocab size: ' + str(vocab_size);\n",
    "    print 'matrix dimensions: ' + str(len(seed_set2)) + ' x ' + str(len(vocab_vec));\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(len(seed_set2)+1)]\n",
    "    frequency_vec = [0 for x in range(vocab_size)]\n",
    "\n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i;\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    term = u_vec[0];\n",
    "    term_neighbor_r = u_vec[1];\n",
    "    \n",
    "    # CHANGED\n",
    "    #index_term = vocab_vec.index(term)\n",
    "    index_term = index_dict[term];\n",
    "    \n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_r = vocab_vec.index(term_neighbor_r);\n",
    "            index_neighbor_r = index_dict[term_neighbor_r];\n",
    "            \n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    for i in range(1, len(u_vec)-1):\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(num_terms) + ' terms'\n",
    "        term = u_vec[i];\n",
    "        term_neighbor_l = u_vec[i-1];\n",
    "        term_neighbor_r = u_vec[i+1];\n",
    "        \n",
    "        # CHANGED\n",
    "        #index_term = vocab_vec.index(term)\n",
    "        index_term = index_dict[term]\n",
    "        \n",
    "        frequency_vec[index_term] += 1;\n",
    "        if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_l = vocab_vec.index(term_neighbor_l);\n",
    "            #index_neighbor_r = vocab_vec.index(term_neighbor_r);\n",
    "            index_neighbor_l = index_dict[term_neighbor_l]\n",
    "            index_neighbor_r = index_dict[term_neighbor_r]\n",
    "            \n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    term = u_vec[len(u_vec)-1];\n",
    "    term_neighbor_l = u_vec[len(u_vec)-2];\n",
    "    \n",
    "    # CHANGED\n",
    "    #index_term = vocab_vec.index(term)\n",
    "    index_term = index_dict[term]\n",
    "    \n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_l = vocab_vec.index(term_neighbor_l);\n",
    "            index_neighbor_l = index_dict[term];\n",
    "            \n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "            \n",
    "    \n",
    "    \n",
    "    #   Filter the matrix by removing all words with frequency less than cutoff_freq\n",
    "    mat = np.transpose(mat)\n",
    "    indicies = []\n",
    "    cutoff_freq = 5\n",
    "    for i in range(0, len(frequency_vec)):\n",
    "        if frequency_vec[i] > cutoff_freq:\n",
    "            indicies.append(i) #keep track of indices of all words with frequency < cutoff_freq\n",
    "    print 'Parsed ' + str(num_terms) + '/' + str(num_terms) + ' terms';\n",
    "    #update mat, freq_vec, and vocab_vec to include only the indices we saved\n",
    "    mat = mat[np.array(indicies)]\n",
    "    frequency_vec = np.array(frequency_vec)\n",
    "    frequency_vec = frequency_vec[np.array(indicies)]\n",
    "    temp_vocab = []\n",
    "    for index in indicies:\n",
    "        temp_vocab.append(vocab_vec[index])\n",
    "    vocab_vec = temp_vocab\n",
    "    #transpose mat back to original shape\n",
    "    mat = np.transpose(mat)\n",
    "    print(\"New vocab size: \" + str(len(mat[0])))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (mat, vocab_vec, frequency_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a word, correlation list tuple for each seed in seed_set2\n",
    "from nltk.corpus import stopwords\n",
    "def getCorrelationLists(mat_obj, rem_stopwords=1):\n",
    "    tupleArr = []\n",
    "    # Word lists for each word\n",
    "    for j in range(len(seed_set2)):\n",
    "        w = mat_obj[0][j]\n",
    "        unfiltered_dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "        \n",
    "        # Without stop words\n",
    "        dists = [];\n",
    "        if rem_stopwords:\n",
    "            dists = [(word, frequency) for (word, frequency) in unfiltered_dists if word not in stopwords.words('english')]\n",
    "        else:\n",
    "            dists = unfiltered_dists\n",
    "        sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "        print \"PMI list for word: \" + seed_set2[j] + \"; \" + str(sorted_dists[:20])\n",
    "        tupleArr.append((seed_set2[j], sorted_dists));\n",
    "    \n",
    "    \n",
    "\n",
    "    #frequency list for each word:\n",
    "    w = mat_obj[2]\n",
    "    dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    print \"Frequency list: \" + str(sorted_dists[:20])\n",
    "    tupleArr.append((seed_set2[j], sorted_dists));\n",
    "    \n",
    "    return tupleArr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This takes fucking forever\n",
    "def createMatrix(): ###DEPRECATED###\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parseTextFile('TomSawyer.txt')];\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(vocab_size)]\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    for i in range(0, len(u_vec)-1):\n",
    "        term_one = u_vec[i];\n",
    "        term_two = u_vec[i+1];\n",
    "        index_one = vocab_vec.index(term_one)\n",
    "        index_two = vocab_vec.index(term_two)\n",
    "        mat[index_one][index_one] += 1;\n",
    "        mat[index_one][index_two] += 1;\n",
    "        mat[index_two][index_one] += 1;\n",
    "\n",
    "    last_term = u_vec[len(u_vec)-1]\n",
    "    last_term_index = vocab_vec.index(last_term)\n",
    "    mat[last_term_index][last_term_index] += 1\n",
    "    return (mat, vocab_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbors(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# PMI log: p(x, y)/ p(x)p(y)\n",
    "def pmi_seed(mat_obj, rownames=None, positive=True):  \n",
    "    rownames = mat_obj[1];\n",
    "    frequencies = mat_obj[2];\n",
    "    word_count = np.sum(frequencies, axis=None)\n",
    "    \n",
    "    # Joint probability table:\n",
    "    p = mat_obj[0] / word_count;\n",
    "    colprobs = frequencies/word_count;\n",
    "    sum_of_colprobs = np.sum(colprobs)\n",
    "    \n",
    "    \n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))    \n",
    "    mat_ppmi = [];\n",
    "    for row in p:\n",
    "        if np.sum(row) > 0:\n",
    "            mat_ppmi.append(np_pmi_log(row / (np.sum(row)*colprobs)));\n",
    "        else:\n",
    "            mat_ppmi.append([0 for x in row])\n",
    "    return (mat_ppmi, rownames, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def pmi(mat, rownames=None, positive=True):  \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    val = 0.0\n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val,0.0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlateds(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], w[i]) for i in range(len(mat))]\n",
    "    #print dists\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    # print sorted_dists\n",
    "    return sorted_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correlation list returns an ordered list of (word, correlation_score) tuples, where higher correlation_score\n",
    "# means the word is more correlated. The correlation list includes all words in the vocabulary, so you can\n",
    "# selectively take the first n elements if you want to use them.\n",
    "def correlationList(mat_ppmi):\n",
    "    return correlateds(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools to save result of mat calculations (deprecated) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.exists('my_file'): numpy.savetxt('my_file', my_array)\n",
    "\n",
    "#this will save the result of our matrix into a human-readable text file, and the original array is easily\n",
    "#recreated using loadtxt.\n",
    "\n",
    "# np.savetxt(\"mat_features\", mat[0])\n",
    "# np.savetxt(\"mat_labels\", mat[1])\n",
    "# np.loadtxt(\"mat_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### tool to remove punctuation from a text ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string  With  Punctuation \n"
     ]
    }
   ],
   "source": [
    "s = \"string. With. Punctuation?\" # Sample string\n",
    "def remove_punctuation(text):\n",
    "    for c in string.punctuation:\n",
    "        if c != '-': #excluding - because we want to preserve african-american as a token\n",
    "            text = text.replace(c,\" \")\n",
    "    return text\n",
    "\n",
    "print(remove_punctuation(s))\n",
    "\n",
    "\n",
    "#period, question mark, exclamation point, comma, semicolon, colon, dash, \n",
    "#hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools to parse out text from ntif/xml document for NYT articles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2007',\n",
       " 'A doctor who works at a clinic in Jamaica has been charged with insurance fraud, accused of billing insurance companies for tests that were never performed on victims of motor vehicle accidents, prosecutors said yesterday. The doctor, Alexander Israeli, 53, of Middle Village, was arraigned in Criminal Court on Monday night on charges of grand larceny and insurance fraud, said Richard A. Brown, the Queens district attorney. Mr. Brown said that Dr. Israeli billed insurance companies last year for $21,000 worth of neurological tests that were not performed. He faces loss of his medical license and up to seven years in prison if convicted, prosecutors said.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://docs.python-guide.org/en/latest/scenarios/xml/\n",
    "# http://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_NYT_article(xmlFile): \n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    year = ''\n",
    "    article_text = '';\n",
    "    for child in root:\n",
    "        if child.tag == 'head':\n",
    "            for subchild in child:\n",
    "                if 'name' in subchild.attrib:\n",
    "                    if subchild.attrib['name'] == 'publication_year':\n",
    "                        year = subchild.attrib['content']\n",
    "        if child.tag == 'body':\n",
    "            body = child\n",
    "    for child in body:\n",
    "        if child.tag == 'body.content':\n",
    "            content = child\n",
    "    for child in content:\n",
    "        if child.attrib == {'class': 'full_text'}:\n",
    "            for paragraph in child:\n",
    "                article_text += paragraph.text\n",
    "            return (year, article_text)\n",
    "                \n",
    "parse_NYT_article('nyt_sample_2.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mat_ppmi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ac0d676286da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneighbors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'colored'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat_ppmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat_ppmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mneighbors_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mat_ppmi' is not defined"
     ]
    }
   ],
   "source": [
    "###DEPRECATED###\n",
    "# neighbors_list = neighbors(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)[: 50]\n",
    "# print neighbors_list\n",
    "\n",
    "# def retrieve_words(tuple_list):\n",
    "#     words = list()\n",
    "#     for _tuple in tuple_list:\n",
    "#         words.append(_tuple[0])\n",
    "#     return words\n",
    "\n",
    "# neighbors_word_list = retrieve_words(neighbors_list)\n",
    "# print neighbors_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Correlation Lists\n",
    "Using the functions above, creates seed-word matrix with a user-specified number of files, performs pmi on that matrix, and computes a resulting correlation list for each seed word.\n",
    "\n",
    "In order to use more files, update the num_files variable. In order to update the seed set, update the seed_set2 global variable to include more words.\n",
    "\n",
    "Note: Creating these correlation lists at scale is very slow. Start off by processing about 10 files, and scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "parsing directory cor-por-a/2006/01/02\n",
      "parsing directory cor-por-a/2006/01/03\n",
      "parsing directory cor-por-a/2006/01/04\n",
      "parsing directory cor-por-a/2006/01/05\n",
      "Num files: 1000\n",
      "num terms in corpus: 724832\n",
      "vocab size: 38923\n",
      "matrix dimensions: 10 x 38923\n",
      "index_dict created!\n",
      "parsed 1000/724832 terms\n",
      "parsed 2000/724832 terms\n",
      "parsed 3000/724832 terms\n",
      "parsed 4000/724832 terms\n",
      "parsed 5000/724832 terms\n",
      "parsed 6000/724832 terms\n",
      "parsed 7000/724832 terms\n",
      "parsed 8000/724832 terms\n",
      "parsed 9000/724832 terms\n",
      "parsed 10000/724832 terms\n",
      "parsed 11000/724832 terms\n",
      "parsed 12000/724832 terms\n",
      "parsed 13000/724832 terms\n",
      "parsed 14000/724832 terms\n",
      "parsed 15000/724832 terms\n",
      "parsed 16000/724832 terms\n",
      "parsed 17000/724832 terms\n",
      "parsed 18000/724832 terms\n",
      "parsed 19000/724832 terms\n",
      "parsed 20000/724832 terms\n",
      "parsed 21000/724832 terms\n",
      "parsed 22000/724832 terms\n",
      "parsed 23000/724832 terms\n",
      "parsed 24000/724832 terms\n",
      "parsed 25000/724832 terms\n",
      "parsed 26000/724832 terms\n",
      "parsed 27000/724832 terms\n",
      "parsed 28000/724832 terms\n",
      "parsed 29000/724832 terms\n",
      "parsed 30000/724832 terms\n",
      "parsed 31000/724832 terms\n",
      "parsed 32000/724832 terms\n",
      "parsed 33000/724832 terms\n",
      "parsed 34000/724832 terms\n",
      "parsed 35000/724832 terms\n",
      "parsed 36000/724832 terms\n",
      "parsed 37000/724832 terms\n",
      "parsed 38000/724832 terms\n",
      "parsed 39000/724832 terms\n",
      "parsed 40000/724832 terms\n",
      "parsed 41000/724832 terms\n",
      "parsed 42000/724832 terms\n",
      "parsed 43000/724832 terms\n",
      "parsed 44000/724832 terms\n",
      "parsed 45000/724832 terms\n",
      "parsed 46000/724832 terms\n",
      "parsed 47000/724832 terms\n",
      "parsed 48000/724832 terms\n",
      "parsed 49000/724832 terms\n",
      "parsed 50000/724832 terms\n",
      "parsed 51000/724832 terms\n",
      "parsed 52000/724832 terms\n",
      "parsed 53000/724832 terms\n",
      "parsed 54000/724832 terms\n",
      "parsed 55000/724832 terms\n",
      "parsed 56000/724832 terms\n",
      "parsed 57000/724832 terms\n",
      "parsed 58000/724832 terms\n",
      "parsed 59000/724832 terms\n",
      "parsed 60000/724832 terms\n",
      "parsed 61000/724832 terms\n",
      "parsed 62000/724832 terms\n",
      "parsed 63000/724832 terms\n",
      "parsed 64000/724832 terms\n",
      "parsed 65000/724832 terms\n",
      "parsed 66000/724832 terms\n",
      "parsed 67000/724832 terms\n",
      "parsed 68000/724832 terms\n",
      "parsed 69000/724832 terms\n",
      "parsed 70000/724832 terms\n",
      "parsed 71000/724832 terms\n",
      "parsed 72000/724832 terms\n",
      "parsed 73000/724832 terms\n",
      "parsed 74000/724832 terms\n",
      "parsed 75000/724832 terms\n",
      "parsed 76000/724832 terms\n",
      "parsed 77000/724832 terms\n",
      "parsed 78000/724832 terms\n",
      "parsed 79000/724832 terms\n",
      "parsed 80000/724832 terms\n",
      "parsed 81000/724832 terms\n",
      "parsed 82000/724832 terms\n",
      "parsed 83000/724832 terms\n",
      "parsed 84000/724832 terms\n",
      "parsed 85000/724832 terms\n",
      "parsed 86000/724832 terms\n",
      "parsed 87000/724832 terms\n",
      "parsed 88000/724832 terms\n",
      "parsed 89000/724832 terms\n",
      "parsed 90000/724832 terms\n",
      "parsed 91000/724832 terms\n",
      "parsed 92000/724832 terms\n",
      "parsed 93000/724832 terms\n",
      "parsed 94000/724832 terms\n",
      "parsed 95000/724832 terms\n",
      "parsed 96000/724832 terms\n",
      "parsed 97000/724832 terms\n",
      "parsed 98000/724832 terms\n",
      "parsed 99000/724832 terms\n",
      "parsed 100000/724832 terms\n",
      "parsed 101000/724832 terms\n",
      "parsed 102000/724832 terms\n",
      "parsed 103000/724832 terms\n",
      "parsed 104000/724832 terms\n",
      "parsed 105000/724832 terms\n",
      "parsed 106000/724832 terms\n",
      "parsed 107000/724832 terms\n",
      "parsed 108000/724832 terms\n",
      "parsed 109000/724832 terms\n",
      "parsed 110000/724832 terms\n",
      "parsed 111000/724832 terms\n",
      "parsed 112000/724832 terms\n",
      "parsed 113000/724832 terms\n",
      "parsed 114000/724832 terms\n",
      "parsed 115000/724832 terms\n",
      "parsed 116000/724832 terms\n",
      "parsed 117000/724832 terms\n",
      "parsed 118000/724832 terms\n",
      "parsed 119000/724832 terms\n",
      "parsed 120000/724832 terms\n",
      "parsed 121000/724832 terms\n",
      "parsed 122000/724832 terms\n",
      "parsed 123000/724832 terms\n",
      "parsed 124000/724832 terms\n",
      "parsed 125000/724832 terms\n",
      "parsed 126000/724832 terms\n",
      "parsed 127000/724832 terms\n",
      "parsed 128000/724832 terms\n",
      "parsed 129000/724832 terms\n",
      "parsed 130000/724832 terms\n",
      "parsed 131000/724832 terms\n",
      "parsed 132000/724832 terms\n",
      "parsed 133000/724832 terms\n",
      "parsed 134000/724832 terms\n",
      "parsed 135000/724832 terms\n",
      "parsed 136000/724832 terms\n",
      "parsed 137000/724832 terms\n",
      "parsed 138000/724832 terms\n",
      "parsed 139000/724832 terms\n",
      "parsed 140000/724832 terms\n",
      "parsed 141000/724832 terms\n",
      "parsed 142000/724832 terms\n",
      "parsed 143000/724832 terms\n",
      "parsed 144000/724832 terms\n",
      "parsed 145000/724832 terms\n",
      "parsed 146000/724832 terms\n",
      "parsed 147000/724832 terms\n",
      "parsed 148000/724832 terms\n",
      "parsed 149000/724832 terms\n",
      "parsed 150000/724832 terms\n",
      "parsed 151000/724832 terms\n",
      "parsed 152000/724832 terms\n",
      "parsed 153000/724832 terms\n",
      "parsed 154000/724832 terms\n",
      "parsed 155000/724832 terms\n",
      "parsed 156000/724832 terms\n",
      "parsed 157000/724832 terms\n",
      "parsed 158000/724832 terms\n",
      "parsed 159000/724832 terms\n",
      "parsed 160000/724832 terms\n",
      "parsed 161000/724832 terms\n",
      "parsed 162000/724832 terms\n",
      "parsed 163000/724832 terms\n",
      "parsed 164000/724832 terms\n",
      "parsed 165000/724832 terms\n",
      "parsed 166000/724832 terms\n",
      "parsed 167000/724832 terms\n",
      "parsed 168000/724832 terms\n",
      "parsed 169000/724832 terms\n",
      "parsed 170000/724832 terms\n",
      "parsed 171000/724832 terms\n",
      "parsed 172000/724832 terms\n",
      "parsed 173000/724832 terms\n",
      "parsed 174000/724832 terms\n",
      "parsed 175000/724832 terms\n",
      "parsed 176000/724832 terms\n",
      "parsed 177000/724832 terms\n",
      "parsed 178000/724832 terms\n",
      "parsed 179000/724832 terms\n",
      "parsed 180000/724832 terms\n",
      "parsed 181000/724832 terms\n",
      "parsed 182000/724832 terms\n",
      "parsed 183000/724832 terms\n",
      "parsed 184000/724832 terms\n",
      "parsed 185000/724832 terms\n",
      "parsed 186000/724832 terms\n",
      "parsed 187000/724832 terms\n",
      "parsed 188000/724832 terms\n",
      "parsed 189000/724832 terms\n",
      "parsed 190000/724832 terms\n",
      "parsed 191000/724832 terms\n",
      "parsed 192000/724832 terms\n",
      "parsed 193000/724832 terms\n",
      "parsed 194000/724832 terms\n",
      "parsed 195000/724832 terms\n",
      "parsed 196000/724832 terms\n",
      "parsed 197000/724832 terms\n",
      "parsed 198000/724832 terms\n",
      "parsed 199000/724832 terms\n",
      "parsed 200000/724832 terms\n",
      "parsed 201000/724832 terms\n",
      "parsed 202000/724832 terms\n",
      "parsed 203000/724832 terms\n",
      "parsed 204000/724832 terms\n",
      "parsed 205000/724832 terms\n",
      "parsed 206000/724832 terms\n",
      "parsed 207000/724832 terms\n",
      "parsed 208000/724832 terms\n",
      "parsed 209000/724832 terms\n",
      "parsed 210000/724832 terms\n",
      "parsed 211000/724832 terms\n",
      "parsed 212000/724832 terms\n",
      "parsed 213000/724832 terms\n",
      "parsed 214000/724832 terms\n",
      "parsed 215000/724832 terms\n",
      "parsed 216000/724832 terms\n",
      "parsed 217000/724832 terms\n",
      "parsed 218000/724832 terms\n",
      "parsed 219000/724832 terms\n",
      "parsed 220000/724832 terms\n",
      "parsed 221000/724832 terms\n",
      "parsed 222000/724832 terms\n",
      "parsed 223000/724832 terms\n",
      "parsed 224000/724832 terms\n",
      "parsed 225000/724832 terms\n",
      "parsed 226000/724832 terms\n",
      "parsed 227000/724832 terms\n",
      "parsed 228000/724832 terms\n",
      "parsed 229000/724832 terms\n",
      "parsed 230000/724832 terms\n",
      "parsed 231000/724832 terms\n",
      "parsed 232000/724832 terms\n",
      "parsed 233000/724832 terms\n",
      "parsed 234000/724832 terms\n",
      "parsed 235000/724832 terms\n",
      "parsed 236000/724832 terms\n",
      "parsed 237000/724832 terms\n",
      "parsed 238000/724832 terms\n",
      "parsed 239000/724832 terms\n",
      "parsed 240000/724832 terms\n",
      "parsed 241000/724832 terms\n",
      "parsed 242000/724832 terms\n",
      "parsed 243000/724832 terms\n",
      "parsed 244000/724832 terms\n",
      "parsed 245000/724832 terms\n",
      "parsed 246000/724832 terms\n",
      "parsed 247000/724832 terms\n",
      "parsed 248000/724832 terms\n",
      "parsed 249000/724832 terms\n",
      "parsed 250000/724832 terms\n",
      "parsed 251000/724832 terms\n",
      "parsed 252000/724832 terms\n",
      "parsed 253000/724832 terms\n",
      "parsed 254000/724832 terms\n",
      "parsed 255000/724832 terms\n",
      "parsed 256000/724832 terms\n",
      "parsed 257000/724832 terms\n",
      "parsed 258000/724832 terms\n",
      "parsed 259000/724832 terms\n",
      "parsed 260000/724832 terms\n",
      "parsed 261000/724832 terms\n",
      "parsed 262000/724832 terms\n",
      "parsed 263000/724832 terms\n",
      "parsed 264000/724832 terms\n",
      "parsed 265000/724832 terms\n",
      "parsed 266000/724832 terms\n",
      "parsed 267000/724832 terms\n",
      "parsed 268000/724832 terms\n",
      "parsed 269000/724832 terms\n",
      "parsed 270000/724832 terms\n",
      "parsed 271000/724832 terms\n",
      "parsed 272000/724832 terms\n",
      "parsed 273000/724832 terms\n",
      "parsed 274000/724832 terms\n",
      "parsed 275000/724832 terms\n",
      "parsed 276000/724832 terms\n",
      "parsed 277000/724832 terms\n",
      "parsed 278000/724832 terms\n",
      "parsed 279000/724832 terms\n",
      "parsed 280000/724832 terms\n",
      "parsed 281000/724832 terms\n",
      "parsed 282000/724832 terms\n",
      "parsed 283000/724832 terms\n",
      "parsed 284000/724832 terms\n",
      "parsed 285000/724832 terms\n",
      "parsed 286000/724832 terms\n",
      "parsed 287000/724832 terms\n",
      "parsed 288000/724832 terms\n",
      "parsed 289000/724832 terms\n",
      "parsed 290000/724832 terms\n",
      "parsed 291000/724832 terms\n",
      "parsed 292000/724832 terms\n",
      "parsed 293000/724832 terms\n",
      "parsed 294000/724832 terms\n",
      "parsed 295000/724832 terms\n",
      "parsed 296000/724832 terms\n",
      "parsed 297000/724832 terms\n",
      "parsed 298000/724832 terms\n",
      "parsed 299000/724832 terms\n",
      "parsed 300000/724832 terms\n",
      "parsed 301000/724832 terms\n",
      "parsed 302000/724832 terms\n",
      "parsed 303000/724832 terms\n",
      "parsed 304000/724832 terms\n",
      "parsed 305000/724832 terms\n",
      "parsed 306000/724832 terms\n",
      "parsed 307000/724832 terms\n",
      "parsed 308000/724832 terms\n",
      "parsed 309000/724832 terms\n",
      "parsed 310000/724832 terms\n",
      "parsed 311000/724832 terms\n",
      "parsed 312000/724832 terms\n",
      "parsed 313000/724832 terms\n",
      "parsed 314000/724832 terms\n",
      "parsed 315000/724832 terms\n",
      "parsed 316000/724832 terms\n",
      "parsed 317000/724832 terms\n",
      "parsed 318000/724832 terms\n",
      "parsed 319000/724832 terms\n",
      "parsed 320000/724832 terms\n",
      "parsed 321000/724832 terms\n",
      "parsed 322000/724832 terms\n",
      "parsed 323000/724832 terms\n",
      "parsed 324000/724832 terms\n",
      "parsed 325000/724832 terms\n",
      "parsed 326000/724832 terms\n",
      "parsed 327000/724832 terms\n",
      "parsed 328000/724832 terms\n",
      "parsed 329000/724832 terms\n",
      "parsed 330000/724832 terms\n",
      "parsed 331000/724832 terms\n",
      "parsed 332000/724832 terms\n",
      "parsed 333000/724832 terms\n",
      "parsed 334000/724832 terms\n",
      "parsed 335000/724832 terms\n",
      "parsed 336000/724832 terms\n",
      "parsed 337000/724832 terms\n",
      "parsed 338000/724832 terms\n",
      "parsed 339000/724832 terms\n",
      "parsed 340000/724832 terms\n",
      "parsed 341000/724832 terms\n",
      "parsed 342000/724832 terms\n",
      "parsed 343000/724832 terms\n",
      "parsed 344000/724832 terms\n",
      "parsed 345000/724832 terms\n",
      "parsed 346000/724832 terms\n",
      "parsed 347000/724832 terms\n",
      "parsed 348000/724832 terms\n",
      "parsed 349000/724832 terms\n",
      "parsed 350000/724832 terms\n",
      "parsed 351000/724832 terms\n",
      "parsed 352000/724832 terms\n",
      "parsed 353000/724832 terms\n",
      "parsed 354000/724832 terms\n",
      "parsed 355000/724832 terms\n",
      "parsed 356000/724832 terms\n",
      "parsed 357000/724832 terms\n",
      "parsed 358000/724832 terms\n",
      "parsed 359000/724832 terms\n",
      "parsed 360000/724832 terms\n",
      "parsed 361000/724832 terms\n",
      "parsed 362000/724832 terms\n",
      "parsed 363000/724832 terms\n",
      "parsed 364000/724832 terms\n",
      "parsed 365000/724832 terms\n",
      "parsed 366000/724832 terms\n",
      "parsed 367000/724832 terms\n",
      "parsed 368000/724832 terms\n",
      "parsed 369000/724832 terms\n",
      "parsed 370000/724832 terms\n",
      "parsed 371000/724832 terms\n",
      "parsed 372000/724832 terms\n",
      "parsed 373000/724832 terms\n",
      "parsed 374000/724832 terms\n",
      "parsed 375000/724832 terms\n",
      "parsed 376000/724832 terms\n",
      "parsed 377000/724832 terms\n",
      "parsed 378000/724832 terms\n",
      "parsed 379000/724832 terms\n",
      "parsed 380000/724832 terms\n",
      "parsed 381000/724832 terms\n",
      "parsed 382000/724832 terms\n",
      "parsed 383000/724832 terms\n",
      "parsed 384000/724832 terms\n",
      "parsed 385000/724832 terms\n",
      "parsed 386000/724832 terms\n",
      "parsed 387000/724832 terms\n",
      "parsed 388000/724832 terms\n",
      "parsed 389000/724832 terms\n",
      "parsed 390000/724832 terms\n",
      "parsed 391000/724832 terms\n",
      "parsed 392000/724832 terms\n",
      "parsed 393000/724832 terms\n",
      "parsed 394000/724832 terms\n",
      "parsed 395000/724832 terms\n",
      "parsed 396000/724832 terms\n",
      "parsed 397000/724832 terms\n",
      "parsed 398000/724832 terms\n",
      "parsed 399000/724832 terms\n",
      "parsed 400000/724832 terms\n",
      "parsed 401000/724832 terms\n",
      "parsed 402000/724832 terms\n",
      "parsed 403000/724832 terms\n",
      "parsed 404000/724832 terms\n",
      "parsed 405000/724832 terms\n",
      "parsed 406000/724832 terms\n",
      "parsed 407000/724832 terms\n",
      "parsed 408000/724832 terms\n",
      "parsed 409000/724832 terms\n",
      "parsed 410000/724832 terms\n",
      "parsed 411000/724832 terms\n",
      "parsed 412000/724832 terms\n",
      "parsed 413000/724832 terms\n",
      "parsed 414000/724832 terms\n",
      "parsed 415000/724832 terms\n",
      "parsed 416000/724832 terms\n",
      "parsed 417000/724832 terms\n",
      "parsed 418000/724832 terms\n",
      "parsed 419000/724832 terms\n",
      "parsed 420000/724832 terms\n",
      "parsed 421000/724832 terms\n",
      "parsed 422000/724832 terms\n",
      "parsed 423000/724832 terms\n",
      "parsed 424000/724832 terms\n",
      "parsed 425000/724832 terms\n",
      "parsed 426000/724832 terms\n",
      "parsed 427000/724832 terms\n",
      "parsed 428000/724832 terms\n",
      "parsed 429000/724832 terms\n",
      "parsed 430000/724832 terms\n",
      "parsed 431000/724832 terms\n",
      "parsed 432000/724832 terms\n",
      "parsed 433000/724832 terms\n",
      "parsed 434000/724832 terms\n",
      "parsed 435000/724832 terms\n",
      "parsed 436000/724832 terms\n",
      "parsed 437000/724832 terms\n",
      "parsed 438000/724832 terms\n",
      "parsed 439000/724832 terms\n",
      "parsed 440000/724832 terms\n",
      "parsed 441000/724832 terms\n",
      "parsed 442000/724832 terms\n",
      "parsed 443000/724832 terms\n",
      "parsed 444000/724832 terms\n",
      "parsed 445000/724832 terms\n",
      "parsed 446000/724832 terms\n",
      "parsed 447000/724832 terms\n",
      "parsed 448000/724832 terms\n",
      "parsed 449000/724832 terms\n",
      "parsed 450000/724832 terms\n",
      "parsed 451000/724832 terms\n",
      "parsed 452000/724832 terms\n",
      "parsed 453000/724832 terms\n",
      "parsed 454000/724832 terms\n",
      "parsed 455000/724832 terms\n",
      "parsed 456000/724832 terms\n",
      "parsed 457000/724832 terms\n",
      "parsed 458000/724832 terms\n",
      "parsed 459000/724832 terms\n",
      "parsed 460000/724832 terms\n",
      "parsed 461000/724832 terms\n",
      "parsed 462000/724832 terms\n",
      "parsed 463000/724832 terms\n",
      "parsed 464000/724832 terms\n",
      "parsed 465000/724832 terms\n",
      "parsed 466000/724832 terms\n",
      "parsed 467000/724832 terms\n",
      "parsed 468000/724832 terms\n",
      "parsed 469000/724832 terms\n",
      "parsed 470000/724832 terms\n",
      "parsed 471000/724832 terms\n",
      "parsed 472000/724832 terms\n",
      "parsed 473000/724832 terms\n",
      "parsed 474000/724832 terms\n",
      "parsed 475000/724832 terms\n",
      "parsed 476000/724832 terms\n",
      "parsed 477000/724832 terms\n",
      "parsed 478000/724832 terms\n",
      "parsed 479000/724832 terms\n",
      "parsed 480000/724832 terms\n",
      "parsed 481000/724832 terms\n",
      "parsed 482000/724832 terms\n",
      "parsed 483000/724832 terms\n",
      "parsed 484000/724832 terms\n",
      "parsed 485000/724832 terms\n",
      "parsed 486000/724832 terms\n",
      "parsed 487000/724832 terms\n",
      "parsed 488000/724832 terms\n",
      "parsed 489000/724832 terms\n",
      "parsed 490000/724832 terms\n",
      "parsed 491000/724832 terms\n",
      "parsed 492000/724832 terms\n",
      "parsed 493000/724832 terms\n",
      "parsed 494000/724832 terms\n",
      "parsed 495000/724832 terms\n",
      "parsed 496000/724832 terms\n",
      "parsed 497000/724832 terms\n",
      "parsed 498000/724832 terms\n",
      "parsed 499000/724832 terms\n",
      "parsed 500000/724832 terms\n",
      "parsed 501000/724832 terms\n",
      "parsed 502000/724832 terms\n",
      "parsed 503000/724832 terms\n",
      "parsed 504000/724832 terms\n",
      "parsed 505000/724832 terms\n",
      "parsed 506000/724832 terms\n",
      "parsed 507000/724832 terms\n",
      "parsed 508000/724832 terms\n",
      "parsed 509000/724832 terms\n",
      "parsed 510000/724832 terms\n",
      "parsed 511000/724832 terms\n",
      "parsed 512000/724832 terms\n",
      "parsed 513000/724832 terms\n",
      "parsed 514000/724832 terms\n",
      "parsed 515000/724832 terms\n",
      "parsed 516000/724832 terms\n",
      "parsed 517000/724832 terms\n",
      "parsed 518000/724832 terms\n",
      "parsed 519000/724832 terms\n",
      "parsed 520000/724832 terms\n",
      "parsed 521000/724832 terms\n",
      "parsed 522000/724832 terms\n",
      "parsed 523000/724832 terms\n",
      "parsed 524000/724832 terms\n",
      "parsed 525000/724832 terms\n",
      "parsed 526000/724832 terms\n",
      "parsed 527000/724832 terms\n",
      "parsed 528000/724832 terms\n",
      "parsed 529000/724832 terms\n",
      "parsed 530000/724832 terms\n",
      "parsed 531000/724832 terms\n",
      "parsed 532000/724832 terms\n",
      "parsed 533000/724832 terms\n",
      "parsed 534000/724832 terms\n",
      "parsed 535000/724832 terms\n",
      "parsed 536000/724832 terms\n",
      "parsed 537000/724832 terms\n",
      "parsed 538000/724832 terms\n",
      "parsed 539000/724832 terms\n",
      "parsed 540000/724832 terms\n",
      "parsed 541000/724832 terms\n",
      "parsed 542000/724832 terms\n",
      "parsed 543000/724832 terms\n",
      "parsed 544000/724832 terms\n",
      "parsed 545000/724832 terms\n",
      "parsed 546000/724832 terms\n",
      "parsed 547000/724832 terms\n",
      "parsed 548000/724832 terms\n",
      "parsed 549000/724832 terms\n",
      "parsed 550000/724832 terms\n",
      "parsed 551000/724832 terms\n",
      "parsed 552000/724832 terms\n",
      "parsed 553000/724832 terms\n",
      "parsed 554000/724832 terms\n",
      "parsed 555000/724832 terms\n",
      "parsed 556000/724832 terms\n",
      "parsed 557000/724832 terms\n",
      "parsed 558000/724832 terms\n",
      "parsed 559000/724832 terms\n",
      "parsed 560000/724832 terms\n",
      "parsed 561000/724832 terms\n",
      "parsed 562000/724832 terms\n",
      "parsed 563000/724832 terms\n",
      "parsed 564000/724832 terms\n",
      "parsed 565000/724832 terms\n",
      "parsed 566000/724832 terms\n",
      "parsed 567000/724832 terms\n",
      "parsed 568000/724832 terms\n",
      "parsed 569000/724832 terms\n",
      "parsed 570000/724832 terms\n",
      "parsed 571000/724832 terms\n",
      "parsed 572000/724832 terms\n",
      "parsed 573000/724832 terms\n",
      "parsed 574000/724832 terms\n",
      "parsed 575000/724832 terms\n",
      "parsed 576000/724832 terms\n",
      "parsed 577000/724832 terms\n",
      "parsed 578000/724832 terms\n",
      "parsed 579000/724832 terms\n",
      "parsed 580000/724832 terms\n",
      "parsed 581000/724832 terms\n",
      "parsed 582000/724832 terms\n",
      "parsed 583000/724832 terms\n",
      "parsed 584000/724832 terms\n",
      "parsed 585000/724832 terms\n",
      "parsed 586000/724832 terms\n",
      "parsed 587000/724832 terms\n",
      "parsed 588000/724832 terms\n",
      "parsed 589000/724832 terms\n",
      "parsed 590000/724832 terms\n",
      "parsed 591000/724832 terms\n",
      "parsed 592000/724832 terms\n",
      "parsed 593000/724832 terms\n",
      "parsed 594000/724832 terms\n",
      "parsed 595000/724832 terms\n",
      "parsed 596000/724832 terms\n",
      "parsed 597000/724832 terms\n",
      "parsed 598000/724832 terms\n",
      "parsed 599000/724832 terms\n",
      "parsed 600000/724832 terms\n",
      "parsed 601000/724832 terms\n",
      "parsed 602000/724832 terms\n",
      "parsed 603000/724832 terms\n",
      "parsed 604000/724832 terms\n",
      "parsed 605000/724832 terms\n",
      "parsed 606000/724832 terms\n",
      "parsed 607000/724832 terms\n",
      "parsed 608000/724832 terms\n",
      "parsed 609000/724832 terms\n",
      "parsed 610000/724832 terms\n",
      "parsed 611000/724832 terms\n",
      "parsed 612000/724832 terms\n",
      "parsed 613000/724832 terms\n",
      "parsed 614000/724832 terms\n",
      "parsed 615000/724832 terms\n",
      "parsed 616000/724832 terms\n",
      "parsed 617000/724832 terms\n",
      "parsed 618000/724832 terms\n",
      "parsed 619000/724832 terms\n",
      "parsed 620000/724832 terms\n",
      "parsed 621000/724832 terms\n",
      "parsed 622000/724832 terms\n",
      "parsed 623000/724832 terms\n",
      "parsed 624000/724832 terms\n",
      "parsed 625000/724832 terms\n",
      "parsed 626000/724832 terms\n",
      "parsed 627000/724832 terms\n",
      "parsed 628000/724832 terms\n",
      "parsed 629000/724832 terms\n",
      "parsed 630000/724832 terms\n",
      "parsed 631000/724832 terms\n",
      "parsed 632000/724832 terms\n",
      "parsed 633000/724832 terms\n",
      "parsed 634000/724832 terms\n",
      "parsed 635000/724832 terms\n",
      "parsed 636000/724832 terms\n",
      "parsed 637000/724832 terms\n",
      "parsed 638000/724832 terms\n",
      "parsed 639000/724832 terms\n",
      "parsed 640000/724832 terms\n",
      "parsed 641000/724832 terms\n",
      "parsed 642000/724832 terms\n",
      "parsed 643000/724832 terms\n",
      "parsed 644000/724832 terms\n",
      "parsed 645000/724832 terms\n",
      "parsed 646000/724832 terms\n",
      "parsed 647000/724832 terms\n",
      "parsed 648000/724832 terms\n",
      "parsed 649000/724832 terms\n",
      "parsed 650000/724832 terms\n",
      "parsed 651000/724832 terms\n",
      "parsed 652000/724832 terms\n",
      "parsed 653000/724832 terms\n",
      "parsed 654000/724832 terms\n",
      "parsed 655000/724832 terms\n",
      "parsed 656000/724832 terms\n",
      "parsed 657000/724832 terms\n",
      "parsed 658000/724832 terms\n",
      "parsed 659000/724832 terms\n",
      "parsed 660000/724832 terms\n",
      "parsed 661000/724832 terms\n",
      "parsed 662000/724832 terms\n",
      "parsed 663000/724832 terms\n",
      "parsed 664000/724832 terms\n",
      "parsed 665000/724832 terms\n",
      "parsed 666000/724832 terms\n",
      "parsed 667000/724832 terms\n",
      "parsed 668000/724832 terms\n",
      "parsed 669000/724832 terms\n",
      "parsed 670000/724832 terms\n",
      "parsed 671000/724832 terms\n",
      "parsed 672000/724832 terms\n",
      "parsed 673000/724832 terms\n",
      "parsed 674000/724832 terms\n",
      "parsed 675000/724832 terms\n",
      "parsed 676000/724832 terms\n",
      "parsed 677000/724832 terms\n",
      "parsed 678000/724832 terms\n",
      "parsed 679000/724832 terms\n",
      "parsed 680000/724832 terms\n",
      "parsed 681000/724832 terms\n",
      "parsed 682000/724832 terms\n",
      "parsed 683000/724832 terms\n",
      "parsed 684000/724832 terms\n",
      "parsed 685000/724832 terms\n",
      "parsed 686000/724832 terms\n",
      "parsed 687000/724832 terms\n",
      "parsed 688000/724832 terms\n",
      "parsed 689000/724832 terms\n",
      "parsed 690000/724832 terms\n",
      "parsed 691000/724832 terms\n",
      "parsed 692000/724832 terms\n",
      "parsed 693000/724832 terms\n",
      "parsed 694000/724832 terms\n",
      "parsed 695000/724832 terms\n",
      "parsed 696000/724832 terms\n",
      "parsed 697000/724832 terms\n",
      "parsed 698000/724832 terms\n",
      "parsed 699000/724832 terms\n",
      "parsed 700000/724832 terms\n",
      "parsed 701000/724832 terms\n",
      "parsed 702000/724832 terms\n",
      "parsed 703000/724832 terms\n",
      "parsed 704000/724832 terms\n",
      "parsed 705000/724832 terms\n",
      "parsed 706000/724832 terms\n",
      "parsed 707000/724832 terms\n",
      "parsed 708000/724832 terms\n",
      "parsed 709000/724832 terms\n",
      "parsed 710000/724832 terms\n",
      "parsed 711000/724832 terms\n",
      "parsed 712000/724832 terms\n",
      "parsed 713000/724832 terms\n",
      "parsed 714000/724832 terms\n",
      "parsed 715000/724832 terms\n",
      "parsed 716000/724832 terms\n",
      "parsed 717000/724832 terms\n",
      "parsed 718000/724832 terms\n",
      "parsed 719000/724832 terms\n",
      "parsed 720000/724832 terms\n",
      "parsed 721000/724832 terms\n",
      "parsed 722000/724832 terms\n",
      "parsed 723000/724832 terms\n",
      "parsed 724000/724832 terms\n",
      "Parsed 724832/724832 terms\n",
      "New vocab size: 9005\n"
     ]
    }
   ],
   "source": [
    "mat_obj = createSeedWordMatrixNYT(num_files=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_obj_ppmi = pmi_seed(mat_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI list for word: asian-american; [(u'', 0), (u' ', 0), (u'-', 0), (u'--', 0), (u'-c', 0), (u'-ft', 0), (u'0', 0), (u'00', 0), (u'000', 0), (u'000-square-foot', 0), (u'05', 0), (u'1', 0), (u'1-bath', 0), (u'1-car', 0), (u'10', 0), (u'10-6', 0), (u'100', 0), (u'100-yard', 0), (u'101', 0), (u'105', 0)]\n",
      "PMI list for word: african-american; [(u'african-american', 9.1274827472343247), (u'literature', 7.3782828924250659), (u'restrict', 7.3357232780062702), (u'admissions', 6.8248976542402788), (u'featuring', 6.4884254176190668), (u'studies', 6.1317504736803343), (u'patients', 5.5579500507529547), (u'girl', 5.3898131289509568), (u'readers', 5.3208202574640051), (u'plays', 4.8789875051849663), (u'children', 4.8000443028448467), (u'movie', 4.5123622303930659), (u'history', 3.8857357321746826), (u'young', 3.6385450210776384), (u'women', 3.6020298081025413), (u'second', 3.5591382434732282), (u'percent', 2.9068925771345859), (u'about', 2.5426913548486088), (u'first', 2.4048529523788766), (u'on', 1.3764376292625231)]\n",
      "PMI list for word: black; [(u'black', 6.9769956477497956), (u'mesa', 6.7256812194688891), (u'fedora', 6.1296977873625913), (u'bookstore', 5.8783833590816856), (u'bookstores', 5.472918250973521), (u'pepper', 5.3675577353156951), (u'heroine', 5.1852361785217402), (u'literate', 5.1852361785217402), (u'penny', 5.1852361785217402), (u'petroleum', 5.1852361785217402), (u'beans', 5.1311689572514645), (u'hole', 5.0310854986944822), (u'olives', 5.0310854986944822), (u'shining', 5.0310854986944822), (u'troops', 4.9620926272075305), (u'cloud', 4.8975541060699594), (u'pastor', 4.8975541060699594), (u'register', 4.8975541060699594), (u't-shirt', 4.8975541060699594), (u'coats', 4.7797710704135756)]\n",
      "PMI list for word: african; [(u'african', 8.8228223382481268), (u'cuisine', 7.0310628690200714), (u'soil', 6.8769121891928133), (u'businessman', 6.6255977609119068), (u'continent', 6.6255977609119068), (u'slave', 6.337915688460126), (u'south', 6.1704610430065046), (u'wildlife', 5.9324505803519614), (u'ambassador', 5.8783833590816856), (u'wines', 5.7317798848898107), (u'lamb', 5.6039465133799258), (u'worried', 5.6039465133799258), (u'plant', 5.061622222554564), (u'species', 4.7797710704135765), (u'artists', 4.6033146330720198), (u'culture', 4.5187572450439566), (u'park', 4.4533744857811044), (u'lawyer', 4.3684750419946186), (u'teams', 4.1880933500184909), (u'west', 4.0735518082862781)]\n",
      "PMI list for word: asian; [(u'asian', 8.6304504456006708), (u'southeast', 7.426477641274734), (u'elephants', 7.0210125331665694), (u'cuisine', 6.8386909763726154), (u'central', 6.0016496161526005), (u'heritage', 5.9224002444984603), (u'trends', 5.9224002444984603), (u'tsunami', 5.8578617233608892), (u'famous', 5.6019283492236882), (u'bomb', 5.4949562296715202), (u'nation', 5.4523966152527246), (u'restaurants', 5.4386032931203889), (u'sets', 5.2982459354254665), (u'status', 5.2631546156141962), (u'male', 4.9415709914867341), (u'deputy', 4.8462608116824093), (u'features', 4.7803028438906114), (u'countries', 4.6792067270192428), (u'studies', 4.5361058833785695), (u'largely', 4.4560631757050331)]\n",
      "PMI list for word: jewish; [(u'jewish', 8.4139958824372574), (u'plaza', 6.8045579700031569), (u'full-service', 6.622236413209202), (u'theological', 6.622236413209202), (u'moroccan', 6.4680857333819439), (u'orthodox', 6.4680857333819439), (u'philanthropy', 6.4680857333819439), (u'creativity', 6.2167713051010374), (u'identity', 6.0161006096388867), (u'community', 5.9290892326492566), (u'parker', 5.9290892326492566), (u'immigrants', 5.6414071601974758), (u'bible', 5.5807825383810412), (u'dealers', 5.5807825383810412), (u'teenager', 5.523624124541092), (u'wealthy', 5.4695569032708162), (u'cooper', 5.2785016665081077), (u'weekly', 5.2785016665081077), (u'leaders', 4.8397793366434607), (u'organizations', 4.6763262641538885)]\n",
      "PMI list for word: latino; [(u'cultural', 6.9196600389062821), (u'residents', 6.8059011535496241), (u'market', 5.6414071601974758), (u'000', 5.0450813678563984), (u'and', 1.6985230146475478), (u'a', 1.6389056408888498), (u'the', 0.84984095527210601), (u'', 0.0), (u' ', 0.0), (u'-', 0.0), (u'--', 0.0), (u'-c', 0.0), (u'-ft', 0.0), (u'0', 0.0), (u'00', 0.0), (u'000-square-foot', 0.0), (u'05', 0.0), (u'1', 0.0), (u'1-bath', 0.0), (u'1-car', 0.0)]\n",
      "PMI list for word: mexican; [(u'mexican', 9.6337525544644542), (u'18-year-old', 7.8419930852363997), (u'cuisine', 7.8419930852363997), (u'gifted', 7.6878424054091417), (u'masters', 7.3311674614704092), (u'authorities', 6.9425094716786262), (u'chef', 6.8005392104082389), (u'telenovelas', 6.6893135752980148), (u'ambitious', 6.4148767295962541), (u'regional', 6.1680166516647281), (u'restaurants', 5.3432931133160642), (u'hospital', 5.251725919790573), (u'felt', 5.0904577721944513), (u'told', 4.3304476464053794), (u'local', 4.2866450237469866), (u'man', 4.1956732455412595), (u'take', 3.816641394501251), (u'the', 0.24370515170179047), (u'', 0.0), (u' ', 0.0)]\n",
      "PMI list for word: russian; [(u'russian', 7.9541103833571061), (u'rivers', 6.5678160222372153), (u'imported', 6.3446724709230056), (u'ally', 6.1623509141290507), (u'import', 6.0082002343017926), (u'soil', 6.0082002343017926), (u'warrant', 6.0082002343017926), (u'faithful', 5.8746688416772699), (u'brutal', 5.756885806020887), (u'initiative', 5.756885806020887), (u'pipelines', 5.7028185847506112), (u'dominance', 5.4692037335691053), (u'export', 5.2799617339305769), (u'dismissed', 5.1815216611173245), (u'immigrants', 5.1815216611173245), (u'gas', 4.9135640940133225), (u'river', 4.8907194613292999), (u'governments', 4.7760565530091608), (u'natural', 4.7015235011465935), (u'exports', 4.5529130016949502)]\n",
      "PMI list for word: american; [(u'american', 6.2878436782571381), (u'commanders', 5.371552946382983), (u'idol', 5.2762427665786582), (u'composers', 5.0350807097617709), (u'indians', 5.0350807097617709), (u'pie', 5.0350807097617709), (u'enterprise', 4.9885606941268774), (u'tenor', 4.9015493171372482), (u'military', 4.6875382493863107), (u'heritage', 4.6784057658230385), (u'troops', 4.6784057658230385), (u'democracy', 4.6296156016536063), (u'airlines', 4.5386438234478792), (u'citizen', 4.4960842090290836), (u'commander', 4.4960842090290836), (u'dramas', 4.4960842090290836), (u'express', 4.4960842090290836), (u'generosity', 4.4960842090290836), (u'honda', 4.4960842090290836), (u'mercury', 4.4960842090290836)]\n",
      "Frequency list: [(u'', 106756), (u'the', 35906), (u'of', 16603), (u'a', 16311), (u'and', 15367), (u'to', 15255), (u'in', 12751), (u'that', 7056), (u's', 6749), (u'for', 5852), (u'is', 5129), (u'on', 4648), (u'it', 4590), (u'was', 4423), (u'he', 4408), (u'with', 4373), (u'said', 4266), (u'at', 3636), (u'as', 3564), (u'mr', 3053)]\n"
     ]
    }
   ],
   "source": [
    "lists2 = getCorrelationLists(mat_obj_ppmi, rem_stopwords=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI list for word: asian-american; [(u'', 0), (u' ', 0), (u'-', 0), (u'--', 0), (u'-c', 0), (u'-ft', 0), (u'0', 0), (u'00', 0), (u'000', 0), (u'000-square-foot', 0), (u'05', 0), (u'1', 0), (u'1-bath', 0), (u'1-car', 0), (u'10', 0), (u'10-6', 0), (u'100', 0), (u'100-yard', 0), (u'101', 0), (u'105', 0)]\n",
      "PMI list for word: african-american; [(u'african-american', 9.1274827472343247), (u'literature', 7.3782828924250659), (u'restrict', 7.3357232780062702), (u'admissions', 6.8248976542402788), (u'featuring', 6.4884254176190668), (u'studies', 6.1317504736803343), (u'patients', 5.5579500507529547), (u'girl', 5.3898131289509568), (u'readers', 5.3208202574640051), (u'plays', 4.8789875051849663), (u'children', 4.8000443028448467), (u'movie', 4.5123622303930659), (u'history', 3.8857357321746826), (u'young', 3.6385450210776384), (u'women', 3.6020298081025413), (u'second', 3.5591382434732282), (u'percent', 2.9068925771345859), (u'first', 2.4048529523788766), (u'', 0.0), (u' ', 0.0)]\n",
      "PMI list for word: black; [(u'black', 6.9769956477497956), (u'mesa', 6.7256812194688891), (u'fedora', 6.1296977873625913), (u'bookstore', 5.8783833590816856), (u'bookstores', 5.472918250973521), (u'pepper', 5.3675577353156951), (u'heroine', 5.1852361785217402), (u'literate', 5.1852361785217402), (u'penny', 5.1852361785217402), (u'petroleum', 5.1852361785217402), (u'beans', 5.1311689572514645), (u'hole', 5.0310854986944822), (u'olives', 5.0310854986944822), (u'shining', 5.0310854986944822), (u'troops', 4.9620926272075305), (u'cloud', 4.8975541060699594), (u'pastor', 4.8975541060699594), (u'register', 4.8975541060699594), (u't-shirt', 4.8975541060699594), (u'coats', 4.7797710704135756)]\n",
      "PMI list for word: african; [(u'african', 8.8228223382481268), (u'cuisine', 7.0310628690200714), (u'soil', 6.8769121891928133), (u'businessman', 6.6255977609119068), (u'continent', 6.6255977609119068), (u'slave', 6.337915688460126), (u'south', 6.1704610430065046), (u'wildlife', 5.9324505803519614), (u'ambassador', 5.8783833590816856), (u'wines', 5.7317798848898107), (u'lamb', 5.6039465133799258), (u'worried', 5.6039465133799258), (u'plant', 5.061622222554564), (u'species', 4.7797710704135765), (u'artists', 4.6033146330720198), (u'culture', 4.5187572450439566), (u'park', 4.4533744857811044), (u'lawyer', 4.3684750419946186), (u'teams', 4.1880933500184909), (u'west', 4.0735518082862781)]\n",
      "PMI list for word: asian; [(u'asian', 8.6304504456006708), (u'southeast', 7.426477641274734), (u'elephants', 7.0210125331665694), (u'cuisine', 6.8386909763726154), (u'central', 6.0016496161526005), (u'heritage', 5.9224002444984603), (u'trends', 5.9224002444984603), (u'tsunami', 5.8578617233608892), (u'famous', 5.6019283492236882), (u'bomb', 5.4949562296715202), (u'nation', 5.4523966152527246), (u'restaurants', 5.4386032931203889), (u'sets', 5.2982459354254665), (u'status', 5.2631546156141962), (u'male', 4.9415709914867341), (u'deputy', 4.8462608116824093), (u'features', 4.7803028438906114), (u'countries', 4.6792067270192428), (u'studies', 4.5361058833785695), (u'largely', 4.4560631757050331)]\n",
      "PMI list for word: jewish; [(u'jewish', 8.4139958824372574), (u'plaza', 6.8045579700031569), (u'full-service', 6.622236413209202), (u'theological', 6.622236413209202), (u'moroccan', 6.4680857333819439), (u'orthodox', 6.4680857333819439), (u'philanthropy', 6.4680857333819439), (u'creativity', 6.2167713051010374), (u'identity', 6.0161006096388867), (u'community', 5.9290892326492566), (u'parker', 5.9290892326492566), (u'immigrants', 5.6414071601974758), (u'bible', 5.5807825383810412), (u'dealers', 5.5807825383810412), (u'teenager', 5.523624124541092), (u'wealthy', 5.4695569032708162), (u'cooper', 5.2785016665081077), (u'weekly', 5.2785016665081077), (u'leaders', 4.8397793366434607), (u'organizations', 4.6763262641538885)]\n",
      "PMI list for word: latino; [(u'cultural', 6.9196600389062821), (u'residents', 6.8059011535496241), (u'market', 5.6414071601974758), (u'000', 5.0450813678563984), (u'', 0.0), (u' ', 0.0), (u'-', 0.0), (u'--', 0.0), (u'-c', 0.0), (u'-ft', 0.0), (u'0', 0.0), (u'00', 0.0), (u'000-square-foot', 0.0), (u'05', 0.0), (u'1', 0.0), (u'1-bath', 0.0), (u'1-car', 0.0), (u'10', 0.0), (u'10-6', 0.0), (u'100', 0.0)]\n",
      "PMI list for word: mexican; [(u'mexican', 9.6337525544644542), (u'18-year-old', 7.8419930852363997), (u'cuisine', 7.8419930852363997), (u'gifted', 7.6878424054091417), (u'masters', 7.3311674614704092), (u'authorities', 6.9425094716786262), (u'chef', 6.8005392104082389), (u'telenovelas', 6.6893135752980148), (u'ambitious', 6.4148767295962541), (u'regional', 6.1680166516647281), (u'restaurants', 5.3432931133160642), (u'hospital', 5.251725919790573), (u'felt', 5.0904577721944513), (u'told', 4.3304476464053794), (u'local', 4.2866450237469866), (u'man', 4.1956732455412595), (u'take', 3.816641394501251), (u'', 0.0), (u' ', 0.0), (u'-', 0.0)]\n",
      "PMI list for word: russian; [(u'russian', 7.9541103833571061), (u'rivers', 6.5678160222372153), (u'imported', 6.3446724709230056), (u'ally', 6.1623509141290507), (u'import', 6.0082002343017926), (u'soil', 6.0082002343017926), (u'warrant', 6.0082002343017926), (u'faithful', 5.8746688416772699), (u'brutal', 5.756885806020887), (u'initiative', 5.756885806020887), (u'pipelines', 5.7028185847506112), (u'dominance', 5.4692037335691053), (u'export', 5.2799617339305769), (u'dismissed', 5.1815216611173245), (u'immigrants', 5.1815216611173245), (u'gas', 4.9135640940133225), (u'river', 4.8907194613292999), (u'governments', 4.7760565530091608), (u'natural', 4.7015235011465935), (u'exports', 4.5529130016949502)]\n",
      "PMI list for word: american; [(u'american', 6.2878436782571381), (u'commanders', 5.371552946382983), (u'idol', 5.2762427665786582), (u'composers', 5.0350807097617709), (u'indians', 5.0350807097617709), (u'pie', 5.0350807097617709), (u'enterprise', 4.9885606941268774), (u'tenor', 4.9015493171372482), (u'military', 4.6875382493863107), (u'heritage', 4.6784057658230385), (u'troops', 4.6784057658230385), (u'democracy', 4.6296156016536063), (u'airlines', 4.5386438234478792), (u'citizen', 4.4960842090290836), (u'commander', 4.4960842090290836), (u'dramas', 4.4960842090290836), (u'express', 4.4960842090290836), (u'generosity', 4.4960842090290836), (u'honda', 4.4960842090290836), (u'mercury', 4.4960842090290836)]\n",
      "Frequency list: [(u'', 106756), (u'the', 35906), (u'of', 16603), (u'a', 16311), (u'and', 15367), (u'to', 15255), (u'in', 12751), (u'that', 7056), (u's', 6749), (u'for', 5852), (u'is', 5129), (u'on', 4648), (u'it', 4590), (u'was', 4423), (u'he', 4408), (u'with', 4373), (u'said', 4266), (u'at', 3636), (u'as', 3564), (u'mr', 3053)]\n"
     ]
    }
   ],
   "source": [
    "# This is for earlier ppmi -- DEPRECATED --\n",
    "#lists = getCorrelationLists(mat_obj_ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI list for word: asian-american; [(u'', 0), (u' ', 0), (u'-', 0), (u'--', 0), (u'-c', 0), (u'-ft', 0), (u'0', 0), (u'00', 0), (u'000', 0), (u'000-square-foot', 0), (u'05', 0), (u'1', 0), (u'1-bath', 0), (u'1-car', 0), (u'10', 0), (u'10-6', 0), (u'100', 0), (u'100-yard', 0), (u'101', 0), (u'105', 0)]\n",
      "PMI list for word: african-american; [(u'african-american', 25), (u'', 6), (u'children', 4), (u'literature', 4), (u'studies', 3), (u'patients', 2), (u'admissions', 1), (u'featuring', 1), (u'first', 1), (u'girl', 1), (u'history', 1), (u'movie', 1), (u'percent', 1), (u'plays', 1), (u'readers', 1), (u'restrict', 1), (u'second', 1), (u'women', 1), (u'young', 1), (u' ', 0)]\n",
      "PMI list for word: black; [(u'black', 227), (u'', 35), (u'first', 15), (u'mesa', 7), (u'patients', 5), (u'player', 4), (u'troops', 4), (u'women', 4), (u'beans', 3), (u'community', 3), (u'fedora', 3), (u'ground', 3), (u'hole', 3), (u'man', 3), (u'people', 3), (u'pepper', 3), (u'ryan', 3), (u'sea', 3), (u'soldiers', 3), (u'among', 2)]\n",
      "PMI list for word: african; [(u'african', 36), (u'south', 16), (u'', 2), (u'another', 2), (u'game', 2), (u'park', 2), (u'west', 2), (u'ambassador', 1), (u'artists', 1), (u'businessman', 1), (u'continent', 1), (u'country', 1), (u'cuisine', 1), (u'culture', 1), (u'five', 1), (u'food', 1), (u'former', 1), (u'lamb', 1), (u'lawyer', 1), (u'museum', 1)]\n",
      "PMI list for word: asian; [(u'asian', 43), (u'central', 7), (u'nation', 4), (u'', 3), (u'famous', 3), (u'restaurants', 3), (u'southeast', 3), (u'countries', 2), (u'country', 2), (u'elephants', 2), (u'artists', 1), (u'bomb', 1), (u'cuisine', 1), (u'culture', 1), (u'deputy', 1), (u'development', 1), (u'east', 1), (u'face', 1), (u'features', 1), (u'gas', 1)]\n",
      "PMI list for word: jewish; [(u'jewish', 53), (u'community', 13), (u'', 4), (u'leaders', 3), (u'american', 2), (u'life', 2), (u'plaza', 2), (u'bible', 1), (u'camp', 1), (u'candidate', 1), (u'center', 1), (u'committee', 1), (u'cooper', 1), (u'creativity', 1), (u'day', 1), (u'dealers', 1), (u'education', 1), (u'families', 1), (u'family', 1), (u'federation', 1)]\n",
      "PMI list for word: latino; [(u'', 1), (u'000', 1), (u'cultural', 1), (u'market', 1), (u'residents', 1), (u' ', 0), (u'-', 0), (u'--', 0), (u'-c', 0), (u'-ft', 0), (u'0', 0), (u'00', 0), (u'000-square-foot', 0), (u'05', 0), (u'1', 0), (u'1-bath', 0), (u'1-car', 0), (u'10', 0), (u'10-6', 0), (u'100', 0)]\n",
      "PMI list for word: mexican; [(u'mexican', 16), (u'', 4), (u'authorities', 4), (u'18-year-old', 1), (u'ambitious', 1), (u'chef', 1), (u'cuisine', 1), (u'felt', 1), (u'gifted', 1), (u'hospital', 1), (u'local', 1), (u'man', 1), (u'masters', 1), (u'regional', 1), (u'restaurants', 1), (u'take', 1), (u'telenovelas', 1), (u'told', 1), (u' ', 0), (u'-', 0)]\n",
      "PMI list for word: russian; [(u'russian', 84), (u'gas', 12), (u'natural', 7), (u'', 5), (u'river', 5), (u'initiative', 3), (u'ally', 2), (u'company', 2), (u'energy', 2), (u'export', 2), (u'foreign', 2), (u'imported', 2), (u'news', 2), (u'pipelines', 2), (u'rivers', 2), (u'television', 2), (u'--', 1), (u'brutal', 1), (u'called', 1), (u'claims', 1)]\n",
      "PMI list for word: american; [(u'american', 440), (u'', 47), (u'military', 22), (u'service', 12), (u'forces', 9), (u'cheese', 7), (u'civil', 7), (u'officials', 7), (u'red', 7), (u'express', 6), (u'football', 6), (u'journal', 6), (u'people', 6), (u'troops', 6), (u'companies', 5), (u'history', 5), (u'museum', 5), (u'north', 5), (u'soldiers', 5), (u'--', 4)]\n",
      "Frequency list: [(u'', 106756), (u'the', 35906), (u'of', 16603), (u'a', 16311), (u'and', 15367), (u'to', 15255), (u'in', 12751), (u'that', 7056), (u's', 6749), (u'for', 5852), (u'is', 5129), (u'on', 4648), (u'it', 4590), (u'was', 4423), (u'he', 4408), (u'with', 4373), (u'said', 4266), (u'at', 3636), (u'as', 3564), (u'mr', 3053)]\n"
     ]
    }
   ],
   "source": [
    "lists = getCorrelationLists(mat_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Document Matrix\n",
    "Creates a word document matrix for use by Theo and her LDA work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def createWordDocumentMatrixNYT(num_files):\n",
    "    result = parse_NYT_articles_worddoc(num_files)\n",
    "    word_file_vec = [(x[0].lower(),x[1]) for x in result[0]]\n",
    "    word_vec = [x[0] for x in word_file_vec]\n",
    "    vocab_vec = np.unique(word_vec).tolist()\n",
    "    file_vec = result[1]\n",
    "    print 'num terms in corpus: ' + str(len(word_vec))\n",
    "    print 'vocab size: ' + str(len(vocab_vec))\n",
    "    print 'matrix dimensions: ' + str(len(vocab_vec)) + ' x ' + str(len(file_vec))\n",
    "    mat = [[0 for x in range(len(file_vec))] for y in range(len(vocab_vec))]\n",
    "    \n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    file_index_dict = {};\n",
    "    for i in range (0, len(file_vec)):\n",
    "        file_index_dict[file_vec[i]] = i\n",
    "    print 'file_index_dict created!'\n",
    "    \n",
    "    i = 0;\n",
    "    for word_file_tuple in word_file_vec:\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(len(word_vec)) + ' terms'\n",
    "        word = word_file_tuple[0]\n",
    "        file_name = word_file_tuple[1]\n",
    "        \n",
    "        # CHANGED\n",
    "        #index_word = vocab_vec.index(word);\n",
    "        index_word = index_dict[word]  \n",
    "        #index_file = file_vec.index(file_name);\n",
    "        index_file = file_index_dict[file_name]\n",
    "        \n",
    "        mat[index_word][index_file] +=1\n",
    "        i = i+1\n",
    "    print 'Parsed all terms'\n",
    "    keep = []\n",
    "    stop = stopwords.words('english')\n",
    "    updated_vocab_vec = []\n",
    "    for ind, word in enumerate(vocab_vec):\n",
    "        if word not in stop:\n",
    "            keep.append(ind)\n",
    "            updated_vocab_vec.append(word)\n",
    "    keep = np.array(mat)[keep]\n",
    "            \n",
    "    return (keep, updated_vocab_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "parsing directory cor-por-a/2006/01/02\n",
      "parsing directory cor-por-a/2006/01/03\n",
      "parsing directory cor-por-a/2006/01/04\n",
      "parsing directory cor-por-a/2006/01/05\n",
      "Num files: 1000\n",
      "num terms in corpus: 723847\n",
      "vocab size: 38922\n",
      "matrix dimensions: 38922 x 1000\n",
      "index_dict created!\n",
      "file_index_dict created!\n",
      "parsed 0/723847 terms\n",
      "parsed 1000/723847 terms\n",
      "parsed 2000/723847 terms\n",
      "parsed 3000/723847 terms\n",
      "parsed 4000/723847 terms\n",
      "parsed 5000/723847 terms\n",
      "parsed 6000/723847 terms\n",
      "parsed 7000/723847 terms\n",
      "parsed 8000/723847 terms\n",
      "parsed 9000/723847 terms\n",
      "parsed 10000/723847 terms\n",
      "parsed 11000/723847 terms\n",
      "parsed 12000/723847 terms\n",
      "parsed 13000/723847 terms\n",
      "parsed 14000/723847 terms\n",
      "parsed 15000/723847 terms\n",
      "parsed 16000/723847 terms\n",
      "parsed 17000/723847 terms\n",
      "parsed 18000/723847 terms\n",
      "parsed 19000/723847 terms\n",
      "parsed 20000/723847 terms\n",
      "parsed 21000/723847 terms\n",
      "parsed 22000/723847 terms\n",
      "parsed 23000/723847 terms\n",
      "parsed 24000/723847 terms\n",
      "parsed 25000/723847 terms\n",
      "parsed 26000/723847 terms\n",
      "parsed 27000/723847 terms\n",
      "parsed 28000/723847 terms\n",
      "parsed 29000/723847 terms\n",
      "parsed 30000/723847 terms\n",
      "parsed 31000/723847 terms\n",
      "parsed 32000/723847 terms\n",
      "parsed 33000/723847 terms\n",
      "parsed 34000/723847 terms\n",
      "parsed 35000/723847 terms\n",
      "parsed 36000/723847 terms\n",
      "parsed 37000/723847 terms\n",
      "parsed 38000/723847 terms\n",
      "parsed 39000/723847 terms\n",
      "parsed 40000/723847 terms\n",
      "parsed 41000/723847 terms\n",
      "parsed 42000/723847 terms\n",
      "parsed 43000/723847 terms\n",
      "parsed 44000/723847 terms\n",
      "parsed 45000/723847 terms\n",
      "parsed 46000/723847 terms\n",
      "parsed 47000/723847 terms\n",
      "parsed 48000/723847 terms\n",
      "parsed 49000/723847 terms\n",
      "parsed 50000/723847 terms\n",
      "parsed 51000/723847 terms\n",
      "parsed 52000/723847 terms\n",
      "parsed 53000/723847 terms\n",
      "parsed 54000/723847 terms\n",
      "parsed 55000/723847 terms\n",
      "parsed 56000/723847 terms\n",
      "parsed 57000/723847 terms\n",
      "parsed 58000/723847 terms\n",
      "parsed 59000/723847 terms\n",
      "parsed 60000/723847 terms\n",
      "parsed 61000/723847 terms\n",
      "parsed 62000/723847 terms\n",
      "parsed 63000/723847 terms\n",
      "parsed 64000/723847 terms\n",
      "parsed 65000/723847 terms\n",
      "parsed 66000/723847 terms\n",
      "parsed 67000/723847 terms\n",
      "parsed 68000/723847 terms\n",
      "parsed 69000/723847 terms\n",
      "parsed 70000/723847 terms\n",
      "parsed 71000/723847 terms\n",
      "parsed 72000/723847 terms\n",
      "parsed 73000/723847 terms\n",
      "parsed 74000/723847 terms\n",
      "parsed 75000/723847 terms\n",
      "parsed 76000/723847 terms\n",
      "parsed 77000/723847 terms\n",
      "parsed 78000/723847 terms\n",
      "parsed 79000/723847 terms\n",
      "parsed 80000/723847 terms\n",
      "parsed 81000/723847 terms\n",
      "parsed 82000/723847 terms\n",
      "parsed 83000/723847 terms\n",
      "parsed 84000/723847 terms\n",
      "parsed 85000/723847 terms\n",
      "parsed 86000/723847 terms\n",
      "parsed 87000/723847 terms\n",
      "parsed 88000/723847 terms\n",
      "parsed 89000/723847 terms\n",
      "parsed 90000/723847 terms\n",
      "parsed 91000/723847 terms\n",
      "parsed 92000/723847 terms\n",
      "parsed 93000/723847 terms\n",
      "parsed 94000/723847 terms\n",
      "parsed 95000/723847 terms\n",
      "parsed 96000/723847 terms\n",
      "parsed 97000/723847 terms\n",
      "parsed 98000/723847 terms\n",
      "parsed 99000/723847 terms\n",
      "parsed 100000/723847 terms\n",
      "parsed 101000/723847 terms\n",
      "parsed 102000/723847 terms\n",
      "parsed 103000/723847 terms\n",
      "parsed 104000/723847 terms\n",
      "parsed 105000/723847 terms\n",
      "parsed 106000/723847 terms\n",
      "parsed 107000/723847 terms\n",
      "parsed 108000/723847 terms\n",
      "parsed 109000/723847 terms\n",
      "parsed 110000/723847 terms\n",
      "parsed 111000/723847 terms\n",
      "parsed 112000/723847 terms\n",
      "parsed 113000/723847 terms\n",
      "parsed 114000/723847 terms\n",
      "parsed 115000/723847 terms\n",
      "parsed 116000/723847 terms\n",
      "parsed 117000/723847 terms\n",
      "parsed 118000/723847 terms\n",
      "parsed 119000/723847 terms\n",
      "parsed 120000/723847 terms\n",
      "parsed 121000/723847 terms\n",
      "parsed 122000/723847 terms\n",
      "parsed 123000/723847 terms\n",
      "parsed 124000/723847 terms\n",
      "parsed 125000/723847 terms\n",
      "parsed 126000/723847 terms\n",
      "parsed 127000/723847 terms\n",
      "parsed 128000/723847 terms\n",
      "parsed 129000/723847 terms\n",
      "parsed 130000/723847 terms\n",
      "parsed 131000/723847 terms\n",
      "parsed 132000/723847 terms\n",
      "parsed 133000/723847 terms\n",
      "parsed 134000/723847 terms\n",
      "parsed 135000/723847 terms\n",
      "parsed 136000/723847 terms\n",
      "parsed 137000/723847 terms\n",
      "parsed 138000/723847 terms\n",
      "parsed 139000/723847 terms\n",
      "parsed 140000/723847 terms\n",
      "parsed 141000/723847 terms\n",
      "parsed 142000/723847 terms\n",
      "parsed 143000/723847 terms\n",
      "parsed 144000/723847 terms\n",
      "parsed 145000/723847 terms\n",
      "parsed 146000/723847 terms\n",
      "parsed 147000/723847 terms\n",
      "parsed 148000/723847 terms\n",
      "parsed 149000/723847 terms\n",
      "parsed 150000/723847 terms\n",
      "parsed 151000/723847 terms\n",
      "parsed 152000/723847 terms\n",
      "parsed 153000/723847 terms\n",
      "parsed 154000/723847 terms\n",
      "parsed 155000/723847 terms\n",
      "parsed 156000/723847 terms\n",
      "parsed 157000/723847 terms\n",
      "parsed 158000/723847 terms\n",
      "parsed 159000/723847 terms\n",
      "parsed 160000/723847 terms\n",
      "parsed 161000/723847 terms\n",
      "parsed 162000/723847 terms\n",
      "parsed 163000/723847 terms\n",
      "parsed 164000/723847 terms\n",
      "parsed 165000/723847 terms\n",
      "parsed 166000/723847 terms\n",
      "parsed 167000/723847 terms\n",
      "parsed 168000/723847 terms\n",
      "parsed 169000/723847 terms\n",
      "parsed 170000/723847 terms\n",
      "parsed 171000/723847 terms\n",
      "parsed 172000/723847 terms\n",
      "parsed 173000/723847 terms\n",
      "parsed 174000/723847 terms\n",
      "parsed 175000/723847 terms\n",
      "parsed 176000/723847 terms\n",
      "parsed 177000/723847 terms\n",
      "parsed 178000/723847 terms\n",
      "parsed 179000/723847 terms\n",
      "parsed 180000/723847 terms\n",
      "parsed 181000/723847 terms\n",
      "parsed 182000/723847 terms\n",
      "parsed 183000/723847 terms\n",
      "parsed 184000/723847 terms\n",
      "parsed 185000/723847 terms\n",
      "parsed 186000/723847 terms\n",
      "parsed 187000/723847 terms\n",
      "parsed 188000/723847 terms\n",
      "parsed 189000/723847 terms\n",
      "parsed 190000/723847 terms\n",
      "parsed 191000/723847 terms\n",
      "parsed 192000/723847 terms\n",
      "parsed 193000/723847 terms\n",
      "parsed 194000/723847 terms\n",
      "parsed 195000/723847 terms\n",
      "parsed 196000/723847 terms\n",
      "parsed 197000/723847 terms\n",
      "parsed 198000/723847 terms\n",
      "parsed 199000/723847 terms\n",
      "parsed 200000/723847 terms\n",
      "parsed 201000/723847 terms\n",
      "parsed 202000/723847 terms\n",
      "parsed 203000/723847 terms\n",
      "parsed 204000/723847 terms\n",
      "parsed 205000/723847 terms\n",
      "parsed 206000/723847 terms\n",
      "parsed 207000/723847 terms\n",
      "parsed 208000/723847 terms\n",
      "parsed 209000/723847 terms\n",
      "parsed 210000/723847 terms\n",
      "parsed 211000/723847 terms\n",
      "parsed 212000/723847 terms\n",
      "parsed 213000/723847 terms\n",
      "parsed 214000/723847 terms\n",
      "parsed 215000/723847 terms\n",
      "parsed 216000/723847 terms\n",
      "parsed 217000/723847 terms\n",
      "parsed 218000/723847 terms\n",
      "parsed 219000/723847 terms\n",
      "parsed 220000/723847 terms\n",
      "parsed 221000/723847 terms\n",
      "parsed 222000/723847 terms\n",
      "parsed 223000/723847 terms\n",
      "parsed 224000/723847 terms\n",
      "parsed 225000/723847 terms\n",
      "parsed 226000/723847 terms\n",
      "parsed 227000/723847 terms\n",
      "parsed 228000/723847 terms\n",
      "parsed 229000/723847 terms\n",
      "parsed 230000/723847 terms\n",
      "parsed 231000/723847 terms\n",
      "parsed 232000/723847 terms\n",
      "parsed 233000/723847 terms\n",
      "parsed 234000/723847 terms\n",
      "parsed 235000/723847 terms\n",
      "parsed 236000/723847 terms\n",
      "parsed 237000/723847 terms\n",
      "parsed 238000/723847 terms\n",
      "parsed 239000/723847 terms\n",
      "parsed 240000/723847 terms\n",
      "parsed 241000/723847 terms\n",
      "parsed 242000/723847 terms\n",
      "parsed 243000/723847 terms\n",
      "parsed 244000/723847 terms\n",
      "parsed 245000/723847 terms\n",
      "parsed 246000/723847 terms\n",
      "parsed 247000/723847 terms\n",
      "parsed 248000/723847 terms\n",
      "parsed 249000/723847 terms\n",
      "parsed 250000/723847 terms\n",
      "parsed 251000/723847 terms\n",
      "parsed 252000/723847 terms\n",
      "parsed 253000/723847 terms\n",
      "parsed 254000/723847 terms\n",
      "parsed 255000/723847 terms\n",
      "parsed 256000/723847 terms\n",
      "parsed 257000/723847 terms\n",
      "parsed 258000/723847 terms\n",
      "parsed 259000/723847 terms\n",
      "parsed 260000/723847 terms\n",
      "parsed 261000/723847 terms\n",
      "parsed 262000/723847 terms\n",
      "parsed 263000/723847 terms\n",
      "parsed 264000/723847 terms\n",
      "parsed 265000/723847 terms\n",
      "parsed 266000/723847 terms\n",
      "parsed 267000/723847 terms\n",
      "parsed 268000/723847 terms\n",
      "parsed 269000/723847 terms\n",
      "parsed 270000/723847 terms\n",
      "parsed 271000/723847 terms\n",
      "parsed 272000/723847 terms\n",
      "parsed 273000/723847 terms\n",
      "parsed 274000/723847 terms\n",
      "parsed 275000/723847 terms\n",
      "parsed 276000/723847 terms\n",
      "parsed 277000/723847 terms\n",
      "parsed 278000/723847 terms\n",
      "parsed 279000/723847 terms\n",
      "parsed 280000/723847 terms\n",
      "parsed 281000/723847 terms\n",
      "parsed 282000/723847 terms\n",
      "parsed 283000/723847 terms\n",
      "parsed 284000/723847 terms\n",
      "parsed 285000/723847 terms\n",
      "parsed 286000/723847 terms\n",
      "parsed 287000/723847 terms\n",
      "parsed 288000/723847 terms\n",
      "parsed 289000/723847 terms\n",
      "parsed 290000/723847 terms\n",
      "parsed 291000/723847 terms\n",
      "parsed 292000/723847 terms\n",
      "parsed 293000/723847 terms\n",
      "parsed 294000/723847 terms\n",
      "parsed 295000/723847 terms\n",
      "parsed 296000/723847 terms\n",
      "parsed 297000/723847 terms\n",
      "parsed 298000/723847 terms\n",
      "parsed 299000/723847 terms\n",
      "parsed 300000/723847 terms\n",
      "parsed 301000/723847 terms\n",
      "parsed 302000/723847 terms\n",
      "parsed 303000/723847 terms\n",
      "parsed 304000/723847 terms\n",
      "parsed 305000/723847 terms\n",
      "parsed 306000/723847 terms\n",
      "parsed 307000/723847 terms\n",
      "parsed 308000/723847 terms\n",
      "parsed 309000/723847 terms\n",
      "parsed 310000/723847 terms\n",
      "parsed 311000/723847 terms\n",
      "parsed 312000/723847 terms\n",
      "parsed 313000/723847 terms\n",
      "parsed 314000/723847 terms\n",
      "parsed 315000/723847 terms\n",
      "parsed 316000/723847 terms\n",
      "parsed 317000/723847 terms\n",
      "parsed 318000/723847 terms\n",
      "parsed 319000/723847 terms\n",
      "parsed 320000/723847 terms\n",
      "parsed 321000/723847 terms\n",
      "parsed 322000/723847 terms\n",
      "parsed 323000/723847 terms\n",
      "parsed 324000/723847 terms\n",
      "parsed 325000/723847 terms\n",
      "parsed 326000/723847 terms\n",
      "parsed 327000/723847 terms\n",
      "parsed 328000/723847 terms\n",
      "parsed 329000/723847 terms\n",
      "parsed 330000/723847 terms\n",
      "parsed 331000/723847 terms\n",
      "parsed 332000/723847 terms\n",
      "parsed 333000/723847 terms\n",
      "parsed 334000/723847 terms\n",
      "parsed 335000/723847 terms\n",
      "parsed 336000/723847 terms\n",
      "parsed 337000/723847 terms\n",
      "parsed 338000/723847 terms\n",
      "parsed 339000/723847 terms\n",
      "parsed 340000/723847 terms\n",
      "parsed 341000/723847 terms\n",
      "parsed 342000/723847 terms\n",
      "parsed 343000/723847 terms\n",
      "parsed 344000/723847 terms\n",
      "parsed 345000/723847 terms\n",
      "parsed 346000/723847 terms\n",
      "parsed 347000/723847 terms\n",
      "parsed 348000/723847 terms\n",
      "parsed 349000/723847 terms\n",
      "parsed 350000/723847 terms\n",
      "parsed 351000/723847 terms\n",
      "parsed 352000/723847 terms\n",
      "parsed 353000/723847 terms\n",
      "parsed 354000/723847 terms\n",
      "parsed 355000/723847 terms\n",
      "parsed 356000/723847 terms\n",
      "parsed 357000/723847 terms\n",
      "parsed 358000/723847 terms\n",
      "parsed 359000/723847 terms\n",
      "parsed 360000/723847 terms\n",
      "parsed 361000/723847 terms\n",
      "parsed 362000/723847 terms\n",
      "parsed 363000/723847 terms\n",
      "parsed 364000/723847 terms\n",
      "parsed 365000/723847 terms\n",
      "parsed 366000/723847 terms\n",
      "parsed 367000/723847 terms\n",
      "parsed 368000/723847 terms\n",
      "parsed 369000/723847 terms\n",
      "parsed 370000/723847 terms\n",
      "parsed 371000/723847 terms\n",
      "parsed 372000/723847 terms\n",
      "parsed 373000/723847 terms\n",
      "parsed 374000/723847 terms\n",
      "parsed 375000/723847 terms\n",
      "parsed 376000/723847 terms\n",
      "parsed 377000/723847 terms\n",
      "parsed 378000/723847 terms\n",
      "parsed 379000/723847 terms\n",
      "parsed 380000/723847 terms\n",
      "parsed 381000/723847 terms\n",
      "parsed 382000/723847 terms\n",
      "parsed 383000/723847 terms\n",
      "parsed 384000/723847 terms\n",
      "parsed 385000/723847 terms\n",
      "parsed 386000/723847 terms\n",
      "parsed 387000/723847 terms\n",
      "parsed 388000/723847 terms\n",
      "parsed 389000/723847 terms\n",
      "parsed 390000/723847 terms\n",
      "parsed 391000/723847 terms\n",
      "parsed 392000/723847 terms\n",
      "parsed 393000/723847 terms\n",
      "parsed 394000/723847 terms\n",
      "parsed 395000/723847 terms\n",
      "parsed 396000/723847 terms\n",
      "parsed 397000/723847 terms\n",
      "parsed 398000/723847 terms\n",
      "parsed 399000/723847 terms\n",
      "parsed 400000/723847 terms\n",
      "parsed 401000/723847 terms\n",
      "parsed 402000/723847 terms\n",
      "parsed 403000/723847 terms\n",
      "parsed 404000/723847 terms\n",
      "parsed 405000/723847 terms\n",
      "parsed 406000/723847 terms\n",
      "parsed 407000/723847 terms\n",
      "parsed 408000/723847 terms\n",
      "parsed 409000/723847 terms\n",
      "parsed 410000/723847 terms\n",
      "parsed 411000/723847 terms\n",
      "parsed 412000/723847 terms\n",
      "parsed 413000/723847 terms\n",
      "parsed 414000/723847 terms\n",
      "parsed 415000/723847 terms\n",
      "parsed 416000/723847 terms\n",
      "parsed 417000/723847 terms\n",
      "parsed 418000/723847 terms\n",
      "parsed 419000/723847 terms\n",
      "parsed 420000/723847 terms\n",
      "parsed 421000/723847 terms\n",
      "parsed 422000/723847 terms\n",
      "parsed 423000/723847 terms\n",
      "parsed 424000/723847 terms\n",
      "parsed 425000/723847 terms\n",
      "parsed 426000/723847 terms\n",
      "parsed 427000/723847 terms\n",
      "parsed 428000/723847 terms\n",
      "parsed 429000/723847 terms\n",
      "parsed 430000/723847 terms\n",
      "parsed 431000/723847 terms\n",
      "parsed 432000/723847 terms\n",
      "parsed 433000/723847 terms\n",
      "parsed 434000/723847 terms\n",
      "parsed 435000/723847 terms\n",
      "parsed 436000/723847 terms\n",
      "parsed 437000/723847 terms\n",
      "parsed 438000/723847 terms\n",
      "parsed 439000/723847 terms\n",
      "parsed 440000/723847 terms\n",
      "parsed 441000/723847 terms\n",
      "parsed 442000/723847 terms\n",
      "parsed 443000/723847 terms\n",
      "parsed 444000/723847 terms\n",
      "parsed 445000/723847 terms\n",
      "parsed 446000/723847 terms\n",
      "parsed 447000/723847 terms\n",
      "parsed 448000/723847 terms\n",
      "parsed 449000/723847 terms\n",
      "parsed 450000/723847 terms\n",
      "parsed 451000/723847 terms\n",
      "parsed 452000/723847 terms\n",
      "parsed 453000/723847 terms\n",
      "parsed 454000/723847 terms\n",
      "parsed 455000/723847 terms\n",
      "parsed 456000/723847 terms\n",
      "parsed 457000/723847 terms\n",
      "parsed 458000/723847 terms\n",
      "parsed 459000/723847 terms\n",
      "parsed 460000/723847 terms\n",
      "parsed 461000/723847 terms\n",
      "parsed 462000/723847 terms\n",
      "parsed 463000/723847 terms\n",
      "parsed 464000/723847 terms\n",
      "parsed 465000/723847 terms\n",
      "parsed 466000/723847 terms\n",
      "parsed 467000/723847 terms\n",
      "parsed 468000/723847 terms\n",
      "parsed 469000/723847 terms\n",
      "parsed 470000/723847 terms\n",
      "parsed 471000/723847 terms\n",
      "parsed 472000/723847 terms\n",
      "parsed 473000/723847 terms\n",
      "parsed 474000/723847 terms\n",
      "parsed 475000/723847 terms\n",
      "parsed 476000/723847 terms\n",
      "parsed 477000/723847 terms\n",
      "parsed 478000/723847 terms\n",
      "parsed 479000/723847 terms\n",
      "parsed 480000/723847 terms\n",
      "parsed 481000/723847 terms\n",
      "parsed 482000/723847 terms\n",
      "parsed 483000/723847 terms\n",
      "parsed 484000/723847 terms\n",
      "parsed 485000/723847 terms\n",
      "parsed 486000/723847 terms\n",
      "parsed 487000/723847 terms\n",
      "parsed 488000/723847 terms\n",
      "parsed 489000/723847 terms\n",
      "parsed 490000/723847 terms\n",
      "parsed 491000/723847 terms\n",
      "parsed 492000/723847 terms\n",
      "parsed 493000/723847 terms\n",
      "parsed 494000/723847 terms\n",
      "parsed 495000/723847 terms\n",
      "parsed 496000/723847 terms\n",
      "parsed 497000/723847 terms\n",
      "parsed 498000/723847 terms\n",
      "parsed 499000/723847 terms\n",
      "parsed 500000/723847 terms\n",
      "parsed 501000/723847 terms\n",
      "parsed 502000/723847 terms\n",
      "parsed 503000/723847 terms\n",
      "parsed 504000/723847 terms\n",
      "parsed 505000/723847 terms\n",
      "parsed 506000/723847 terms\n",
      "parsed 507000/723847 terms\n",
      "parsed 508000/723847 terms\n",
      "parsed 509000/723847 terms\n",
      "parsed 510000/723847 terms\n",
      "parsed 511000/723847 terms\n",
      "parsed 512000/723847 terms\n",
      "parsed 513000/723847 terms\n",
      "parsed 514000/723847 terms\n",
      "parsed 515000/723847 terms\n",
      "parsed 516000/723847 terms\n",
      "parsed 517000/723847 terms\n",
      "parsed 518000/723847 terms\n",
      "parsed 519000/723847 terms\n",
      "parsed 520000/723847 terms\n",
      "parsed 521000/723847 terms\n",
      "parsed 522000/723847 terms\n",
      "parsed 523000/723847 terms\n",
      "parsed 524000/723847 terms\n",
      "parsed 525000/723847 terms\n",
      "parsed 526000/723847 terms\n",
      "parsed 527000/723847 terms\n",
      "parsed 528000/723847 terms\n",
      "parsed 529000/723847 terms\n",
      "parsed 530000/723847 terms\n",
      "parsed 531000/723847 terms\n",
      "parsed 532000/723847 terms\n",
      "parsed 533000/723847 terms\n",
      "parsed 534000/723847 terms\n",
      "parsed 535000/723847 terms\n",
      "parsed 536000/723847 terms\n",
      "parsed 537000/723847 terms\n",
      "parsed 538000/723847 terms\n",
      "parsed 539000/723847 terms\n",
      "parsed 540000/723847 terms\n",
      "parsed 541000/723847 terms\n",
      "parsed 542000/723847 terms\n",
      "parsed 543000/723847 terms\n",
      "parsed 544000/723847 terms\n",
      "parsed 545000/723847 terms\n",
      "parsed 546000/723847 terms\n",
      "parsed 547000/723847 terms\n",
      "parsed 548000/723847 terms\n",
      "parsed 549000/723847 terms\n",
      "parsed 550000/723847 terms\n",
      "parsed 551000/723847 terms\n",
      "parsed 552000/723847 terms\n",
      "parsed 553000/723847 terms\n",
      "parsed 554000/723847 terms\n",
      "parsed 555000/723847 terms\n",
      "parsed 556000/723847 terms\n",
      "parsed 557000/723847 terms\n",
      "parsed 558000/723847 terms\n",
      "parsed 559000/723847 terms\n",
      "parsed 560000/723847 terms\n",
      "parsed 561000/723847 terms\n",
      "parsed 562000/723847 terms\n",
      "parsed 563000/723847 terms\n",
      "parsed 564000/723847 terms\n",
      "parsed 565000/723847 terms\n",
      "parsed 566000/723847 terms\n",
      "parsed 567000/723847 terms\n",
      "parsed 568000/723847 terms\n",
      "parsed 569000/723847 terms\n",
      "parsed 570000/723847 terms\n",
      "parsed 571000/723847 terms\n",
      "parsed 572000/723847 terms\n",
      "parsed 573000/723847 terms\n",
      "parsed 574000/723847 terms\n",
      "parsed 575000/723847 terms\n",
      "parsed 576000/723847 terms\n",
      "parsed 577000/723847 terms\n",
      "parsed 578000/723847 terms\n",
      "parsed 579000/723847 terms\n",
      "parsed 580000/723847 terms\n",
      "parsed 581000/723847 terms\n",
      "parsed 582000/723847 terms\n",
      "parsed 583000/723847 terms\n",
      "parsed 584000/723847 terms\n",
      "parsed 585000/723847 terms\n",
      "parsed 586000/723847 terms\n",
      "parsed 587000/723847 terms\n",
      "parsed 588000/723847 terms\n",
      "parsed 589000/723847 terms\n",
      "parsed 590000/723847 terms\n",
      "parsed 591000/723847 terms\n",
      "parsed 592000/723847 terms\n",
      "parsed 593000/723847 terms\n",
      "parsed 594000/723847 terms\n",
      "parsed 595000/723847 terms\n",
      "parsed 596000/723847 terms\n",
      "parsed 597000/723847 terms\n",
      "parsed 598000/723847 terms\n",
      "parsed 599000/723847 terms\n",
      "parsed 600000/723847 terms\n",
      "parsed 601000/723847 terms\n",
      "parsed 602000/723847 terms\n",
      "parsed 603000/723847 terms\n",
      "parsed 604000/723847 terms\n",
      "parsed 605000/723847 terms\n",
      "parsed 606000/723847 terms\n",
      "parsed 607000/723847 terms\n",
      "parsed 608000/723847 terms\n",
      "parsed 609000/723847 terms\n",
      "parsed 610000/723847 terms\n",
      "parsed 611000/723847 terms\n",
      "parsed 612000/723847 terms\n",
      "parsed 613000/723847 terms\n",
      "parsed 614000/723847 terms\n",
      "parsed 615000/723847 terms\n",
      "parsed 616000/723847 terms\n",
      "parsed 617000/723847 terms\n",
      "parsed 618000/723847 terms\n",
      "parsed 619000/723847 terms\n",
      "parsed 620000/723847 terms\n",
      "parsed 621000/723847 terms\n",
      "parsed 622000/723847 terms\n",
      "parsed 623000/723847 terms\n",
      "parsed 624000/723847 terms\n",
      "parsed 625000/723847 terms\n",
      "parsed 626000/723847 terms\n",
      "parsed 627000/723847 terms\n",
      "parsed 628000/723847 terms\n",
      "parsed 629000/723847 terms\n",
      "parsed 630000/723847 terms\n",
      "parsed 631000/723847 terms\n",
      "parsed 632000/723847 terms\n",
      "parsed 633000/723847 terms\n",
      "parsed 634000/723847 terms\n",
      "parsed 635000/723847 terms\n",
      "parsed 636000/723847 terms\n",
      "parsed 637000/723847 terms\n",
      "parsed 638000/723847 terms\n",
      "parsed 639000/723847 terms\n",
      "parsed 640000/723847 terms\n",
      "parsed 641000/723847 terms\n",
      "parsed 642000/723847 terms\n",
      "parsed 643000/723847 terms\n",
      "parsed 644000/723847 terms\n",
      "parsed 645000/723847 terms\n",
      "parsed 646000/723847 terms\n",
      "parsed 647000/723847 terms\n",
      "parsed 648000/723847 terms\n",
      "parsed 649000/723847 terms\n",
      "parsed 650000/723847 terms\n",
      "parsed 651000/723847 terms\n",
      "parsed 652000/723847 terms\n",
      "parsed 653000/723847 terms\n",
      "parsed 654000/723847 terms\n",
      "parsed 655000/723847 terms\n",
      "parsed 656000/723847 terms\n",
      "parsed 657000/723847 terms\n",
      "parsed 658000/723847 terms\n",
      "parsed 659000/723847 terms\n",
      "parsed 660000/723847 terms\n",
      "parsed 661000/723847 terms\n",
      "parsed 662000/723847 terms\n",
      "parsed 663000/723847 terms\n",
      "parsed 664000/723847 terms\n",
      "parsed 665000/723847 terms\n",
      "parsed 666000/723847 terms\n",
      "parsed 667000/723847 terms\n",
      "parsed 668000/723847 terms\n",
      "parsed 669000/723847 terms\n",
      "parsed 670000/723847 terms\n",
      "parsed 671000/723847 terms\n",
      "parsed 672000/723847 terms\n",
      "parsed 673000/723847 terms\n",
      "parsed 674000/723847 terms\n",
      "parsed 675000/723847 terms\n",
      "parsed 676000/723847 terms\n",
      "parsed 677000/723847 terms\n",
      "parsed 678000/723847 terms\n",
      "parsed 679000/723847 terms\n",
      "parsed 680000/723847 terms\n",
      "parsed 681000/723847 terms\n",
      "parsed 682000/723847 terms\n",
      "parsed 683000/723847 terms\n",
      "parsed 684000/723847 terms\n",
      "parsed 685000/723847 terms\n",
      "parsed 686000/723847 terms\n",
      "parsed 687000/723847 terms\n",
      "parsed 688000/723847 terms\n",
      "parsed 689000/723847 terms\n",
      "parsed 690000/723847 terms\n",
      "parsed 691000/723847 terms\n",
      "parsed 692000/723847 terms\n",
      "parsed 693000/723847 terms\n",
      "parsed 694000/723847 terms\n",
      "parsed 695000/723847 terms\n",
      "parsed 696000/723847 terms\n",
      "parsed 697000/723847 terms\n",
      "parsed 698000/723847 terms\n",
      "parsed 699000/723847 terms\n",
      "parsed 700000/723847 terms\n",
      "parsed 701000/723847 terms\n",
      "parsed 702000/723847 terms\n",
      "parsed 703000/723847 terms\n",
      "parsed 704000/723847 terms\n",
      "parsed 705000/723847 terms\n",
      "parsed 706000/723847 terms\n",
      "parsed 707000/723847 terms\n",
      "parsed 708000/723847 terms\n",
      "parsed 709000/723847 terms\n",
      "parsed 710000/723847 terms\n",
      "parsed 711000/723847 terms\n",
      "parsed 712000/723847 terms\n",
      "parsed 713000/723847 terms\n",
      "parsed 714000/723847 terms\n",
      "parsed 715000/723847 terms\n",
      "parsed 716000/723847 terms\n",
      "parsed 717000/723847 terms\n",
      "parsed 718000/723847 terms\n",
      "parsed 719000/723847 terms\n",
      "parsed 720000/723847 terms\n",
      "parsed 721000/723847 terms\n",
      "parsed 722000/723847 terms\n",
      "parsed 723000/723847 terms\n",
      "Parsed all terms\n"
     ]
    }
   ],
   "source": [
    "(mat, lda_vocab) = createWordDocumentMatrixNYT(num_files=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "Takes in a list V of words and returns the average sentiment score across all terms in V as determined by freebase. Note to Jason: consider other sentiment databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlated_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7b768ec0c092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#print generate_sentiment_2(neighbors_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgenerate_sentiment_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelated_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correlated_words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def getSentiment(word):\n",
    "    synset = list(swn.senti_synsets(word))\n",
    "    if len(synset) > 0: #if a synset exists for this word\n",
    "        synset = synset[0]\n",
    "        return(synset.pos_score(), synset.neg_score(), synset.obj_score())\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "V = ['good', 'bad', 'great', 'awesome', 'amazing', 'holy', 'beautiful', 'worrisome', 'stupid']\n",
    "def generate_sentiment(wordList):\n",
    "    totalSentiment = 0.0;\n",
    "    for word in wordList:\n",
    "        if is_ascii(word): #see note below for rationale\n",
    "            sentiment = getSentiment(word)\n",
    "            if sentiment == None:\n",
    "                sentiment = 0.0\n",
    "            if type(sentiment) is float: #why does this happen\n",
    "                print \"n/a\"\n",
    "            else:  \n",
    "                totalSentiment += (sentiment[0] - sentiment[1]) \n",
    "                print (sentiment[0] - sentiment[1])\n",
    "        #sentiwordnet generates tuples of pos, neg, and neu. currently naively choosing to consider only sum of pos and neg. \n",
    "    averageSentiment = totalSentiment/len(wordList)\n",
    "    return averageSentiment\n",
    "\n",
    "def generate_sentiment_2(wordTupleList):\n",
    "    reader = csv.reader(open('sentiment_words.txt', 'rb'))\n",
    "    sentiment_words = dict(reader)\n",
    "    sentiment_score = 0\n",
    "    for wordTuple in wordTupleList:\n",
    "        word = wordTuple[0]\n",
    "        score = 1/wordTuple[1] #inverse of distance\n",
    "        if word in sentiment_words:\n",
    "            if sentiment_words[word] == 'pos':\n",
    "                print word + \" +\" + str(score)\n",
    "                sentiment_score += score\n",
    "            if sentiment_words[word] == 'neg':\n",
    "                print word + \" -\" + str(score)\n",
    "                sentiment_score -= score\n",
    "    return sentiment_score\n",
    "\n",
    "#print generate_sentiment_2(neighbors_list)\n",
    "print generate_sentiment_2(correlated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parser_wrapper(year):\n",
    "    num_files = 1000000000 #all files\n",
    "    root_directory = 'cor-por-a/'+ str(year) + '/'\n",
    "    word2vec = parse_NYT_articles_word2vec(num_files, root_directory)\n",
    "    seedword = parse_NYT_articles_seedword(num_files, root_directory)\n",
    "    worddoc = parse_NYT_articles_worddoc(num_files, root_directory)\n",
    "    return (word2vec, seedword, worddoc)\n",
    "def process_year(year):\n",
    "    parses = parser_wrapper(year) #[0]=word2vec, [1]=seedword, [2]=worddoc\n",
    "    #make matrices\n",
    "    #get correlation lists\n",
    "    #make word2vec VSM\n",
    "    #get sentiment of correlation list\n",
    "    #get lda topics \n",
    "    #print out and save / year\n",
    "    #graph across years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim example ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def parse_NYT_articles_word2vec(num_files, root_directory='cor-por-a/2006/'):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') #pretrained probabilistic model for parsing sentences\n",
    "    \n",
    "    sentenceList = [];\n",
    "    i = 0;\n",
    "    numSentences = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):                \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    print \"num sentences: \" + str(numSentences)\n",
    "                    return sentenceList\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                #print article_file\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article = article_rep[1];\n",
    "                    for sentence in sent_detector.tokenize(article.strip()):\n",
    "                        tokenized_sentence = []\n",
    "                        sentence = remove_punctuation(sentence.lower())\n",
    "                        for word in sentence.split():\n",
    "                            tokenized_sentence.append(word)\n",
    "                        sentenceList.append(tokenized_sentence)\n",
    "                        numSentences += 1\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i);\n",
    "    print \"num sentences: \" + str(numSentences)\n",
    "    return sentenceList;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "parsing directory cor-por-a/2006/01/02\n",
      "parsing directory cor-por-a/2006/01/03\n",
      "parsing directory cor-por-a/2006/01/04\n",
      "parsing directory cor-por-a/2006/01/05\n",
      "parsing directory cor-por-a/2006/01/06\n",
      "parsing directory cor-por-a/2006/01/07\n",
      "parsing directory cor-por-a/2006/01/08\n",
      "parsing directory cor-por-a/2006/01/09\n",
      "parsing directory cor-por-a/2006/01/10\n",
      "parsing directory cor-por-a/2006/01/11\n",
      "parsing directory cor-por-a/2006/01/12\n",
      "parsing directory cor-por-a/2006/01/13\n",
      "parsing directory cor-por-a/2006/01/14\n",
      "parsing directory cor-por-a/2006/01/15\n",
      "parsing directory cor-por-a/2006/01/16\n",
      "parsing directory cor-por-a/2006/01/17\n",
      "parsing directory cor-por-a/2006/01/18\n",
      "parsing directory cor-por-a/2006/01/19\n",
      "parsing directory cor-por-a/2006/01/20\n",
      "parsing directory cor-por-a/2006/01/21\n",
      "parsing directory cor-por-a/2006/01/22\n",
      "parsing directory cor-por-a/2006/01/23\n",
      "parsing directory cor-por-a/2006/01/24\n",
      "parsing directory cor-por-a/2006/01/25\n",
      "parsing directory cor-por-a/2006/01/26\n",
      "parsing directory cor-por-a/2006/01/27\n",
      "parsing directory cor-por-a/2006/01/28\n",
      "parsing directory cor-por-a/2006/01/29\n",
      "parsing directory cor-por-a/2006/01/30\n",
      "parsing directory cor-por-a/2006/01/31\n",
      "parsing outer file directory 02\n",
      "parsing directory cor-por-a/2006/02/01\n",
      "parsing directory cor-por-a/2006/02/02\n",
      "parsing directory cor-por-a/2006/02/03\n",
      "parsing directory cor-por-a/2006/02/04\n",
      "parsing directory cor-por-a/2006/02/05\n",
      "parsing directory cor-por-a/2006/02/06\n",
      "parsing directory cor-por-a/2006/02/07\n",
      "parsing directory cor-por-a/2006/02/08\n",
      "parsing directory cor-por-a/2006/02/09\n",
      "parsing directory cor-por-a/2006/02/10\n",
      "Num files: 10000\n",
      "num sentences: 223623\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "# mr = Word2Vec(movie_reviews.sents())\n",
    "# t = Word2Vec(treebank.sents())\n",
    "\n",
    "parse = parse_NYT_articles_word2vec(10000)\n",
    "ours = Word2Vec(parse, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('breeders', 0.8067673444747925), (u'medal', 0.7972220778465271), ('bronze', 0.77171790599823), ('cup', 0.7609206438064575), ('medals', 0.7460742592811584)]\n"
     ]
    }
   ],
   "source": [
    "# print(b.most_similar('money', topn=5))\n",
    "# print(t.most_similar('money', topn=5))\n",
    "print(ours.most_similar('gold', topn=5))\n",
    "\n",
    "#train gensim to generate a vector representation of all words in vocabulary\n",
    "#create new matrix by getting ours['vocab_word'] for all vocab words\n",
    "#pass that new matrix into the semantic orientation model to get sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word2vec_mat():\n",
    "    #get vocabulary list\n",
    "    #create new vocab list\n",
    "    #create new mat\n",
    "    #for each word in vocab list\n",
    "    #get its raw vector and append to new mat, append word to new vocab list\n",
    "    #return (new mat, new vocab list)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_orientation(\n",
    "        mat, \n",
    "        rownames,\n",
    "        seeds1=('bad', 'nasty', 'poor', 'negative', 'unfortunate', 'wrong', 'inferior'),\n",
    "        seeds2=('good', 'nice', 'excellent', 'positive', 'fortunate', 'correct', 'superior'),\n",
    "        distfunc=cosine):    \n",
    "    \"\"\"No frills implementation of the semantic Orientation (SO) method of \n",
    "    Turney and Littman. seeds1 and seeds2 should be representative members \n",
    "    of two intutively opposing semantic classes. The method will then try \n",
    "    to rank the vocabulary by its relative association with each seed set.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : 2d np.array\n",
    "        The matrix used to derive the SO ranking.\n",
    "        \n",
    "    rownames : list of str\n",
    "        The names of the rows of `mat` (the vocabulary).\n",
    "        \n",
    "    seeds1 : tuple of str\n",
    "        The default is the negative seed set of Turney and Littman.\n",
    "        \n",
    "    seeds2 : tuple of str\n",
    "        The default is the positive seed set of Turney and Littman.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors. \n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    list of tuples\n",
    "        The vocabulary ranked according to the SO method, with words \n",
    "        closest to `seeds1` at the top and words closest to `seeds2` at the \n",
    "        bottom. Each member of the list is a (word, score) pair.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sm1 = _so_seed_matrix(seeds1, mat, rownames)\n",
    "    sm2 = _so_seed_matrix(seeds2, mat, rownames)\n",
    "    scores = [(rownames[i], _so_row_func(mat[i], sm1, sm2, distfunc)) for i in xrange(len(mat))]\n",
    "    return sorted(scores, key=itemgetter(1), reverse=False)\n",
    "\n",
    "def _so_seed_matrix(seeds, mat, rownames):\n",
    "    indices = [rownames.index(word) for word in seeds if word in rownames]\n",
    "    if not indices:\n",
    "        raise ValueError('The matrix contains no members of the seed set: %s' % \",\".join(seeds))\n",
    "    print indices\n",
    "    print np.array(indices)\n",
    "    return mat[np.array(indices)]\n",
    "    \n",
    "def _so_row_func(row, sm1, sm2, distfunc):\n",
    "    val1 = np.sum([distfunc(row, srow) for srow in sm1])\n",
    "    val2 = np.sum([distfunc(row, srow) for srow in sm2])\n",
    "    return val1 - val2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we tokenize ignoring punctuation?\n",
    "\n",
    "Solution: for each word, look at last letter, if it is in a set of punctuation, remove that punctuation. Or, just strip away punctuation from the entire text in the very beginning. We're losing some degree of information but it is essentially a way of \"normalizing\" the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_rowshit = [u'!', u'):', u');', u'1', u'1/10', u'1/2', u'10', u'10/10', u'100', u'11', u'12', u'13', u'14', u'15', u'17', u'1950', u'1950s', u'1970', u'1980', u'2', u'20', u'2000', u'25', u'3', u'3/10', u'30', u'4', u'4/10', u'40', u'5', u'50', u'6', u'60', u'60s', u'7', u'7/10', u'70', u'70s', u'8', u'8/', u'80', u'80s', u'9', u'90', u':)', u'?', u'a', u'abandoned', u'ability', u'able', u'about', u'above', u'absolute', u'absolutely', u'absurd', u'abuse', u'academy', u'accent', u'accents', u'accept', u'accident', u'accidentally', u'according', u'account', u'accurate', u'achieve', u'across', u'act', u'acted', u'acting', u'action', u'actions', u'actor', u'actors', u'actress', u'actresses', u'acts', u'actual', u'actually', u'adam', u'adaptation', u'add', u'added', u'adding', u'addition', u'adds', u'admit', u'adult', u'adults', u'adventure', u'adventures', u'advice', u'affair', u'afraid', u'africa', u'african', u'after', u'afternoon', u'again', u'against', u'age', u'agent', u'ages', u'ago', u'agree', u'ahead', u\"ain't\", u'air', u'aka', u'al', u'alan', u'alas', u'albert', u'alex', u'alice', u'alien', u'aliens', u'alive', u'all', u'allen', u'allow', u'allowed', u'allows', u'almost', u'alone', u'along', u'already', u'alright', u'also', u'although', u'always', u'am', u'amateur', u'amateurish', u'amazed', u'amazing', u'amazingly', u'america', u'american', u'americans', u'among', u'amongst', u'amount', u'amusing', u'an', u'ancient', u'and', u'anderson', u'andy', u'angel', u'angels', u'anger', u'angle', u'angles', u'angry', u'animal', u'animals', u'animated', u'animation', u'anime', u'ann', u'anna', u'anne', u'annoying', u'another', u'answer', u'answers', u'anthony', u'any', u'anybody', u'anymore', u'anyone', u'anything', u'anyway', u'anywhere', u'apart', u'apartment', u'apparent', u'apparently', u'appeal', u'appealing', u'appear', u'appearance', u'appeared', u'appears', u'appreciate', u'appreciated', u'approach', u'appropriate', u'are', u'area', u\"aren't\", u'arms', u'army', u'around', u'arrives', u'art', u'arthur', u'artist', u'artistic', u'artists', u'arts', u'as', u'ashamed', u'asian', u'aside', u'ask', u'asked', u'asking', u'asks', u'asleep', u'aspect', u'aspects', u'ass', u'assume', u'at', u'atmosphere', u'atrocious', u'attack', u'attacked', u'attacks', u'attempt', u'attempting', u'attempts', u'attention', u'attitude', u'attractive', u'audience', u'audiences', u'aunt', u'australian', u'authentic', u'author', u'available', u'average', u'avoid', u'award', u'awards', u'aware', u'away', u'awesome', u'awful', u'awkward', u'b', u'b-movie', u'baby', u'back', u'background', u'bad', u'badly', u'balance', u'ball', u'band', u'bank', u'bar', u'barbara', u'barely', u'base', u'baseball', u'based', u'basic', u'basically', u'basis', u'batman', u'battle', u'bbc', u'be', u'beach', u'bear', u'beast', u'beat', u'beautiful', u'beautifully', u'beauty', u'became', u'because', u'become', u'becomes', u'becoming', u'bed', u'been', u'before', u'began', u'begin', u'beginning', u'begins', u'behavior', u'behind', u'being', u'belief', u'believable', u'believe', u'believed', u'believes', u'beloved', u'below', u'ben', u'besides', u'best', u'bet', u'better', u'between', u'beyond', u'big', u'bigger', u'biggest', u'bill', u'billy', u'birth', u'bit', u'bits', u'bizarre', u'black', u'blah', u'blair', u'blame', u'bland', u'blind', u'blockbuster', u'blonde', u'blood', u'bloody', u'blow', u'blown', u'blue', u'board', u'boat', u'bob', u'bodies', u'body', u'bollywood', u'bomb', u'bond', u'book', u'books', u'bore', u'bored', u'boring', u'born', u'boss', u'both', u'bother', u'bothered', u'bottom', u'bought', u'bourne', u'box', u'boy', u'boyfriend', u'boys', u'brad', u'brain', u'brave', u'break', u'breaking', u'breaks', u'breath', u'breathtaking', u'brian', u'brief', u'bright', u'brilliant', u'brilliantly', u'bring', u'bringing', u'brings', u'british', u'broadway', u'broken', u'brooks', u'brother', u'brothers', u'brought', u'brown', u'bruce', u'brutal', u'buddy', u'budget', u'build', u'building', u'built', u'bunch', u'burns', u'burt', u'bus', u'business', u'busy', u'but', u'buy', u'buying', u'by', u'c', u'cabin', u'cable', u'cage', u'caine', u'california', u'call', u'called', u'calling', u'calls', u'came', u'cameo', u'camera', u'camp', u'campy', u'can', u\"can't\", u'canadian', u'candy', u'cannot', u'cant', u'capable', u'captain', u'capture', u'captured', u'captures', u'car', u'care', u'career', u'cares', u'caring', u'carried', u'carries', u'carry', u'carrying', u'cars', u'cartoon', u'cartoons', u'case', u'cases', u'cash', u'cast', u'casting', u'castle', u'cat', u'catch', u'category', u'caught', u'cause', u'caused', u'causes', u'cell', u'center', u'central', u'century', u'certain', u'certainly', u'cgi', u'challenge', u'chance', u'change', u'changed', u'changes', u'changing', u'channel', u'character', u\"character's\", u'characters', u'charge', u'charles', u'charlie', u'charm', u'charming', u'chase', u'che', u'cheap', u'check', u'cheesy', u'chemistry', u'chick', u'chief', u'child', u'childhood', u'children', u\"children's\", u'chilling', u'china', u'chinese', u'choice', u'choices', u'choose', u'chose', u'chosen', u'chris', u'christian', u'christmas', u'christopher', u'church', u'cinderella', u'cinema', u'cinematic', u'cinematography', u'circumstances', u'city', u'claim', u'claims', u'claire', u'clark', u'class', u'classic', u'classics', u'clean', u'clear', u'clearly', u'clever', u'clich', u'climax', u'clips', u'close', u'closer', u'closing', u'clothes', u'club', u'clue', u'code', u'cold', u'collection', u'college', u'color', u'colors', u'columbo', u'combination', u'combined', u'come', u'comedic', u'comedies', u'comedy', u'comes', u'comic', u'comical', u'coming', u'comment', u'commentary', u'comments', u'commercial', u'committed', u'common', u'community', u'company', u'compare', u'compared', u'comparison', u'compelling', u'complete', u'completely', u'complex', u'complicated', u'computer', u'concept', u'concerned', u'conclusion', u'conflict', u'confused', u'confusing', u'confusion', u'connection', u'consider', u'considered', u'considering', u'constant', u'constantly', u'contain', u'contains', u'contemporary', u'content', u'context', u'continue', u'continues', u'continuity', u'contrast', u'contrived', u'control', u'conversation', u'convey', u'convince', u'convinced', u'convincing', u'cool', u'cop', u'cops', u'copy', u'core', u'corny', u'correct', u'cost', u'costs', u'costume', u'costumes', u'could', u\"could've\", u\"couldn't\", u'count', u'country', u'couple', u'course', u'court', u'cover', u'covered', u'cowboy', u'crap', u'crappy', u'crash', u'crazy', u'create', u'created', u'creates', u'creating', u'creative', u'creature', u'creatures', u'credit', u'credits', u'creepy', u'crew', u'crime', u'criminal', u'criminals', u'critical', u'criticism', u'critics', u'cross', u'crowd', u'crude', u'cruel', u'cry', u'crying', u'cult', u'cultural', u'culture', u'curious', u'current', u'cut', u'cute', u'cuts', u'cutting', u'd', u'dad', u'daily', u'damn', u'dan', u'dance', u'dancing', u'danger', u'dangerous', u'daniel', u'danny', u'dark', u'darkness', u'date', u'dated', u'daughter', u'daughters', u'david', u'davis', u'day', u'days', u'de', u'dead', u'deadly', u'deal', u'dealing', u'deals', u'dean', u'death', u'deaths', u'debut', u'decade', u'decades', u'decent', u'decide', u'decided', u'decides', u'decision', u'deep', u'deeper', u'deeply', u'definitely', u'degree', u'delight', u'delightful', u'deliver', u'delivered', u'delivers', u'delivery', u'demon', u'demons', u'dennis', u'department', u'depicted', u'depiction', u'depressing', u'depth', u'describe', u'described', u'description', u'desert', u'deserve', u'deserved', u'deserves', u'design', u'designed', u'desire', u'desperate', u'desperately', u'despite', u'destroy', u'destroyed', u'detail', u'details', u'detective', u'determined', u'develop', u'developed', u'development', u'device', u'devil', u'dialog', u'dialogue', u'dick', u'did', u\"didn't\", u'die', u'died', u'dies', u'difference', u'different', u'difficult', u'direct', u'directed', u'directing', u'direction', u'directly', u'director', u\"director's\", u'directors', u'dirty', u'disappointed', u'disappointing', u'disappointment', u'disaster', u'disbelief', u'discover', u'discovered', u'discovers', u'disgusting', u'disney', u'display', u'disturbing', u'do', u'doctor', u'documentary', u'does', u\"doesn't\", u'dog', u'dogs', u'doing', u'dollar', u'dollars', u'don', u\"don't\", u'donald', u'done', u'door', u'double', u'doubt', u'douglas', u'down', u'downright', u'dozen', u'dr', u'drag', u'dragon', u'drama', u'dramatic', u'draw', u'drawn', u'dreadful', u'dream', u'dreams', u'dress', u'dressed', u'drew', u'drinking', u'drive', u'driven', u'driver', u'driving', u'drop', u'drug', u'drugs', u'drunk', u'dry', u'dubbed', u'dude', u'due', u'dull', u'dumb', u'during', u'dvd', u'dying', u'e', u'each', u'earlier', u'early', u'earth', u'easily', u'easy', u'eat', u'eating', u'ed', u'eddie', u'edge', u'edited', u'editing', u'edward', u'effect', u'effective', u'effectively', u'effects', u'effort', u'efforts', u'eight', u'either', u'element', u'elements', u'elizabeth', u'else', u'embarrassed', u'embarrassing', u'emma', u'emotion', u'emotional', u'emotionally', u'emotions', u'empty', u'encounter', u'end', u'ended', u'ending', u'endless', u'ends', u'enemy', u'energy', u'engaging', u'england', u'english', u'enjoy', u'enjoyable', u'enjoyed', u'enjoying', u'enough', u'enter', u'entertain', u'entertained', u'entertaining', u'entertainment', u'entire', u'entirely', u'environment', u'epic', u'episode', u'episodes', u'equally', u'era', u'eric', u'erotic', u'escape', u'escapes', u'especially', u'essential', u'essentially', u'established', u'etc', u'europe', u'european', u'even', u'evening', u'event', u'events', u'eventually', u'ever', u'every', u'everybody', u'everyday', u'everyone', u'everything', u'everywhere', u'evidence', u'evil', u'exact', u'exactly', u'example', u'examples', u'excellent', u'except', u'exception', u'excited', u'excitement', u'exciting', u'excuse', u'executed', u'execution', u'exist', u'existence', u'exists', u'expect', u'expectations', u'expected', u'expecting', u'experience', u'experienced', u'experiences', u'experiment', u'expert', u'explain', u'explained', u'explains', u'explanation', u'exploitation', u'express', u'expression', u'extent', u'extra', u'extraordinary', u'extras', u'extreme', u'extremely', u'eye', u'eyes', u'f', u'fabulous', u'face', u'faces', u'facial', u'fact', u'factor', u'facts', u'fail', u'failed', u'fails', u'failure', u'fair', u'fairly', u'fairy', u'faith', u'faithful', u'fake', u'fall', u'fallen', u'falling', u'falls', u'false', u'fame', u'familiar', u'families', u'family', u'famous', u'fan', u'fans', u'fantastic', u'fantasy', u'far', u'fare', u'fascinating', u'fashion', u'fast', u'fat', u'fate', u'father', u\"father's\", u'fault', u'favor', u'favorite', u'favorites', u'favourite', u'fear', u'feature', u'featured', u'features', u'featuring', u'feel', u'feeling', u'feelings', u'feels', u'feet', u'fell', u'fellow', u'felt', u'female', u'festival', u'few', u'fiction', u'fictional', u'field', u'fight', u'fighting', u'fights', u'figure', u'figured', u'figures', u'fill', u'filled', u'film', u\"film's\", u'film-making', u'filmed', u'filming', u'filmmaker', u'filmmakers', u'films', u'final', u'finale', u'finally', u'find', u'finding', u'finds', u'fine', u'finest', u'finish', u'finished', u'fire', u'first', u'fit', u'fits', u'five', u'flashback', u'flashbacks', u'flat', u'flaws', u'flesh', u'flick', u'flicks', u'flight', u'floor', u'flow', u'fly', u'flying', u'focus', u'focused', u'focuses', u'folks', u'follow', u'followed', u'following', u'follows', u'food', u'fool', u'foot', u'footage', u'football', u'for', u'force', u'forced', u'forces', u'ford', u'foreign', u'forest', u'forever', u'forget', u'forgettable', u'forgot', u'forgotten', u'form', u'format', u'former', u'formula', u'forth', u'fortunately', u'forward', u'foster', u'found', u'four', u'fourth', u'fox', u'frame', u'france', u'frank', u'frankly', u'fred', u'freddy', u'free', u'freedom', u'freeman', u'french', u'frequently', u'fresh', u'friday', u'friend', u'friendly', u'friends', u'friendship', u'frightening', u'from', u'front', u'fu', u'full', u'fully', u'fun', u'funnier', u'funniest', u'funny', u'further', u'future', u'g', u'gags', u'game', u'games', u'gang', u'gangster', u'garbage', u'gary', u'gas', u'gave', u'gay', u'gem', u'gene', u'general', u'generally', u'generation', u'genius', u'genre', u'genuine', u'genuinely', u'george', u'german', u'germany', u'get', u'gets', u'getting', u'ghost', u'ghosts', u'giant', u'girl', u'girlfriend', u'girls', u'give', u'given', u'gives', u'giving', u'glad', u'go', u'god', u'goes', u'going', u'gold', u'golden', u'gone', u'gonna', u'good', u'goofy', u'gordon', u'gore', u'gorgeous', u'gory', u'got', u'gotten', u'government', u'grace', u'grade', u'grand', u'grant', u'granted', u'graphic', u'graphics', u'gratuitous', u'grave', u'great', u'greater', u'greatest', u'green', u'grew', u'grim', u'gritty', u'ground', u'group', u'grow', u'growing', u'grown', u'gruesome', u'guard', u'guess', u'guilty', u'gun', u'guns', u'guy', u'guys', u'h', u'ha', u'had', u\"hadn't\", u'hair', u'half', u'halfway', u'hall', u'halloween', u'hand', u'handle', u'handled', u'hands', u'handsome', u'hanging', u'happen', u'happened', u'happening', u'happens', u'happiness', u'happy', u'hard', u'hardly', u'hardy', u'harris', u'harry', u'harsh', u'has', u\"hasn't\", u'hat', u'hate', u'hated', u'haunted', u'haunting', u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he's\", u'head', u'heads', u'hear', u'heard', u'hearing', u'heart', u'heaven', u'heavily', u'heavy', u'heck', u'held', u'hell', u'help', u'helped', u'helping', u'helps', u'henry', u'her', u'here', u\"here's\", u'hero', u'heroes', u'heroine', u'herself', u'hey', u'hidden', u'hide', u'high', u'higher', u'highlight', u'highly', u'hilarious', u'hill', u'him', u'himself', u'hired', u'his', u'historical', u'history', u'hit', u'hitchcock', u'hitler', u'hits', u'hoffman', u'hold', u'holding', u'holds', u'holes', u'hollywood', u'home', u'honest', u'honestly', u'hong', u'honor', u'hope', u'hopefully', u'hopes', u'hoping', u'horrible', u'horribly', u'horrific', u'horror', u'horse', u'hospital', u'hot', u'hotel', u'hour', u'hours', u'house', u'how', u'howard', u'however', u'huge', u'human', u'humanity', u'humans', u'humor', u'humorous', u'humour', u'hunt', u'hunter', u'hurt', u'husband', u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'ice', u'idea', u'ideas', u'identity', u'idiot', u'if', u'ignore', u'ii', u'ill', u'image', u'imagery', u'images', u'imagination', u'imagine', u'imdb', u'immediately', u'impact', u'important', u'impossible', u'impressed', u'impression', u'impressive', u'in', u'include', u'included', u'includes', u'including', u'incredible', u'incredibly', u'indeed', u'independent', u'india', u'indian', u'indie', u'individual', u'industry', u'inept', u'influence', u'information', u'initial', u'initially', u'inner', u'innocent', u'insane', u'inside', u'insight', u'inspector', u'inspiration', u'inspired', u'instance', u'instead', u'insult', u'intellectual', u'intelligence', u'intelligent', u'intended', u'intense', u'intensity', u'intentions', u'interest', u'interested', u'interesting', u'international', u'interpretation', u'interview', u'interviews', u'into', u'intriguing', u'introduced', u'introduction', u'invisible', u'involved', u'involves', u'involving', u'irish', u'ironic', u'irritating', u'is', u'island', u\"isn't\", u'issue', u'issues', u'it', u\"it's\", u'italian', u'its', u'itself', u'j', u'jack', u'jackie', u'jackson', u'jail', u'james', u'jane', u'japan', u'japanese', u'jason', u'jean', u'jeff', u'jennifer', u'jerry', u'jessica', u'jesus', u'jewish', u'jim', u'jimmy', u'joan', u'job', u'jobs', u'joe', u'john', u'johnny', u'johnson', u'join', u'joke', u'jokes', u'jon', u'jones', u'joseph', u'journey', u'joy', u'jr', u'judge', u'julia', u'julie', u'jump', u'jumps', u'jungle', u'junk', u'just', u'justice', u'k', u'kate', u'keaton', u'keep', u'keeping', u'keeps', u'kelly', u'kept', u'kevin', u'key', u'kick', u'kid', u'kids', u'kill', u'killed', u'killer', u'killers', u'killing', u'kills', u'kim', u'kind', u'kinda', u'kinds', u'king', u'kiss', u'knew', u'know', u'knowing', u'knowledge', u'known', u'knows', u'kong', u'kung', u'l', u'la', u'lack', u'lacking', u'lacks', u'ladies', u'lady', u'lake', u'lame', u'land', u'lane', u'language', u'large', u'largely', u'larry', u'last', u'late', u'later', u'latest', u'latter', u'laugh', u'laughable', u'laughed', u'laughing', u'laughs', u'laughter', u'laura', u'law', u'lawyer', u'lazy', u'lead', u'leader', u'leading', u'leads', u'league', u'learn', u'learned', u'learning', u'learns', u'least', u'leave', u'leaves', u'leaving', u'led', u'lee', u'left', u'legend', u'legendary', u'legs', u'length', u'lesbian', u'leslie', u'less', u'lesson', u'let', u\"let's\", u'lets', u'level', u'levels', u'lewis', u'lie', u'lies', u'life', u'lifetime', u'light', u'lighting', u'lights', u'likable', u'like', u'liked', u'likely', u'likes', u'limited', u'line', u'lines', u'lisa', u'list', u'listen', u'listening', u'literally', u'little', u'live', u'lived', u'lives', u'living', u'local', u'location', u'locations', u'locked', u'logic', u'london', u'lonely', u'long', u'longer', u'look', u'looked', u'looking', u'looks', u'loose', u'lord', u'lose', u'loses', u'losing', u'loss', u'lost', u'lot', u'lots', u'loud', u'louis', u'lousy', u'love', u'loved', u'lovely', u'lover', u'lovers', u'loves', u'loving', u'low', u'low-budget', u'lower', u'luck', u'lucky', u'ludicrous', u'lugosi', u'luke', u'lynch', u'm', u'machine', u'mad', u'made', u'madness', u'magic', u'magical', u'magnificent', u'main', u'mainly', u'mainstream', u'major', u'majority', u'make', u'make-up', u'makers', u'makes', u'makeup', u'making', u'male', u'man', u\"man's\", u'manage', u'managed', u'manager', u'manages', u'manner', u'mansion', u'many', u'maria', u'marie', u'mark', u'market', u'marriage', u'married', u'marry', u'martial', u'martin', u'marvelous', u'mary', u'mask', u'massive', u'master', u'masterpiece', u'match', u'material', u'matrix', u'matt', u'matter', u'matters', u'mature', u'max', u'may', u'maybe', u'me', u'mean', u'meaning', u'means', u'meant', u'meanwhile', u'media', u'mediocre', u'meet', u'meeting', u'meets', u'melodrama', u'member', u'members', u'memorable', u'memories', u'memory', u'men', u'mental', u'mention', u'mentioned', u'mere', u'merely', u'mess', u'message', u'met', u'metal', u'mexican', u'mexico', u'mgm', u'michael', u'michelle', u'mid', u'middle', u'midnight', u'might', u'mike', u'mildly', u'miles', u'military', u'million', u'mind', u'minds', u'mine', u'minor', u'minute', u'minutes', u'mirror', u'miss', u'missed', u'missing', u'mission', u'mistake', u'mistakes', u'mix', u'mixed', u'model', u'modern', u'mom', u'moment', u'moments', u'money', u'monster', u'monsters', u'months', u'mood', u'moon', u'moore', u'moral', u'more', u'morgan', u'morning', u'most', u'mostly', u'mother', u'motion', u'mountain', u'mouth', u'move', u'moved', u'movement', u'moves', u'movie', u\"movie's\", u'movies', u'moving', u'mr', u'mrs', u'ms', u'mst', u'much', u'multiple', u'murder', u'murdered', u'murderer', u'murders', u'murphy', u'music', u'musical', u'musicals', u'must', u'my', u'myself', u'mysterious', u'mystery', u'n', u'naive', u'naked', u'name', u'named', u'names', u'nancy', u'narration', u'narrative', u'nasty', u'nation', u'national', u'native', u'natural', u'naturally', u'nature', u'navy', u'near', u'nearly', u'necessarily', u'necessary', u'ned', u'need', u'needed', u'needless', u'needs', u'negative', u'neither', u'network', u'never', u'nevertheless', u'new', u'news', u'next', u'nice', u'nicely', u'nick', u'night', u'nightmare', u'no', u'nobody', u'noir', u'nominated', u'none', u'nonetheless', u'nonsense', u'nor', u'normal', u'normally', u'north', u'not', u'notable', u'note', u'nothing', u'notice', u'noticed', u'notorious', u'novel', u'novels', u'now', u'nowadays', u'nowhere', u'nude', u'nudity', u'number', u'numbers', u'numerous', u'o', u'obnoxious', u'obsessed', u'obsession', u'obvious', u'obviously', u'occasional', u'occasionally', u'odd', u'oddly', u'of', u'off', u'offensive', u'offer', u'offered', u'offers', u'office', u'officer', u'often', u'oh', u'ok', u'okay', u'old', u'older', u'oliver', u'on', u'once', u'one', u\"one's\", u'ones', u'only', u'onto', u'open', u'opening', u'opens', u'opera', u'opinion', u'opportunity', u'opposite', u'or', u'order', u'ordinary', u'original', u'originality', u'originally', u'oscar', u'other', u'others', u'otherwise', u'our', u'out', u'outside', u'outstanding', u'over', u'over-the-top', u'overall', u'overly', u'own', u'owner', u'p', u'pace', u'paced', u'pacing', u'pacino', u'page', u'paid', u'pain', u'painful', u'painfully', u'paint', u'pair', u'paper', u'par', u'parents', u'paris', u'park', u'parker', u'parody', u'part', u'particular', u'particularly', u'partner', u'parts', u'party', u'pass', u'passed', u'passing', u'passion', u'past', u'path', u'pathetic', u'patrick', u'paul', u'pay', u'paying', u'peace', u'people', u\"people's\", u'perfect', u'perfectly', u'performance', u'performances', u'performed', u'perhaps', u'period', u'person', u'personal', u'personalities', u'personality', u'personally', u'perspective', u'pet', u'peter', u'phone', u'photography', u'physical', u'pick', u'picked', u'picks', u'picture', u'pictures', u'piece', u'pieces', u'pile', u'pilot', u'pitt', u'pity', u'place', u'placed', u'places', u'plain', u'plan', u'plane', u'planet', u'plans', u'play', u'played', u'player', u'players', u'playing', u'plays', u'pleasant', u'please', u'pleasure', u'plenty', u'plot', u'plots', u'plus', u'poignant', u'point', u'pointless', u'points', u'police', u'political', u'politics', u'poor', u'poorly', u'pop', u'popular', u'porn', u'portray', u'portrayal', u'portrayed', u'portraying', u'portrays', u'position', u'positive', u'possible', u'possibly', u'post', u'potential', u'powell', u'power', u'powerful', u'powers', u'practically', u'praise', u'predictable', u'prefer', u'pregnant', u'premise', u'prepared', u'presence', u'present', u'presentation', u'presented', u'presents', u'president', u'pretentious', u'pretty', u'previous', u'previously', u'price', u'priest', u'prime', u'prince', u'princess', u'print', u'prior', u'prison', u'private', u'probably', u'problem', u'problems', u'process', u'produce', u'produced', u'producer', u'producers', u'product', u'production', u'productions', u'professional', u'professor', u'program', u'project', u'promise', u'promising', u'propaganda', u'proper', u'properly', u'protagonist', u'protect', u'proud', u'prove', u'proved', u'proves', u'provide', u'provided', u'provides', u'psycho', u'psychological', u'public', u'pull', u'pulled', u'pulls', u'punch', u'pure', u'purely', u'purpose', u'put', u'puts', u'putting', u'qualities', u'quality', u'queen', u'quest', u'question', u'questions', u'quick', u'quickly', u'quiet', u'quirky', u'quite', u'r', u'race', u'rachel', u'racist', u'radio', u'rain', u'raise', u'raised', u'ran', u'random', u'range', u'rape', u'rare', u'rarely', u'rate', u'rated', u'rather', u'rating', u'ratings', u'raw', u'ray', u'reach', u'reaction', u'read', u'reading', u'ready', u'real', u'realism', u'realistic', u'reality', u'realize', u'realized', u'realizes', u'really', u'reason', u'reasons', u'recall', u'received', u'recent', u'recently', u'recognize', u'recommend', u'recommended', u'record', u'red', u'redeeming', u'reference', u'references', u'refreshing', u'regard', u'regarding', u'regret', u'regular', u'relate', u'related', u'relationship', u'relationships', u'relatively', u'release', u'released', u'relief', u'religion', u'religious', u'remain', u'remains', u'remake', u'remarkable', u'remember', u'remembered', u'remind', u'reminded', u'reminds', u'reminiscent', u'remote', u'remotely', u'rent', u'rental', u'rented', u'renting', u'repeated', u'replaced', u'reporter', u'reputation', u'required', u'rescue', u'research', u'respect', u'responsible', u'rest', u'result', u'results', u'retarded', u'return', u'returns', u'reveal', u'revealed', u'reveals', u'revenge', u'review', u'reviewer', u'reviewers', u'reviews', u'revolution', u'rich', u'richard', u'ride', u'ridiculous', u'right', u'rights', u'ring', u'rings', u'rip', u'rise', u'risk', u'rival', u'river', u'road', u'rob', u'robert', u'robin', u'robot', u'rock', u'roger', u'rogers', u'role', u'roles', u'roll', u'rolling', u'romance', u'romantic', u'ron', u'room', u'rose', u'rough', u'round', u'routine', u'roy', u'rubbish', u'ruin', u'ruined', u'rule', u'rules', u'run', u'running', u'runs', u'russell', u'russian', u'ryan', u's', u'sad', u'sadly', u'safe', u'said', u'sake', u'sam', u'same', u'san', u'santa', u'sarah', u'sat', u'satire', u'satisfying', u'saturday', u'save', u'saved', u'saving', u'saw', u'say', u'saying', u'says', u'scale', u'scare', u'scared', u'scares', u'scary', u'scenario', u'scene', u'scenery', u'scenes', u'school', u'sci-fi', u'science', u'scientist', u'score', u'scott', u'scream', u'screaming', u'screen', u'screening', u'screenplay', u'script', u'sea', u'sean', u'search', u'season', u'seasons', u'seat', u'second', u'seconds', u'secret', u'section', u'security', u'see', u'seeing', u'seek', u'seem', u'seemed', u'seemingly', u'seems', u'seen', u'sees', u'segment', u'self', u'sell', u'send', u'sense', u'sensitive', u'sent', u'sequel', u'sequels', u'sequence', u'sequences', u'serial', u'series', u'serious', u'seriously', u'serve', u'served', u'serves', u'service', u'set', u'sets', u'setting', u'settings', u'seven', u'several', u'sex', u'sexual', u'sexy', u'shadow', u'shakespeare', u'shallow', u'shame', u'share', u'sharp', u'she', u\"she's\", u'sheer', u'sheriff', u'ship', u'shock', u'shocked', u'shocking', u'shoot', u'shooting', u'shop', u'short', u'shot', u'shots', u'should', u\"shouldn't\", u'show', u'showed', u'shower', u'showing', u'shown', u'shows', u'shut', u'sick', u'side', u'sides', u'sidney', u'sight', u'sign', u'significant', u'silent', u'silly', u'similar', u'simon', u'simple', u'simply', u'sinatra', u'since', u'sing', u'singer', u'singing', u'single', u'sinister', u'sir', u'sister', u'sisters', u'sit', u'sitcom', u'site', u'sitting', u'situation', u'situations', u'six', u'skill', u'skills', u'skin', u'skip', u'sky', u'slapstick', u'slasher', u'sleazy', u'sleep', u'sleeping', u'slightly', u'slow', u'slowly', u'small', u'smart', u'smile', u'smith', u'so', u'so-called', u'soap', u'social', u'society', u'soft', u'sold', u'soldier', u'soldiers', u'solid', u'some', u'somebody', u'somehow', u'someone', u'something', u'sometimes', u'somewhat', u'somewhere', u'son', u'song', u'songs', u'soon', u'sorry', u'sort', u'sorts', u'soul', u'sound', u'sounded', u'sounds', u'soundtrack', u'source', u'south', u'southern', u'space', u'spanish', u'speak', u'speaking', u'speaks', u'special', u'spectacular', u'speech', u'speed', u'spend', u'spends', u'spent', u'spirit', u'spite', u'spoil', u'spoiler', u'spoilers', u'spoof', u'sports', u'spot', u'spy', u'stage', u'stand', u'standard', u'standards', u'standing', u'stands', u'stanley', u'star', u'starred', u'starring', u'stars', u'start', u'started', u'starting', u'starts', u'state', u'statement', u'states', u'station', u'status', u'stay', u'stayed', u'stays', u'steal', u'steals', u'step', u'stephen', u'stereotypes', u'stereotypical', u'steve', u'steven', u'stewart', u'stick', u'still', u'stock', u'stolen', u'stomach', u'stone', u'stop', u'stopped', u'stops', u'store', u'stories', u'story', u'storyline', u'storytelling', u'straight', u'strange', u'strangely', u'street', u'streets', u'strength', u'strong', u'strongly', u'structure', u'struggle', u'struggling', u'stuck', u'student', u'students', u'studio', u'studios', u'study', u'stuff', u'stunning', u'stupid', u'stupidity', u'style', u'stylish', u'subject', u'substance', u'subtitles', u'subtle', u'succeeds', u'success', u'successful', u'successfully', u'such', u'suck', u'sucked', u'sucks', u'sudden', u'suddenly', u'suffer', u'suffering', u'suffers', u'suggest', u'suicide', u'suit', u'summary', u'summer', u'sun', u'sunday', u'super', u'superb', u'superior', u'superman', u'supernatural', u'support', u'supporting', u'suppose', u'supposed', u'supposedly', u'sure', u'surely', u'surface', u'surprise', u'surprised', u'surprises', u'surprising', u'surprisingly', u'surreal', u'survive', u'susan', u'suspect', u'suspects', u'suspense', u'suspenseful', u'sweet', u'sword', u'sympathetic', u'sympathy', u'system', u't', u'table', u'take', u'taken', u'takes', u'taking', u'tale', u'talent', u'talented', u'talents', u'tales', u'talk', u'talking', u'talks', u'tape', u'target', u'tarzan', u'task', u'taste', u'taylor', u'teacher', u'team', u'tears', u'technical', u'technically', u'technology', u'ted', u'tedious', u'teen', u'teenage', u'teenager', u'teenagers', u'teens', u'teeth', u'television', u'tell', u'telling', u'tells', u'ten', u'tend', u'tension', u'term', u'terms', u'terrible', u'terribly', u'terrific', u'terror', u'test', u'texas', u'than', u'thank', u'thankfully', u'thanks', u'that', u\"that's\", u'thats', u'the', u'theater', u'theaters', u'theatre', u'theatrical', u'their', u'them', u'theme', u'themes', u'themselves', u'then', u'theory', u'there', u\"there's\", u'therefore', u'these', u'they', u\"they're\", u\"they've\", u'thin', u'thing', u'things', u'think', u'thinking', u'thinks', u'third', u'this', u'thomas', u'thoroughly', u'those', u'though', u'thought', u'thoughts', u'three', u'thriller', u'thrilling', u'through', u'throughout', u'throw', u'throwing', u'thrown', u'throws', u'thus', u'tight', u'till', u'tim', u'time', u'times', u'timing', u'tiny', u'tired', u'titanic', u'title', u'titles', u'to', u'today', u\"today's\", u'together', u'told', u'tom', u'tone', u'tony', u'too', u'took', u'top', u'topic', u'torture', u'total', u'totally', u'touch', u'touched', u'touches', u'touching', u'tough', u'toward', u'towards', u'town', u'track', u'tradition', u'traditional', u'tragedy', u'tragic', u'trailer', u'train', u'training', u'trapped', u'trash', u'travel', u'treasure', u'treat', u'treated', u'treatment', u'tree', u'trek', u'trick', u'tried', u'tries', u'trilogy', u'trip', u'trouble', u'truck', u'true', u'truly', u'trust', u'truth', u'try', u'trying', u'turkey', u'turn', u'turned', u'turning', u'turns', u'tv', u'twenty', u'twice', u'twist', u'twisted', u'twists', u'two', u'type', u'types', u'typical', u'u', u'ugly', u'uk', u'ultimate', u'ultimately', u'unable', u'unbelievable', u'uncle', u'unconvincing', u'under', u'underground', u'underrated', u'understand', u'understanding', u'understood', u'unexpected', u'unfortunate', u'unfortunately', u'unfunny', u'uninteresting', u'unique', u'united', u'universal', u'universe', u'unknown', u'unless', u'unlike', u'unlikely', u'unnecessary', u'unrealistic', u'until', u'unusual', u'up', u'upon', u'urban', u'us', u'usa', u'use', u'used', u'uses', u'using', u'usual', u'usually', u'utter', u'utterly', u'v', u'vacation', u'value', u'values', u'vampire', u'vampires', u'van', u'variety', u'various', u'vehicle', u'version', u'versions', u'very', u'veteran', u'vhs', u'via', u'victim', u'victims', u'victor', u'victoria', u'video', u'vietnam', u'view', u'viewed', u'viewer', u'viewers', u'viewing', u'views', u'village', u'villain', u'villains', u'violence', u'violent', u'virtually', u'vision', u'visit', u'visual', u'visually', u'visuals', u'voice', u'voices', u'von', u'vote', u'vs', u'w', u'wait', u'waiting', u'walk', u'walked', u'walking', u'walks', u'wall', u'walter', u'want', u'wanted', u'wanting', u'wants', u'war', u'warm', u'warned', u'warner', u'warning', u'wars', u'was', u'washington', u\"wasn't\", u'waste', u'wasted', u'watch', u'watchable', u'watched', u'watching', u'water', u'wave', u'way', u'wayne', u'ways', u'we', u\"we're\", u\"we've\", u'weak', u'weapons', u'wear', u'wearing', u'wears', u'wedding', u'week', u'weekend', u'weeks', u'weird', u'welcome', u'well', u'welles', u'went', u'were', u\"weren't\", u'werewolf', u'west', u'western', u'westerns', u'what', u\"what's\", u'whatever', u'whatsoever', u'when', u'whenever', u'where', u'whether', u'which', u'while', u'whilst', u'white', u'who', u\"who's\", u'whoever', u'whole', u'whom', u'whose', u'why', u'wide', u'wife', u'wild', u'will', u'william', u'williams', u'willing', u'wilson', u'win', u'wind', u'window', u'winner', u'winning', u'wins', u'wise', u'wish', u'wit', u'witch', u'with', u'within', u'without', u'witness', u'witty', u'woman', u'women', u'won', u\"won't\", u'wonder', u'wonderful', u'wonderfully', u'wondering', u'wood', u'wooden', u'woods', u'woody', u'word', u'words', u'work', u'worked', u'working', u'works', u'world', u'worse', u'worst', u'worth', u'worthwhile', u'worthy', u'would', u\"would've\", u\"wouldn't\", u'wow', u'write', u'writer', u'writers', u'writing', u'written', u'wrong', u'wrote', u'x', u'yeah', u'year', u'year-old', u'years', u'yes', u'yet', u'york', u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'young', u'younger', u'your', u'yourself', u'youth', u'zero', u'zombie', u'zombies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.array(mat[0])\n",
    "print np.array(temp_rowshit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_rownames = ['hello', 'test', 'pie', 'dirty', 'bad', 'good']\n",
    "so = semantic_orientation(mat=np.array(mat[0]), rownames=mat[1])\n",
    "so[:5]\n",
    "so[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert from list to easily searchable hashmap\n",
    "word_scores = dict()\n",
    "for tup in so:\n",
    "    word_scores[tup[0]] = tup[1]\n",
    "    \n",
    "def get_semantic_score(word_list):\n",
    "    score = 0\n",
    "    for word in word_list:\n",
    "        if word in word_scores:\n",
    "            score += word_scores[word]\n",
    "        else:\n",
    "            print 'not in vocab'\n",
    "    return score\n",
    "\n",
    "get_semantic_score(['good', 'bad', 'john'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"你好\".encode('utf-8')\n",
    "encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent of\n",
    "\n",
    "\"你好\".decode().encode('utf-8')\n",
    "But the decode fails because the string isn't valid ascii. That's why you get a complaint about not being able to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR/AND\n",
    "Takes in a dict of corpus:list of words and returns a dict of corpus:XOR words and dict of corpus:AND words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyList = ['black', 'block', 'beer']\n",
    "\n",
    "def XOR(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first ^ second\n",
    "def AND(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first & second\n",
    "\n",
    "print 'XOR'\n",
    "print XOR(toyList, neighbors_word_list)\n",
    "print 'AND'\n",
    "print AND(toyList, neighbors_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "Takes in a matrix M and correlation list L. Using t-sne, produces a word cloud which represents correlation between all terms. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix: array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold='nan')\n",
    "\n",
    "def word_cloud_preprocessing(words, matrix=mat_ppmi):\n",
    "    output = []\n",
    "    for word in words:\n",
    "        ind = matrix[1].index(word)\n",
    "        output.append(matrix[0][ind])\n",
    "    return output\n",
    "processed_mat = word_cloud_preprocessing(neighbors_word_list)\n",
    "print processed_mat\n",
    "\n",
    "def word_cloud(corr_list): #i think its processed_mat / didn't tsne take in a vector of labels as well?\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    tsne_matrix = model.fit_transform(corr_list)\n",
    "    \n",
    "word_cloud(processed_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modeling, currently using dummy data from lda.datasets\n",
    "\n",
    "NOTE: rerunning can cause relabeling, which means that topic 0 in the first run might now be topic 15 in the next run, so don't be worried if the topic numbers change from run to run\n",
    "\n",
    "run this on the command line first: pip install --user lda\n",
    "\n",
    "https://pypi.python.org/pypi/lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vec = CountVectorizer(stop_words='english')\n",
    "# data = vec.fit_transform(wdmat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #imports\n",
    "\n",
    "# from __future__ import division, print_function\n",
    "\n",
    "# np.set_printoptions(threshold='nan')\n",
    "\n",
    "# #use pip show lda to find the path of where it's installed for you and modify the path append line below with your location\n",
    "# import sys\n",
    "# sys.path.append('/Users/theodorachu/.local/lib/python2.7/site-packages')\n",
    "\n",
    "import lda\n",
    "# import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X): <type 'numpy.ndarray'>\n",
      "shape: (1000, 38772)\n",
      "\n",
      "type(vocab): <type 'tuple'>\n",
      "len(vocab): 38772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document-term matrix\n",
    "X = mat.transpose()\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# the vocab\n",
    "vocab = tuple(lda_vocab)\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "\n",
    "# titles for each story\n",
    "# titles = lda.datasets.load_reuters_titles()\n",
    "# print(\"type(titles): {}\".format(type(titles)))\n",
    "# print(\"len(titles): {}\\n\".format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(rel_X): <type 'numpy.ndarray'>\n",
      "shape: (120, 38772)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input is the equivalence set\n",
    "# output is doc-term matrix of just relevant docs\n",
    "def find_word(input):\n",
    "    indices = [i for i, x in enumerate(lda_vocab) if x in input]\n",
    "    mod_mat = (X.transpose()[indices]).transpose()\n",
    "    mod_mat_indices = mod_mat.sum(axis = 1) != 0\n",
    "    return X[mod_mat_indices]\n",
    "rel_X = find_word(equivalence_set)\n",
    "print(\"type(rel_X): {}\".format(type(rel_X)))\n",
    "print(\"shape: {}\\n\".format(rel_X.shape))\n",
    "X = rel_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 0 word id: 1\n",
      "-- count: 0\n",
      "-- word : -\n"
     ]
    }
   ],
   "source": [
    "#example print statements\n",
    "#gets word 3117 from document 0\n",
    "\n",
    "doc_id = 0\n",
    "word_id = 1\n",
    "\n",
    "print(\"doc id: {} word id: {}\".format(doc_id, word_id))\n",
    "print(\"-- count: {}\".format(X[doc_id, word_id]))\n",
    "print(\"-- word : {}\".format(vocab[word_id]))\n",
    "#print(\"-- doc  : {}\".format(titles[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.35389610e-01,   1.19047619e-03,   1.08225108e-04, ...,\n",
       "          2.39177489e-02,   3.28030303e-01,   3.35497835e-03],\n",
       "       [  8.07502842e-02,   7.99545282e-03,   1.55740811e-02, ...,\n",
       "          6.10079576e-03,   3.42591891e-01,   1.84956423e-01],\n",
       "       [  3.70300752e-02,   6.26566416e-05,   2.32456140e-02, ...,\n",
       "          6.26566416e-05,   3.13972431e-01,   6.26566416e-05],\n",
       "       ..., \n",
       "       [  3.97590361e-02,   1.72117040e-04,   1.72117040e-04, ...,\n",
       "          1.89328744e-03,   4.08089501e-01,   1.72117040e-04],\n",
       "       [  1.02798507e-01,   1.51119403e-02,   3.91791045e-03, ...,\n",
       "          1.86567164e-04,   3.45335821e-01,   1.86567164e-04],\n",
       "       [  1.53894737e-01,   2.10526316e-04,   2.31578947e-03, ...,\n",
       "          8.63157895e-03,   2.90736842e-01,   2.31578947e-03]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(topic_word): <type 'numpy.ndarray'>\n",
      "shape: (20, 38772)\n"
     ]
    }
   ],
   "source": [
    "#topic-word probabilities\n",
    "#shape: (num topics, num words)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "print(\"shape: {}\".format(topic_word.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0 sum: 1.0\n",
      "topic: 1 sum: 1.0\n",
      "topic: 2 sum: 1.0\n",
      "topic: 3 sum: 1.0\n",
      "topic: 4 sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    sum_pr = sum(topic_word[n,:])\n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Topic 0\n",
      "- one -- two also people last years work still first\n",
      "*Topic 1\n",
      "- said police team game football brown carroll c fans coach\n",
      "*Topic 2\n",
      "- said mr ms room christmas york hotel square sticky apartment\n",
      "*Topic 3\n",
      "- city -- ny1 new mayor first giants york world game\n",
      "*Topic 4\n",
      "- list lists magazine bar ms fashion lunch top hot like\n",
      "*Topic 5\n",
      "- school ms schools said mother dr children johnson family choir\n",
      "*Topic 6\n",
      "- mr years died president state first law court department member\n",
      "*Topic 7\n",
      "- book black collection life museum norris photography books history writing\n",
      "*Topic 8\n",
      "- ms judge alito said party nelson house nicolas republican estate\n",
      "*Topic 9\n",
      "- p -- street 5 10 203 7 jan 1 saturday\n",
      "*Topic 10\n",
      "- -- one another good make less many place among something\n",
      "*Topic 11\n",
      "- cheese restaurant chicken food restaurants wine dishes sauce 1 street\n",
      "*Topic 12\n",
      "- oil water tv percent use 99 7 home set money\n",
      "*Topic 13\n",
      "- women world people cultural men human culture bark find century\n",
      "*Topic 14\n",
      "- quagga rau skin cute zebra animal project plains zebras like\n",
      "*Topic 15\n",
      "- said rights united nations human patients commission study council countries\n",
      "*Topic 16\n",
      "- ukraine tymoshenko yushchenko israeli minister prime russia israel political revolution\n",
      "*Topic 17\n",
      "- mr music theater bloom opera grier play director cash prison\n",
      "*Topic 18\n",
      "-  said new like would time black even made year\n",
      "*Topic 19\n",
      "- hartford connecticut world perth australia barrier city river north voorhees\n"
     ]
    }
   ],
   "source": [
    "#spits out top n words for each topic by probability\n",
    "\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(doc_topic): <type 'numpy.ndarray'>\n",
      "shape: (120, 20)\n"
     ]
    }
   ],
   "source": [
    "#document-topic probabilities\n",
    "#shape: (num documents, num topics)\n",
    "\n",
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3, 3: 4, 4: 1, 6: 3, 8: 1, 11: 5, 12: 3, 13: 1, 15: 5, 18: 88, 19: 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = {}\n",
    "for i in range(115):\n",
    "    topic = doc_topic[i].argmax()\n",
    "    if topic in dist:\n",
    "        dist[topic] += 1\n",
    "    else:\n",
    "        dist[topic] = 1\n",
    "    #print(\"{} (top topic: {})\".format(i, doc_topic[i].argmax()))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualizing the inference - matlab setup/imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now the plots don't print? it just throws the notebook into busy mode for a very long time so not sure if something is off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theodorachu/anaconda2/lib/python2.7/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n"
     ]
    }
   ],
   "source": [
    "#stem plots - height of each stem reflects the probability of the word in the focus topic\n",
    "\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,4350)\n",
    "    ax[i].set_ylim(0, 0.08)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.draw()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topic distribution - probability of each of the 20 topics for every document\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([1, 3, 4, 8, 9]): #only plotting these specified topics\n",
    "    ax[i].stem(doc_topic[k,:], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[i].set_xlim(-1, 21)\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"Document {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"Topic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
