{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__author__ = \"Theodora Chu, Josh Cohen, Jason Chen\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info for creating VSM data\n",
    "vsmdata_home = \"vsmdata\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "equivalence_set = ['african american', 'african-american', 'black', 'african', 'nigger', 'nigga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_set2 = ['asian-american', 'african-american', 'black', 'african', 'asian', 'jewish', 'latino', 'mexican', 'russian', 'american']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input\n",
    "Takes in a text file and returns a list of ordered unigrams U. \n",
    "It should also consider stemming and other relevant pre-processing. Josh's note: parse \"African American\" as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_seedword(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"Parsing outer directory: \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):                \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: parsed overall ' + str(i);\n",
    "                    return overallCorpus\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    overallCorpus += ' ';\n",
    "                    overallCorpus += article_text;\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i);\n",
    "    return overallCorpus;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_worddoc(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    file_list = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"Parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):      \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files parsed in total: ' + str(i);\n",
    "                    return (overallCorpus, file_list)\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                file_list.append(filename)\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    for word in article_text:\n",
    "                        overallCorpus.append((word, filename))\n",
    "                i = i+1;\n",
    "    print \"Num files parsed in total: \" + str(i)\n",
    "    return (overallCorpus, file_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "1. Parse U to create a word-word frequency matrix M, where each row represents a word and each entry x(i,j) represents the number of times word i co-occurs with word j.\n",
    "2. Convert M to a new matrix M’ with some sort of correlation operation. We could use PMI, Occai (see Josh’s paper), CSA, or some other correlation structure.\n",
    "3. Let row a represent the unigram “African American”. Take in that row, and output an ordered list of (this_unigram, correlation_score) pairs which represent the correlation score of this_unigram with the term “African American”\n",
    "4. Produce a list L of the top 100 correlated words with the term “African American”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns matrix object where mat_obj[0] refers to the seed_word matrix, where mat[1] refers\n",
    "# to the vocab list, where mat[2] refers to a frequency list,\n",
    "# where mat_obj[0][0] refers to the vector representing co-occurrence for first word in seed\n",
    "# set, and where mat_obj[0][len(seed_set)] refers to a vector of overall counts for each term \n",
    "def createSeedWordMatrixNYT(num_files, file_year=\"2006\"):\n",
    "    # Initializes vector of terms\n",
    "    corpus = \"cor-por-a/\" + file_year + \"/\"\n",
    "    u_vec = [x.lower() for x in parse_NYT_articles_seedword(num_files, root_directory=corpus)];\n",
    "    num_terms = len(u_vec);\n",
    "    print 'num terms in corpus: ' + str(num_terms);\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    print 'vocab size: ' + str(vocab_size);\n",
    "    print 'matrix dimensions: ' + str(len(seed_set2)) + ' x ' + str(len(vocab_vec));\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(len(seed_set2)+1)]\n",
    "    frequency_vec = [0 for x in range(vocab_size)]\n",
    "\n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i;\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    term = u_vec[0];\n",
    "    term_neighbor_r = u_vec[1];\n",
    "    index_term = index_dict[term];\n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            index_neighbor_r = index_dict[term_neighbor_r];\n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    for i in range(1, len(u_vec)-1):\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(num_terms) + ' terms'\n",
    "        term = u_vec[i];\n",
    "        term_neighbor_l = u_vec[i-1];\n",
    "        term_neighbor_r = u_vec[i+1];\n",
    "        index_term = index_dict[term]\n",
    "        \n",
    "        frequency_vec[index_term] += 1;\n",
    "        if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            index_neighbor_l = index_dict[term_neighbor_l]\n",
    "            index_neighbor_r = index_dict[term_neighbor_r]\n",
    "            \n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    term = u_vec[len(u_vec)-1];\n",
    "    term_neighbor_l = u_vec[len(u_vec)-2];\n",
    "    index_term = index_dict[term]\n",
    "    \n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            index_neighbor_l = index_dict[term];\n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    \n",
    "\n",
    "    #   Filter the matrix by removing all words with frequency less than cutoff_freq\n",
    "    mat = np.transpose(mat)\n",
    "    indicies = []\n",
    "    cutoff_freq = 5\n",
    "    for i in range(0, len(frequency_vec)):\n",
    "        if frequency_vec[i] > cutoff_freq:\n",
    "            indicies.append(i) #keep track of indices of all words with frequency < cutoff_freq\n",
    "    print 'Parsed ' + str(num_terms) + '/' + str(num_terms) + ' terms';\n",
    "    #update mat, freq_vec, and vocab_vec to include only the indices we saved\n",
    "    mat = mat[np.array(indicies)]\n",
    "    frequency_vec = np.array(frequency_vec)\n",
    "    frequency_vec = frequency_vec[np.array(indicies)]\n",
    "    temp_vocab = []\n",
    "    for index in indicies:\n",
    "        temp_vocab.append(vocab_vec[index])\n",
    "    vocab_vec = temp_vocab\n",
    "    #transpose mat back to original shape\n",
    "    mat = np.transpose(mat)\n",
    "    print(\"New vocab size: \" + str(len(mat[0])))\n",
    "    return (mat, vocab_vec, frequency_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a word, correlation list tuple for each seed in seed_set2\n",
    "from nltk.corpus import stopwords\n",
    "def getCorrelationLists(mat_obj, rem_stopwords=1):\n",
    "    tupleArr = []\n",
    "    # Word lists for each word\n",
    "    for j in range(len(seed_set2)):\n",
    "        w = mat_obj[0][j]\n",
    "        unfiltered_dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "        \n",
    "        # Without stop words\n",
    "        dists = [];\n",
    "        if rem_stopwords:\n",
    "            dists = [(word, frequency) for (word, frequency) in unfiltered_dists if word not in stopwords.words('english')]\n",
    "        else:\n",
    "            dists = unfiltered_dists\n",
    "        sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "        print \"Correlation list for word: \" + seed_set2[j] + \"; \" + str(sorted_dists[:20])\n",
    "        tupleArr.append((seed_set2[j], sorted_dists));\n",
    "\n",
    "    #frequency list for each word:\n",
    "    w = mat_obj[2]\n",
    "    dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    print \"Frequency list: \" + str(sorted_dists[:20])\n",
    "    tupleArr.append((seed_set2[j], sorted_dists));\n",
    "    \n",
    "    return tupleArr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbors(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# PMI log: p(x, y)/ p(x)p(y)\n",
    "def pmi_seed(mat_obj, rownames=None, positive=True):  \n",
    "    rownames = mat_obj[1];\n",
    "    frequencies = mat_obj[2];\n",
    "    word_count = np.sum(frequencies, axis=None)\n",
    "    \n",
    "    # Joint probability table:\n",
    "    p = mat_obj[0] / word_count;\n",
    "    colprobs = frequencies/word_count;\n",
    "    sum_of_colprobs = np.sum(colprobs)\n",
    "    \n",
    "    \n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))    \n",
    "    mat_ppmi = [];\n",
    "    for row in p:\n",
    "        if np.sum(row) > 0:\n",
    "            mat_ppmi.append(np_pmi_log(row / (np.sum(row)*colprobs)));\n",
    "        else:\n",
    "            mat_ppmi.append([0 for x in row])\n",
    "    return (mat_ppmi, rownames, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pmi_log(x, positive=True):\n",
    "    val = 0.0\n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val,0.0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a matrix ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.exists('my_file'): numpy.savetxt('my_file', my_array)\n",
    "\n",
    "#this will save the result of our matrix into a human-readable text file, and the original array is easily\n",
    "#recreated using loadtxt.\n",
    "\n",
    "# np.savetxt(\"mat_features\", mat[0])\n",
    "# np.savetxt(\"mat_labels\", mat[1])\n",
    "# np.loadtxt(\"mat_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### tool to remove punctuation from a text ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string  With  Punctuation \n"
     ]
    }
   ],
   "source": [
    "s = \"string. With. Punctuation?\" # Sample string\n",
    "def remove_punctuation(text):\n",
    "    for c in string.punctuation:\n",
    "        if c != '-': #excluding - because we want to preserve african-american as a token\n",
    "            text = text.replace(c,\" \")\n",
    "    return text\n",
    "\n",
    "print(remove_punctuation(s))\n",
    "\n",
    "\n",
    "#period, question mark, exclamation point, comma, semicolon, colon, dash, \n",
    "#hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools to parse out text from ntif/xml document for NYT articles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2007',\n",
       " 'A doctor who works at a clinic in Jamaica has been charged with insurance fraud, accused of billing insurance companies for tests that were never performed on victims of motor vehicle accidents, prosecutors said yesterday. The doctor, Alexander Israeli, 53, of Middle Village, was arraigned in Criminal Court on Monday night on charges of grand larceny and insurance fraud, said Richard A. Brown, the Queens district attorney. Mr. Brown said that Dr. Israeli billed insurance companies last year for $21,000 worth of neurological tests that were not performed. He faces loss of his medical license and up to seven years in prison if convicted, prosecutors said.')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://docs.python-guide.org/en/latest/scenarios/xml/\n",
    "# http://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_NYT_article(xmlFile): \n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    year = ''\n",
    "    article_text = '';\n",
    "    for child in root:\n",
    "        if child.tag == 'head':\n",
    "            for subchild in child:\n",
    "                if 'name' in subchild.attrib:\n",
    "                    if subchild.attrib['name'] == 'publication_year':\n",
    "                        year = subchild.attrib['content']\n",
    "        if child.tag == 'body':\n",
    "            body = child\n",
    "    for child in body:\n",
    "        if child.tag == 'body.content':\n",
    "            content = child\n",
    "    for child in content:\n",
    "        if child.attrib == {'class': 'full_text'}:\n",
    "            for paragraph in child:\n",
    "                article_text += paragraph.text\n",
    "            return (year, article_text)\n",
    "                \n",
    "parse_NYT_article('nyt_sample_2.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Seed Word Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_file_year = \"2006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing outer directory: 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "Num files: parsed overall 100\n",
      "num terms in corpus: 102381\n",
      "vocab size: 14051\n",
      "matrix dimensions: 10 x 14051\n",
      "index_dict created!\n",
      "parsed 1000/102381 terms\n",
      "parsed 2000/102381 terms\n",
      "parsed 3000/102381 terms\n",
      "parsed 4000/102381 terms\n",
      "parsed 5000/102381 terms\n",
      "parsed 6000/102381 terms\n",
      "parsed 7000/102381 terms\n",
      "parsed 8000/102381 terms\n",
      "parsed 9000/102381 terms\n",
      "parsed 10000/102381 terms\n",
      "parsed 11000/102381 terms\n",
      "parsed 12000/102381 terms\n",
      "parsed 13000/102381 terms\n",
      "parsed 14000/102381 terms\n",
      "parsed 15000/102381 terms\n",
      "parsed 16000/102381 terms\n",
      "parsed 17000/102381 terms\n",
      "parsed 18000/102381 terms\n",
      "parsed 19000/102381 terms\n",
      "parsed 20000/102381 terms\n",
      "parsed 21000/102381 terms\n",
      "parsed 22000/102381 terms\n",
      "parsed 23000/102381 terms\n",
      "parsed 24000/102381 terms\n",
      "parsed 25000/102381 terms\n",
      "parsed 26000/102381 terms\n",
      "parsed 27000/102381 terms\n",
      "parsed 28000/102381 terms\n",
      "parsed 29000/102381 terms\n",
      "parsed 30000/102381 terms\n",
      "parsed 31000/102381 terms\n",
      "parsed 32000/102381 terms\n",
      "parsed 33000/102381 terms\n",
      "parsed 34000/102381 terms\n",
      "parsed 35000/102381 terms\n",
      "parsed 36000/102381 terms\n",
      "parsed 37000/102381 terms\n",
      "parsed 38000/102381 terms\n",
      "parsed 39000/102381 terms\n",
      "parsed 40000/102381 terms\n",
      "parsed 41000/102381 terms\n",
      "parsed 42000/102381 terms\n",
      "parsed 43000/102381 terms\n",
      "parsed 44000/102381 terms\n",
      "parsed 45000/102381 terms\n",
      "parsed 46000/102381 terms\n",
      "parsed 47000/102381 terms\n",
      "parsed 48000/102381 terms\n",
      "parsed 49000/102381 terms\n",
      "parsed 50000/102381 terms\n",
      "parsed 51000/102381 terms\n",
      "parsed 52000/102381 terms\n",
      "parsed 53000/102381 terms\n",
      "parsed 54000/102381 terms\n",
      "parsed 55000/102381 terms\n",
      "parsed 56000/102381 terms\n",
      "parsed 57000/102381 terms\n",
      "parsed 58000/102381 terms\n",
      "parsed 59000/102381 terms\n",
      "parsed 60000/102381 terms\n",
      "parsed 61000/102381 terms\n",
      "parsed 62000/102381 terms\n",
      "parsed 63000/102381 terms\n",
      "parsed 64000/102381 terms\n",
      "parsed 65000/102381 terms\n",
      "parsed 66000/102381 terms\n",
      "parsed 67000/102381 terms\n",
      "parsed 68000/102381 terms\n",
      "parsed 69000/102381 terms\n",
      "parsed 70000/102381 terms\n",
      "parsed 71000/102381 terms\n",
      "parsed 72000/102381 terms\n",
      "parsed 73000/102381 terms\n",
      "parsed 74000/102381 terms\n",
      "parsed 75000/102381 terms\n",
      "parsed 76000/102381 terms\n",
      "parsed 77000/102381 terms\n",
      "parsed 78000/102381 terms\n",
      "parsed 79000/102381 terms\n",
      "parsed 80000/102381 terms\n",
      "parsed 81000/102381 terms\n",
      "parsed 82000/102381 terms\n",
      "parsed 83000/102381 terms\n",
      "parsed 84000/102381 terms\n",
      "parsed 85000/102381 terms\n",
      "parsed 86000/102381 terms\n",
      "parsed 87000/102381 terms\n",
      "parsed 88000/102381 terms\n",
      "parsed 89000/102381 terms\n",
      "parsed 90000/102381 terms\n",
      "parsed 91000/102381 terms\n",
      "parsed 92000/102381 terms\n",
      "parsed 93000/102381 terms\n",
      "parsed 94000/102381 terms\n",
      "parsed 95000/102381 terms\n",
      "parsed 96000/102381 terms\n",
      "parsed 97000/102381 terms\n",
      "parsed 98000/102381 terms\n",
      "parsed 99000/102381 terms\n",
      "parsed 100000/102381 terms\n",
      "parsed 101000/102381 terms\n",
      "parsed 102000/102381 terms\n",
      "Parsed 102381/102381 terms\n",
      "New vocab size: 1932\n"
     ]
    }
   ],
   "source": [
    "mat_obj = createSeedWordMatrixNYT(num_files=100, file_year=this_file_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(this_file_year + \"_seedword\", mat_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Correlation Lists\n",
    "Using the functions above, creates seed-word matrix with a user-specified number of files, performs pmi on that matrix, and computes a resulting correlation list for each seed word.\n",
    "\n",
    "In order to use more files, update the num_files variable. In order to update the seed set, update the seed_set2 global variable to include more words.\n",
    "\n",
    "Note: Creating these correlation lists at scale is very slow. Start off by processing about 10 files, and scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rehydrate_file_year = \"2006\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_rehydrate = np.load(rehydrate_file_year + \"_seedword.npy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_rehydrate_ppmi = pmi_seed(mat_rehydrate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation list for word: asian-american; [(u'asian-american', 12.153338576082547), (u'cowed', 8.5424206634383228), (u'walk-on', 8.4644591219686109), (u'whiz', 7.7966297493929559), (u'receptionist', 7.7344979682859494), (u'spotlights', 7.6100437938125438), (u'assisting', 7.0719342110980845), (u'35-year-old', 7.0058440992690949), (u'29th', 6.6520803655378211), (u'actresses', 6.5402104696944772), (u'classmates', 6.2425419320420206), (u'turnout', 6.041871236579869), (u'best-known', 5.9169689858788432), (u'affecting', 5.8397905298054527), (u'studies', 5.737014873759767), (u'69', 5.2366235557289391), (u'themes', 5.1686222559642827), (u'supports', 5.1006175268502254), (u'population', 5.0653472467974776), (u'advisory', 5.0632617403064559)]\n",
      "Correlation list for word: african-american; [(u'african-american', 9.6691920935679452), (u'jive-talking', 7.8774326243398898), (u'complainants', 7.4719675162317252), (u'manservant', 7.4719675162317252), (u'foodways', 7.3666070005738993), (u'predominately', 7.2712968207695745), (u'stolidity', 7.2712968207695745), (u'lower-class', 7.1842854437799444), (u'six-foot-tall', 7.1842854437799444), (u'light-skinned', 7.1434634492596896), (u'ex-cop', 7.104242736106408), (u'well-built', 7.104242736106408), (u'muralist', 7.0301347639526863), (u'a13new', 6.7788203356717798), (u'consign', 6.7788203356717798), (u'dyspeptic', 6.7788203356717798), (u'long-track', 6.7788203356717798), (u'gun-toting', 6.6734598200139539), (u'unmotivated', 6.6734598200139539), (u'periodical', 6.5781496402096291)]\n",
      "Correlation list for word: black; [(u'black', 7.2520098036915135), (u'cohosh', 7.251740479693118), (u'mambazo', 7.251740479693118), (u'donnellys', 7.1716977720195816), (u'potatoe', 7.1339574440367342), (u'ladysmith', 7.0694189228991631), (u'pepper1', 6.9562762667992821), (u'pepperfor', 6.9415855513892781), (u'dahlia', 6.8462753715849534), (u'marketeers', 6.7817368504473823), (u'currant', 6.6763763347895555), (u'pepper2', 6.6763763347895555), (u'cofer', 6.6456046761228018), (u'abayas', 6.5585932991331726), (u'miso-glazed', 6.5585932991331726), (u'renova', 6.5585932991331726), (u'hebrews', 6.5045260778628968), (u'hollies', 6.4785505914596362), (u'empire-style', 6.4408102634767888), (u'pegboard', 6.4408102634767888)]\n",
      "Correlation list for word: african; [(u'african', 8.949036519659666), (u'tarantella', 8.2558893390997206), (u'fractals', 7.850424230991556), (u'200-member', 7.4449591228833913), (u'diaspora', 7.3834012298839582), (u'000-strong', 7.3395986072255655), (u'goatskins', 7.3395986072255655), (u'proverb', 7.2144354642715589), (u'baobab', 7.1572770504316106), (u'kwaito', 7.1572770504316106), (u'rands', 7.1572770504316106), (u'veld', 7.1572770504316106), (u'violets', 7.1244872276086202), (u'juba', 7.0031263706043525), (u'stonechats', 7.0031263706043525), (u'underequipped', 7.0031263706043525), (u'folktale', 6.8695949779798298), (u'polyrhythms', 6.8695949779798298), (u'sarong', 6.8695949779798298), (u'swallowtail', 6.8695949779798298)]\n",
      "Correlation list for word: asian; [(u'asian', 9.1571504793800429), (u'weavings', 8.403378677003662), (u'becks', 7.3653910101519875), (u'mignonette', 7.3653910101519875), (u'bureaucratese', 7.2112403303247294), (u'character-based', 7.0777089377002067), (u'gigantism', 7.0777089377002067), (u'lacquerwares', 7.0777089377002067), (u'moonstone', 7.0777089377002067), (u'bling-bling', 6.9599259020438238), (u'flyway', 6.9599259020438238), (u'marinades', 6.9599259020438238), (u'salade', 6.9599259020438238), (u'siddhas', 6.9599259020438238), (u'dub', 6.944177545075684), (u'diasporas', 6.854565386385997), (u'bonne', 6.7592552065816722), (u'bergamot', 6.672243829592043), (u'ghazal', 6.672243829592043), (u'mandalas', 6.672243829592043)]\n"
     ]
    }
   ],
   "source": [
    "correlation_list = getCorrelationLists(mat_rehydrate_ppmi, rem_stopwords=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Document Matrix\n",
    "Creates a word document matrix for use by Theo and her LDA work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def createWordDocumentMatrixNYT(num_files):\n",
    "    result = parse_NYT_articles_worddoc(num_files)\n",
    "    word_file_vec = [(x[0].lower(),x[1]) for x in result[0]]\n",
    "    word_vec = [x[0] for x in word_file_vec]\n",
    "    vocab_vec = np.unique(word_vec).tolist()\n",
    "    file_vec = result[1]\n",
    "    print 'num terms in corpus: ' + str(len(word_vec))\n",
    "    print 'vocab size: ' + str(len(vocab_vec))\n",
    "    print 'matrix dimensions: ' + str(len(vocab_vec)) + ' x ' + str(len(file_vec))\n",
    "    mat = [[0 for x in range(len(file_vec))] for y in range(len(vocab_vec))]\n",
    "    \n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    file_index_dict = {};\n",
    "    for i in range (0, len(file_vec)):\n",
    "        file_index_dict[file_vec[i]] = i\n",
    "    print 'file_index_dict created!'\n",
    "    \n",
    "    i = 0;\n",
    "    for word_file_tuple in word_file_vec:\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(len(word_vec)) + ' terms'\n",
    "        word = word_file_tuple[0]\n",
    "        file_name = word_file_tuple[1]\n",
    "        \n",
    "        # CHANGED\n",
    "        #index_word = vocab_vec.index(word);\n",
    "        index_word = index_dict[word]  \n",
    "        #index_file = file_vec.index(file_name);\n",
    "        index_file = file_index_dict[file_name]\n",
    "        \n",
    "        mat[index_word][index_file] +=1\n",
    "        i = i+1\n",
    "    print 'Parsed all terms'\n",
    "    keep = []\n",
    "    stop = stopwords.words('english')\n",
    "    updated_vocab_vec = []\n",
    "    for ind, word in enumerate(vocab_vec):\n",
    "        if word not in stop:\n",
    "            keep.append(ind)\n",
    "            updated_vocab_vec.append(word)\n",
    "    keep = np.array(mat)[keep]\n",
    "            \n",
    "    return (keep, updated_vocab_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "Num files: 100\n",
      "num terms in corpus: 102283\n",
      "vocab size: 14050\n",
      "matrix dimensions: 14050 x 100\n",
      "index_dict created!\n",
      "file_index_dict created!\n",
      "parsed 0/102283 terms\n",
      "parsed 1000/102283 terms\n",
      "parsed 2000/102283 terms\n",
      "parsed 3000/102283 terms\n",
      "parsed 4000/102283 terms\n",
      "parsed 5000/102283 terms\n",
      "parsed 6000/102283 terms\n",
      "parsed 7000/102283 terms\n",
      "parsed 8000/102283 terms\n",
      "parsed 9000/102283 terms\n",
      "parsed 10000/102283 terms\n",
      "parsed 11000/102283 terms\n",
      "parsed 12000/102283 terms\n",
      "parsed 13000/102283 terms\n",
      "parsed 14000/102283 terms\n",
      "parsed 15000/102283 terms\n",
      "parsed 16000/102283 terms\n",
      "parsed 17000/102283 terms\n",
      "parsed 18000/102283 terms\n",
      "parsed 19000/102283 terms\n",
      "parsed 20000/102283 terms\n",
      "parsed 21000/102283 terms\n",
      "parsed 22000/102283 terms\n",
      "parsed 23000/102283 terms\n",
      "parsed 24000/102283 terms\n",
      "parsed 25000/102283 terms\n",
      "parsed 26000/102283 terms\n",
      "parsed 27000/102283 terms\n",
      "parsed 28000/102283 terms\n",
      "parsed 29000/102283 terms\n",
      "parsed 30000/102283 terms\n",
      "parsed 31000/102283 terms\n",
      "parsed 32000/102283 terms\n",
      "parsed 33000/102283 terms\n",
      "parsed 34000/102283 terms\n",
      "parsed 35000/102283 terms\n",
      "parsed 36000/102283 terms\n",
      "parsed 37000/102283 terms\n",
      "parsed 38000/102283 terms\n",
      "parsed 39000/102283 terms\n",
      "parsed 40000/102283 terms\n",
      "parsed 41000/102283 terms\n",
      "parsed 42000/102283 terms\n",
      "parsed 43000/102283 terms\n",
      "parsed 44000/102283 terms\n",
      "parsed 45000/102283 terms\n",
      "parsed 46000/102283 terms\n",
      "parsed 47000/102283 terms\n",
      "parsed 48000/102283 terms\n",
      "parsed 49000/102283 terms\n",
      "parsed 50000/102283 terms\n",
      "parsed 51000/102283 terms\n",
      "parsed 52000/102283 terms\n",
      "parsed 53000/102283 terms\n",
      "parsed 54000/102283 terms\n",
      "parsed 55000/102283 terms\n",
      "parsed 56000/102283 terms\n",
      "parsed 57000/102283 terms\n",
      "parsed 58000/102283 terms\n",
      "parsed 59000/102283 terms\n",
      "parsed 60000/102283 terms\n",
      "parsed 61000/102283 terms\n",
      "parsed 62000/102283 terms\n",
      "parsed 63000/102283 terms\n",
      "parsed 64000/102283 terms\n",
      "parsed 65000/102283 terms\n",
      "parsed 66000/102283 terms\n",
      "parsed 67000/102283 terms\n",
      "parsed 68000/102283 terms\n",
      "parsed 69000/102283 terms\n",
      "parsed 70000/102283 terms\n",
      "parsed 71000/102283 terms\n",
      "parsed 72000/102283 terms\n",
      "parsed 73000/102283 terms\n",
      "parsed 74000/102283 terms\n",
      "parsed 75000/102283 terms\n",
      "parsed 76000/102283 terms\n",
      "parsed 77000/102283 terms\n",
      "parsed 78000/102283 terms\n",
      "parsed 79000/102283 terms\n",
      "parsed 80000/102283 terms\n",
      "parsed 81000/102283 terms\n",
      "parsed 82000/102283 terms\n",
      "parsed 83000/102283 terms\n",
      "parsed 84000/102283 terms\n",
      "parsed 85000/102283 terms\n",
      "parsed 86000/102283 terms\n",
      "parsed 87000/102283 terms\n",
      "parsed 88000/102283 terms\n",
      "parsed 89000/102283 terms\n",
      "parsed 90000/102283 terms\n",
      "parsed 91000/102283 terms\n",
      "parsed 92000/102283 terms\n",
      "parsed 93000/102283 terms\n",
      "parsed 94000/102283 terms\n",
      "parsed 95000/102283 terms\n",
      "parsed 96000/102283 terms\n",
      "parsed 97000/102283 terms\n",
      "parsed 98000/102283 terms\n",
      "parsed 99000/102283 terms\n",
      "parsed 100000/102283 terms\n",
      "parsed 101000/102283 terms\n",
      "parsed 102000/102283 terms\n",
      "Parsed all terms\n"
     ]
    }
   ],
   "source": [
    "(mat, lda_vocab) = createWordDocumentMatrixNYT(num_files=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "Takes in a list V of words and returns the average sentiment score across all terms in V as determined by freebase. Note to Jason: consider other sentiment databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlated_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7b768ec0c092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#print generate_sentiment_2(neighbors_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgenerate_sentiment_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelated_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'correlated_words' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def getSentiment(word):\n",
    "    synset = list(swn.senti_synsets(word))\n",
    "    if len(synset) > 0: #if a synset exists for this word\n",
    "        synset = synset[0]\n",
    "        return(synset.pos_score(), synset.neg_score(), synset.obj_score())\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "V = ['good', 'bad', 'great', 'awesome', 'amazing', 'holy', 'beautiful', 'worrisome', 'stupid']\n",
    "def generate_sentiment(wordList):\n",
    "    totalSentiment = 0.0;\n",
    "    for word in wordList:\n",
    "        if is_ascii(word): #see note below for rationale\n",
    "            sentiment = getSentiment(word)\n",
    "            if sentiment == None:\n",
    "                sentiment = 0.0\n",
    "            if type(sentiment) is float: #why does this happen\n",
    "                print \"n/a\"\n",
    "            else:  \n",
    "                totalSentiment += (sentiment[0] - sentiment[1]) \n",
    "                print (sentiment[0] - sentiment[1])\n",
    "        #sentiwordnet generates tuples of pos, neg, and neu. currently naively choosing to consider only sum of pos and neg. \n",
    "    averageSentiment = totalSentiment/len(wordList)\n",
    "    return averageSentiment\n",
    "\n",
    "def generate_sentiment_2(wordTupleList):\n",
    "    reader = csv.reader(open('sentiment_words.txt', 'rb'))\n",
    "    sentiment_words = dict(reader)\n",
    "    sentiment_score = 0\n",
    "    for wordTuple in wordTupleList:\n",
    "        word = wordTuple[0]\n",
    "        score = 1/wordTuple[1] #inverse of distance\n",
    "        if word in sentiment_words:\n",
    "            if sentiment_words[word] == 'pos':\n",
    "                print word + \" +\" + str(score)\n",
    "                sentiment_score += score\n",
    "            if sentiment_words[word] == 'neg':\n",
    "                print word + \" -\" + str(score)\n",
    "                sentiment_score -= score\n",
    "    return sentiment_score\n",
    "\n",
    "#print generate_sentiment_2(neighbors_list)\n",
    "print generate_sentiment_2(correlated_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim example ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2653, in run_cell\n    cell = self.input_transformer_manager.transform_cell(raw_cell)\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 597, in transform_cell\n    self.push(cell)\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 641, in push\n    out = self.push_line(line)\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 668, in push_line\n    line = self.assemble_python_lines.push(line)\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/core/inputtransformer.py\", line 151, in push\n    for intok in self.tokenizer:\n",
      "  File \"/Users/Cheson/anaconda2/lib/python2.7/site-packages/IPython/utils/_tokenize_py2.py\", line 296, in generate_tokens\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def parser_wrapper(year):\n",
    "    import time\n",
    "    num_files = 1000000000 #all files\n",
    "    root_directory = 'cor-por-a/'+ str(year) + '/'\n",
    "    \n",
    "    start = time.clock()\n",
    "    word2vec = parse_NYT_articles_word2vec(num_files, root_directory)\n",
    "    np.save('parses/' + str(year) + '_' + 'word2vec_parse', word2vec)\n",
    "    end = time.clock()\n",
    "    print \"word2vec parse time: \" + str(end-start)\n",
    "    \n",
    "    start = time.clock()\n",
    "    seedword = parse_NYT_articles_seedword(num_files, root_directory)\n",
    "    np.save('parses/' + str(yea\n",
    "    end = time.clock()\n",
    "    print \"seedword parse time: \" + str(end-start)\n",
    "    \n",
    "    start = time.clock()\n",
    "    worddoc = parse_NYT_articles_worddoc(num_files, root_directory)\n",
    "    np.save('parses/' + str(year) + '_' + 'worddoc_parse', worddoc)\n",
    "    end = time.clock()\n",
    "    print \"worddoc parse time: \" + str(end-start)\n",
    "    \n",
    "    #long-term goal: this is dumb. we should probably rewrite the functions to create all three types of parses at the same time\n",
    "    \n",
    "    return (word2vec, seedword, worddoc)\n",
    "\n",
    "def process_year(year):\n",
    "    parses = parser_wrapper(year) #[0]=word2vec, [1]=seedword, [2]=worddoc\n",
    "    #make matrices\n",
    "    #get correlation lists\n",
    "    #make word2vec VSM\n",
    "    #get sentiment of correlation list\n",
    "    #get lda topics \n",
    "    #print out and save / year\n",
    "    #graph across years\n",
    "parser_wrapper(2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def parse_NYT_articles_word2vec(num_files, root_directory='cor-por-a/2006/'):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') #pretrained probabilistic model for parsing sentences\n",
    "    \n",
    "    sentenceList = [];\n",
    "    i = 0;\n",
    "    numSentences = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):                \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    print \"num sentences: \" + str(numSentences)\n",
    "                    return sentenceList\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                #print article_file\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article = article_rep[1];\n",
    "                    for sentence in sent_detector.tokenize(article.strip()):\n",
    "                        tokenized_sentence = []\n",
    "                        sentence = remove_punctuation(sentence.lower())\n",
    "                        for word in sentence.split():\n",
    "                            tokenized_sentence.append(word)\n",
    "                        sentenceList.append(tokenized_sentence)\n",
    "                        numSentences += 1\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i);\n",
    "    print \"num sentences: \" + str(numSentences)\n",
    "    return sentenceList;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "parsing directory cor-por-a/2006/01/02\n",
      "parsing directory cor-por-a/2006/01/03\n",
      "parsing directory cor-por-a/2006/01/04\n",
      "parsing directory cor-por-a/2006/01/05\n",
      "parsing directory cor-por-a/2006/01/06\n",
      "parsing directory cor-por-a/2006/01/07\n",
      "parsing directory cor-por-a/2006/01/08\n",
      "parsing directory cor-por-a/2006/01/09\n",
      "parsing directory cor-por-a/2006/01/10\n",
      "parsing directory cor-por-a/2006/01/11\n",
      "parsing directory cor-por-a/2006/01/12\n",
      "parsing directory cor-por-a/2006/01/13\n",
      "parsing directory cor-por-a/2006/01/14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-676d784c145d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_NYT_articles_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"parse time: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-36ec6ee85a1e>\u001b[0m in \u001b[0;36mparse_NYT_articles_word2vec\u001b[0;34m(num_files, root_directory)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0marticle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdirname_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;31m#print article_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0marticle_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_NYT_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticle_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-1052d6a2ffd4>\u001b[0m in \u001b[0;36mparse_NYT_article\u001b[0;34m(xmlFile)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_NYT_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmlFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmlFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cheson/anaconda2/lib/python2.7/xml/etree/ElementTree.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, parser)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cheson/anaconda2/lib/python2.7/xml/etree/ElementTree.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, source, parser)\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXMLParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTreeBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, movie_reviews, treebank\n",
    "import time\n",
    "\n",
    "start = time.clock()\n",
    "parse = parse_NYT_articles_word2vec(10000000000)\n",
    "end = time.clock()\n",
    "print \"parse time: \" + str(end-start)\n",
    "\n",
    "start = time.clock()\n",
    "ours = Word2Vec(parse, min_count=5)\n",
    "end = time.clock()\n",
    "print \"Word2Vec model generation time: \" + str(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(b.most_similar('money', topn=5))\n",
    "# print(t.most_similar('money', topn=5))\n",
    "print(ours.most_similar('gold', topn=5))\n",
    "\n",
    "#train gensim to generate a vector representation of all words in vocabulary\n",
    "#create new matrix by getting ours['vocab_word'] for all vocab words\n",
    "#pass that new matrix into the semantic orientation model to get sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"2006_word2vec_model\", ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ours.save(\"2006_word2vec_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(ours_2.most_similar('good', topn=5))\n",
    "# print(ours_2.most_similar('good'))\n",
    "ours_2 = Word2Vec.load(\"2006_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'bad', 0.8446568250656128), (u'great', 0.710376501083374), (u'nice', 0.694532573223114), ('decent', 0.675004243850708), (u'tough', 0.6688003540039062)]\n"
     ]
    }
   ],
   "source": [
    "print(ours_2.most_similar('good', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word2vec_mat():\n",
    "    #mat_obj_ppmi -> mat_ppmi, rownames, frequencies\n",
    "    #get vocabulary list\n",
    "    vocab_list = mat_obj_ppmi[1]\n",
    "    #create new vocab list\n",
    "    new_vocab_list = []\n",
    "    #create new mat\n",
    "    new_vsm = []\n",
    "    #for each word in vocab list\n",
    "    for word in vocab_list:\n",
    "    #get its raw vector and append to new mat, append word to new vocab list\n",
    "        new_vsm.append(ours[word]) #or whatever the appropriate syntax is to append to bottom\n",
    "        new_vocab_list.append(word)\n",
    "    #return (new mat, new vocab list)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_orientation(\n",
    "        mat, \n",
    "        rownames,\n",
    "        seeds1=('bad', 'nasty', 'poor', 'negative', 'unfortunate', 'wrong', 'inferior'),\n",
    "        seeds2=('good', 'nice', 'excellent', 'positive', 'fortunate', 'correct', 'superior'),\n",
    "        distfunc=cosine):    \n",
    "    \"\"\"No frills implementation of the semantic Orientation (SO) method of \n",
    "    Turney and Littman. seeds1 and seeds2 should be representative members \n",
    "    of two intutively opposing semantic classes. The method will then try \n",
    "    to rank the vocabulary by its relative association with each seed set.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : 2d np.array\n",
    "        The matrix used to derive the SO ranking.\n",
    "        \n",
    "    rownames : list of str\n",
    "        The names of the rows of `mat` (the vocabulary).\n",
    "        \n",
    "    seeds1 : tuple of str\n",
    "        The default is the negative seed set of Turney and Littman.\n",
    "        \n",
    "    seeds2 : tuple of str\n",
    "        The default is the positive seed set of Turney and Littman.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors. \n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    list of tuples\n",
    "        The vocabulary ranked according to the SO method, with words \n",
    "        closest to `seeds1` at the top and words closest to `seeds2` at the \n",
    "        bottom. Each member of the list is a (word, score) pair.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sm1 = _so_seed_matrix(seeds1, mat, rownames)\n",
    "    sm2 = _so_seed_matrix(seeds2, mat, rownames)\n",
    "    scores = [(rownames[i], _so_row_func(mat[i], sm1, sm2, distfunc)) for i in xrange(len(mat))]\n",
    "    return sorted(scores, key=itemgetter(1), reverse=False)\n",
    "\n",
    "def _so_seed_matrix(seeds, mat, rownames):\n",
    "    indices = [rownames.index(word) for word in seeds if word in rownames]\n",
    "    if not indices:\n",
    "        raise ValueError('The matrix contains no members of the seed set: %s' % \",\".join(seeds))\n",
    "    print indices\n",
    "    print np.array(indices)\n",
    "    return mat[np.array(indices)]\n",
    "    \n",
    "def _so_row_func(row, sm1, sm2, distfunc):\n",
    "    val1 = np.sum([distfunc(row, srow) for srow in sm1])\n",
    "    val2 = np.sum([distfunc(row, srow) for srow in sm2])\n",
    "    return val1 - val2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we tokenize ignoring punctuation?\n",
    "\n",
    "Solution: for each word, look at last letter, if it is in a set of punctuation, remove that punctuation. Or, just strip away punctuation from the entire text in the very beginning. We're losing some degree of information but it is essentially a way of \"normalizing\" the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_rowshit = [u'!', u'):', u');', u'1', u'1/10', u'1/2', u'10', u'10/10', u'100', u'11', u'12', u'13', u'14', u'15', u'17', u'1950', u'1950s', u'1970', u'1980', u'2', u'20', u'2000', u'25', u'3', u'3/10', u'30', u'4', u'4/10', u'40', u'5', u'50', u'6', u'60', u'60s', u'7', u'7/10', u'70', u'70s', u'8', u'8/', u'80', u'80s', u'9', u'90', u':)', u'?', u'a', u'abandoned', u'ability', u'able', u'about', u'above', u'absolute', u'absolutely', u'absurd', u'abuse', u'academy', u'accent', u'accents', u'accept', u'accident', u'accidentally', u'according', u'account', u'accurate', u'achieve', u'across', u'act', u'acted', u'acting', u'action', u'actions', u'actor', u'actors', u'actress', u'actresses', u'acts', u'actual', u'actually', u'adam', u'adaptation', u'add', u'added', u'adding', u'addition', u'adds', u'admit', u'adult', u'adults', u'adventure', u'adventures', u'advice', u'affair', u'afraid', u'africa', u'african', u'after', u'afternoon', u'again', u'against', u'age', u'agent', u'ages', u'ago', u'agree', u'ahead', u\"ain't\", u'air', u'aka', u'al', u'alan', u'alas', u'albert', u'alex', u'alice', u'alien', u'aliens', u'alive', u'all', u'allen', u'allow', u'allowed', u'allows', u'almost', u'alone', u'along', u'already', u'alright', u'also', u'although', u'always', u'am', u'amateur', u'amateurish', u'amazed', u'amazing', u'amazingly', u'america', u'american', u'americans', u'among', u'amongst', u'amount', u'amusing', u'an', u'ancient', u'and', u'anderson', u'andy', u'angel', u'angels', u'anger', u'angle', u'angles', u'angry', u'animal', u'animals', u'animated', u'animation', u'anime', u'ann', u'anna', u'anne', u'annoying', u'another', u'answer', u'answers', u'anthony', u'any', u'anybody', u'anymore', u'anyone', u'anything', u'anyway', u'anywhere', u'apart', u'apartment', u'apparent', u'apparently', u'appeal', u'appealing', u'appear', u'appearance', u'appeared', u'appears', u'appreciate', u'appreciated', u'approach', u'appropriate', u'are', u'area', u\"aren't\", u'arms', u'army', u'around', u'arrives', u'art', u'arthur', u'artist', u'artistic', u'artists', u'arts', u'as', u'ashamed', u'asian', u'aside', u'ask', u'asked', u'asking', u'asks', u'asleep', u'aspect', u'aspects', u'ass', u'assume', u'at', u'atmosphere', u'atrocious', u'attack', u'attacked', u'attacks', u'attempt', u'attempting', u'attempts', u'attention', u'attitude', u'attractive', u'audience', u'audiences', u'aunt', u'australian', u'authentic', u'author', u'available', u'average', u'avoid', u'award', u'awards', u'aware', u'away', u'awesome', u'awful', u'awkward', u'b', u'b-movie', u'baby', u'back', u'background', u'bad', u'badly', u'balance', u'ball', u'band', u'bank', u'bar', u'barbara', u'barely', u'base', u'baseball', u'based', u'basic', u'basically', u'basis', u'batman', u'battle', u'bbc', u'be', u'beach', u'bear', u'beast', u'beat', u'beautiful', u'beautifully', u'beauty', u'became', u'because', u'become', u'becomes', u'becoming', u'bed', u'been', u'before', u'began', u'begin', u'beginning', u'begins', u'behavior', u'behind', u'being', u'belief', u'believable', u'believe', u'believed', u'believes', u'beloved', u'below', u'ben', u'besides', u'best', u'bet', u'better', u'between', u'beyond', u'big', u'bigger', u'biggest', u'bill', u'billy', u'birth', u'bit', u'bits', u'bizarre', u'black', u'blah', u'blair', u'blame', u'bland', u'blind', u'blockbuster', u'blonde', u'blood', u'bloody', u'blow', u'blown', u'blue', u'board', u'boat', u'bob', u'bodies', u'body', u'bollywood', u'bomb', u'bond', u'book', u'books', u'bore', u'bored', u'boring', u'born', u'boss', u'both', u'bother', u'bothered', u'bottom', u'bought', u'bourne', u'box', u'boy', u'boyfriend', u'boys', u'brad', u'brain', u'brave', u'break', u'breaking', u'breaks', u'breath', u'breathtaking', u'brian', u'brief', u'bright', u'brilliant', u'brilliantly', u'bring', u'bringing', u'brings', u'british', u'broadway', u'broken', u'brooks', u'brother', u'brothers', u'brought', u'brown', u'bruce', u'brutal', u'buddy', u'budget', u'build', u'building', u'built', u'bunch', u'burns', u'burt', u'bus', u'business', u'busy', u'but', u'buy', u'buying', u'by', u'c', u'cabin', u'cable', u'cage', u'caine', u'california', u'call', u'called', u'calling', u'calls', u'came', u'cameo', u'camera', u'camp', u'campy', u'can', u\"can't\", u'canadian', u'candy', u'cannot', u'cant', u'capable', u'captain', u'capture', u'captured', u'captures', u'car', u'care', u'career', u'cares', u'caring', u'carried', u'carries', u'carry', u'carrying', u'cars', u'cartoon', u'cartoons', u'case', u'cases', u'cash', u'cast', u'casting', u'castle', u'cat', u'catch', u'category', u'caught', u'cause', u'caused', u'causes', u'cell', u'center', u'central', u'century', u'certain', u'certainly', u'cgi', u'challenge', u'chance', u'change', u'changed', u'changes', u'changing', u'channel', u'character', u\"character's\", u'characters', u'charge', u'charles', u'charlie', u'charm', u'charming', u'chase', u'che', u'cheap', u'check', u'cheesy', u'chemistry', u'chick', u'chief', u'child', u'childhood', u'children', u\"children's\", u'chilling', u'china', u'chinese', u'choice', u'choices', u'choose', u'chose', u'chosen', u'chris', u'christian', u'christmas', u'christopher', u'church', u'cinderella', u'cinema', u'cinematic', u'cinematography', u'circumstances', u'city', u'claim', u'claims', u'claire', u'clark', u'class', u'classic', u'classics', u'clean', u'clear', u'clearly', u'clever', u'clich', u'climax', u'clips', u'close', u'closer', u'closing', u'clothes', u'club', u'clue', u'code', u'cold', u'collection', u'college', u'color', u'colors', u'columbo', u'combination', u'combined', u'come', u'comedic', u'comedies', u'comedy', u'comes', u'comic', u'comical', u'coming', u'comment', u'commentary', u'comments', u'commercial', u'committed', u'common', u'community', u'company', u'compare', u'compared', u'comparison', u'compelling', u'complete', u'completely', u'complex', u'complicated', u'computer', u'concept', u'concerned', u'conclusion', u'conflict', u'confused', u'confusing', u'confusion', u'connection', u'consider', u'considered', u'considering', u'constant', u'constantly', u'contain', u'contains', u'contemporary', u'content', u'context', u'continue', u'continues', u'continuity', u'contrast', u'contrived', u'control', u'conversation', u'convey', u'convince', u'convinced', u'convincing', u'cool', u'cop', u'cops', u'copy', u'core', u'corny', u'correct', u'cost', u'costs', u'costume', u'costumes', u'could', u\"could've\", u\"couldn't\", u'count', u'country', u'couple', u'course', u'court', u'cover', u'covered', u'cowboy', u'crap', u'crappy', u'crash', u'crazy', u'create', u'created', u'creates', u'creating', u'creative', u'creature', u'creatures', u'credit', u'credits', u'creepy', u'crew', u'crime', u'criminal', u'criminals', u'critical', u'criticism', u'critics', u'cross', u'crowd', u'crude', u'cruel', u'cry', u'crying', u'cult', u'cultural', u'culture', u'curious', u'current', u'cut', u'cute', u'cuts', u'cutting', u'd', u'dad', u'daily', u'damn', u'dan', u'dance', u'dancing', u'danger', u'dangerous', u'daniel', u'danny', u'dark', u'darkness', u'date', u'dated', u'daughter', u'daughters', u'david', u'davis', u'day', u'days', u'de', u'dead', u'deadly', u'deal', u'dealing', u'deals', u'dean', u'death', u'deaths', u'debut', u'decade', u'decades', u'decent', u'decide', u'decided', u'decides', u'decision', u'deep', u'deeper', u'deeply', u'definitely', u'degree', u'delight', u'delightful', u'deliver', u'delivered', u'delivers', u'delivery', u'demon', u'demons', u'dennis', u'department', u'depicted', u'depiction', u'depressing', u'depth', u'describe', u'described', u'description', u'desert', u'deserve', u'deserved', u'deserves', u'design', u'designed', u'desire', u'desperate', u'desperately', u'despite', u'destroy', u'destroyed', u'detail', u'details', u'detective', u'determined', u'develop', u'developed', u'development', u'device', u'devil', u'dialog', u'dialogue', u'dick', u'did', u\"didn't\", u'die', u'died', u'dies', u'difference', u'different', u'difficult', u'direct', u'directed', u'directing', u'direction', u'directly', u'director', u\"director's\", u'directors', u'dirty', u'disappointed', u'disappointing', u'disappointment', u'disaster', u'disbelief', u'discover', u'discovered', u'discovers', u'disgusting', u'disney', u'display', u'disturbing', u'do', u'doctor', u'documentary', u'does', u\"doesn't\", u'dog', u'dogs', u'doing', u'dollar', u'dollars', u'don', u\"don't\", u'donald', u'done', u'door', u'double', u'doubt', u'douglas', u'down', u'downright', u'dozen', u'dr', u'drag', u'dragon', u'drama', u'dramatic', u'draw', u'drawn', u'dreadful', u'dream', u'dreams', u'dress', u'dressed', u'drew', u'drinking', u'drive', u'driven', u'driver', u'driving', u'drop', u'drug', u'drugs', u'drunk', u'dry', u'dubbed', u'dude', u'due', u'dull', u'dumb', u'during', u'dvd', u'dying', u'e', u'each', u'earlier', u'early', u'earth', u'easily', u'easy', u'eat', u'eating', u'ed', u'eddie', u'edge', u'edited', u'editing', u'edward', u'effect', u'effective', u'effectively', u'effects', u'effort', u'efforts', u'eight', u'either', u'element', u'elements', u'elizabeth', u'else', u'embarrassed', u'embarrassing', u'emma', u'emotion', u'emotional', u'emotionally', u'emotions', u'empty', u'encounter', u'end', u'ended', u'ending', u'endless', u'ends', u'enemy', u'energy', u'engaging', u'england', u'english', u'enjoy', u'enjoyable', u'enjoyed', u'enjoying', u'enough', u'enter', u'entertain', u'entertained', u'entertaining', u'entertainment', u'entire', u'entirely', u'environment', u'epic', u'episode', u'episodes', u'equally', u'era', u'eric', u'erotic', u'escape', u'escapes', u'especially', u'essential', u'essentially', u'established', u'etc', u'europe', u'european', u'even', u'evening', u'event', u'events', u'eventually', u'ever', u'every', u'everybody', u'everyday', u'everyone', u'everything', u'everywhere', u'evidence', u'evil', u'exact', u'exactly', u'example', u'examples', u'excellent', u'except', u'exception', u'excited', u'excitement', u'exciting', u'excuse', u'executed', u'execution', u'exist', u'existence', u'exists', u'expect', u'expectations', u'expected', u'expecting', u'experience', u'experienced', u'experiences', u'experiment', u'expert', u'explain', u'explained', u'explains', u'explanation', u'exploitation', u'express', u'expression', u'extent', u'extra', u'extraordinary', u'extras', u'extreme', u'extremely', u'eye', u'eyes', u'f', u'fabulous', u'face', u'faces', u'facial', u'fact', u'factor', u'facts', u'fail', u'failed', u'fails', u'failure', u'fair', u'fairly', u'fairy', u'faith', u'faithful', u'fake', u'fall', u'fallen', u'falling', u'falls', u'false', u'fame', u'familiar', u'families', u'family', u'famous', u'fan', u'fans', u'fantastic', u'fantasy', u'far', u'fare', u'fascinating', u'fashion', u'fast', u'fat', u'fate', u'father', u\"father's\", u'fault', u'favor', u'favorite', u'favorites', u'favourite', u'fear', u'feature', u'featured', u'features', u'featuring', u'feel', u'feeling', u'feelings', u'feels', u'feet', u'fell', u'fellow', u'felt', u'female', u'festival', u'few', u'fiction', u'fictional', u'field', u'fight', u'fighting', u'fights', u'figure', u'figured', u'figures', u'fill', u'filled', u'film', u\"film's\", u'film-making', u'filmed', u'filming', u'filmmaker', u'filmmakers', u'films', u'final', u'finale', u'finally', u'find', u'finding', u'finds', u'fine', u'finest', u'finish', u'finished', u'fire', u'first', u'fit', u'fits', u'five', u'flashback', u'flashbacks', u'flat', u'flaws', u'flesh', u'flick', u'flicks', u'flight', u'floor', u'flow', u'fly', u'flying', u'focus', u'focused', u'focuses', u'folks', u'follow', u'followed', u'following', u'follows', u'food', u'fool', u'foot', u'footage', u'football', u'for', u'force', u'forced', u'forces', u'ford', u'foreign', u'forest', u'forever', u'forget', u'forgettable', u'forgot', u'forgotten', u'form', u'format', u'former', u'formula', u'forth', u'fortunately', u'forward', u'foster', u'found', u'four', u'fourth', u'fox', u'frame', u'france', u'frank', u'frankly', u'fred', u'freddy', u'free', u'freedom', u'freeman', u'french', u'frequently', u'fresh', u'friday', u'friend', u'friendly', u'friends', u'friendship', u'frightening', u'from', u'front', u'fu', u'full', u'fully', u'fun', u'funnier', u'funniest', u'funny', u'further', u'future', u'g', u'gags', u'game', u'games', u'gang', u'gangster', u'garbage', u'gary', u'gas', u'gave', u'gay', u'gem', u'gene', u'general', u'generally', u'generation', u'genius', u'genre', u'genuine', u'genuinely', u'george', u'german', u'germany', u'get', u'gets', u'getting', u'ghost', u'ghosts', u'giant', u'girl', u'girlfriend', u'girls', u'give', u'given', u'gives', u'giving', u'glad', u'go', u'god', u'goes', u'going', u'gold', u'golden', u'gone', u'gonna', u'good', u'goofy', u'gordon', u'gore', u'gorgeous', u'gory', u'got', u'gotten', u'government', u'grace', u'grade', u'grand', u'grant', u'granted', u'graphic', u'graphics', u'gratuitous', u'grave', u'great', u'greater', u'greatest', u'green', u'grew', u'grim', u'gritty', u'ground', u'group', u'grow', u'growing', u'grown', u'gruesome', u'guard', u'guess', u'guilty', u'gun', u'guns', u'guy', u'guys', u'h', u'ha', u'had', u\"hadn't\", u'hair', u'half', u'halfway', u'hall', u'halloween', u'hand', u'handle', u'handled', u'hands', u'handsome', u'hanging', u'happen', u'happened', u'happening', u'happens', u'happiness', u'happy', u'hard', u'hardly', u'hardy', u'harris', u'harry', u'harsh', u'has', u\"hasn't\", u'hat', u'hate', u'hated', u'haunted', u'haunting', u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he's\", u'head', u'heads', u'hear', u'heard', u'hearing', u'heart', u'heaven', u'heavily', u'heavy', u'heck', u'held', u'hell', u'help', u'helped', u'helping', u'helps', u'henry', u'her', u'here', u\"here's\", u'hero', u'heroes', u'heroine', u'herself', u'hey', u'hidden', u'hide', u'high', u'higher', u'highlight', u'highly', u'hilarious', u'hill', u'him', u'himself', u'hired', u'his', u'historical', u'history', u'hit', u'hitchcock', u'hitler', u'hits', u'hoffman', u'hold', u'holding', u'holds', u'holes', u'hollywood', u'home', u'honest', u'honestly', u'hong', u'honor', u'hope', u'hopefully', u'hopes', u'hoping', u'horrible', u'horribly', u'horrific', u'horror', u'horse', u'hospital', u'hot', u'hotel', u'hour', u'hours', u'house', u'how', u'howard', u'however', u'huge', u'human', u'humanity', u'humans', u'humor', u'humorous', u'humour', u'hunt', u'hunter', u'hurt', u'husband', u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'ice', u'idea', u'ideas', u'identity', u'idiot', u'if', u'ignore', u'ii', u'ill', u'image', u'imagery', u'images', u'imagination', u'imagine', u'imdb', u'immediately', u'impact', u'important', u'impossible', u'impressed', u'impression', u'impressive', u'in', u'include', u'included', u'includes', u'including', u'incredible', u'incredibly', u'indeed', u'independent', u'india', u'indian', u'indie', u'individual', u'industry', u'inept', u'influence', u'information', u'initial', u'initially', u'inner', u'innocent', u'insane', u'inside', u'insight', u'inspector', u'inspiration', u'inspired', u'instance', u'instead', u'insult', u'intellectual', u'intelligence', u'intelligent', u'intended', u'intense', u'intensity', u'intentions', u'interest', u'interested', u'interesting', u'international', u'interpretation', u'interview', u'interviews', u'into', u'intriguing', u'introduced', u'introduction', u'invisible', u'involved', u'involves', u'involving', u'irish', u'ironic', u'irritating', u'is', u'island', u\"isn't\", u'issue', u'issues', u'it', u\"it's\", u'italian', u'its', u'itself', u'j', u'jack', u'jackie', u'jackson', u'jail', u'james', u'jane', u'japan', u'japanese', u'jason', u'jean', u'jeff', u'jennifer', u'jerry', u'jessica', u'jesus', u'jewish', u'jim', u'jimmy', u'joan', u'job', u'jobs', u'joe', u'john', u'johnny', u'johnson', u'join', u'joke', u'jokes', u'jon', u'jones', u'joseph', u'journey', u'joy', u'jr', u'judge', u'julia', u'julie', u'jump', u'jumps', u'jungle', u'junk', u'just', u'justice', u'k', u'kate', u'keaton', u'keep', u'keeping', u'keeps', u'kelly', u'kept', u'kevin', u'key', u'kick', u'kid', u'kids', u'kill', u'killed', u'killer', u'killers', u'killing', u'kills', u'kim', u'kind', u'kinda', u'kinds', u'king', u'kiss', u'knew', u'know', u'knowing', u'knowledge', u'known', u'knows', u'kong', u'kung', u'l', u'la', u'lack', u'lacking', u'lacks', u'ladies', u'lady', u'lake', u'lame', u'land', u'lane', u'language', u'large', u'largely', u'larry', u'last', u'late', u'later', u'latest', u'latter', u'laugh', u'laughable', u'laughed', u'laughing', u'laughs', u'laughter', u'laura', u'law', u'lawyer', u'lazy', u'lead', u'leader', u'leading', u'leads', u'league', u'learn', u'learned', u'learning', u'learns', u'least', u'leave', u'leaves', u'leaving', u'led', u'lee', u'left', u'legend', u'legendary', u'legs', u'length', u'lesbian', u'leslie', u'less', u'lesson', u'let', u\"let's\", u'lets', u'level', u'levels', u'lewis', u'lie', u'lies', u'life', u'lifetime', u'light', u'lighting', u'lights', u'likable', u'like', u'liked', u'likely', u'likes', u'limited', u'line', u'lines', u'lisa', u'list', u'listen', u'listening', u'literally', u'little', u'live', u'lived', u'lives', u'living', u'local', u'location', u'locations', u'locked', u'logic', u'london', u'lonely', u'long', u'longer', u'look', u'looked', u'looking', u'looks', u'loose', u'lord', u'lose', u'loses', u'losing', u'loss', u'lost', u'lot', u'lots', u'loud', u'louis', u'lousy', u'love', u'loved', u'lovely', u'lover', u'lovers', u'loves', u'loving', u'low', u'low-budget', u'lower', u'luck', u'lucky', u'ludicrous', u'lugosi', u'luke', u'lynch', u'm', u'machine', u'mad', u'made', u'madness', u'magic', u'magical', u'magnificent', u'main', u'mainly', u'mainstream', u'major', u'majority', u'make', u'make-up', u'makers', u'makes', u'makeup', u'making', u'male', u'man', u\"man's\", u'manage', u'managed', u'manager', u'manages', u'manner', u'mansion', u'many', u'maria', u'marie', u'mark', u'market', u'marriage', u'married', u'marry', u'martial', u'martin', u'marvelous', u'mary', u'mask', u'massive', u'master', u'masterpiece', u'match', u'material', u'matrix', u'matt', u'matter', u'matters', u'mature', u'max', u'may', u'maybe', u'me', u'mean', u'meaning', u'means', u'meant', u'meanwhile', u'media', u'mediocre', u'meet', u'meeting', u'meets', u'melodrama', u'member', u'members', u'memorable', u'memories', u'memory', u'men', u'mental', u'mention', u'mentioned', u'mere', u'merely', u'mess', u'message', u'met', u'metal', u'mexican', u'mexico', u'mgm', u'michael', u'michelle', u'mid', u'middle', u'midnight', u'might', u'mike', u'mildly', u'miles', u'military', u'million', u'mind', u'minds', u'mine', u'minor', u'minute', u'minutes', u'mirror', u'miss', u'missed', u'missing', u'mission', u'mistake', u'mistakes', u'mix', u'mixed', u'model', u'modern', u'mom', u'moment', u'moments', u'money', u'monster', u'monsters', u'months', u'mood', u'moon', u'moore', u'moral', u'more', u'morgan', u'morning', u'most', u'mostly', u'mother', u'motion', u'mountain', u'mouth', u'move', u'moved', u'movement', u'moves', u'movie', u\"movie's\", u'movies', u'moving', u'mr', u'mrs', u'ms', u'mst', u'much', u'multiple', u'murder', u'murdered', u'murderer', u'murders', u'murphy', u'music', u'musical', u'musicals', u'must', u'my', u'myself', u'mysterious', u'mystery', u'n', u'naive', u'naked', u'name', u'named', u'names', u'nancy', u'narration', u'narrative', u'nasty', u'nation', u'national', u'native', u'natural', u'naturally', u'nature', u'navy', u'near', u'nearly', u'necessarily', u'necessary', u'ned', u'need', u'needed', u'needless', u'needs', u'negative', u'neither', u'network', u'never', u'nevertheless', u'new', u'news', u'next', u'nice', u'nicely', u'nick', u'night', u'nightmare', u'no', u'nobody', u'noir', u'nominated', u'none', u'nonetheless', u'nonsense', u'nor', u'normal', u'normally', u'north', u'not', u'notable', u'note', u'nothing', u'notice', u'noticed', u'notorious', u'novel', u'novels', u'now', u'nowadays', u'nowhere', u'nude', u'nudity', u'number', u'numbers', u'numerous', u'o', u'obnoxious', u'obsessed', u'obsession', u'obvious', u'obviously', u'occasional', u'occasionally', u'odd', u'oddly', u'of', u'off', u'offensive', u'offer', u'offered', u'offers', u'office', u'officer', u'often', u'oh', u'ok', u'okay', u'old', u'older', u'oliver', u'on', u'once', u'one', u\"one's\", u'ones', u'only', u'onto', u'open', u'opening', u'opens', u'opera', u'opinion', u'opportunity', u'opposite', u'or', u'order', u'ordinary', u'original', u'originality', u'originally', u'oscar', u'other', u'others', u'otherwise', u'our', u'out', u'outside', u'outstanding', u'over', u'over-the-top', u'overall', u'overly', u'own', u'owner', u'p', u'pace', u'paced', u'pacing', u'pacino', u'page', u'paid', u'pain', u'painful', u'painfully', u'paint', u'pair', u'paper', u'par', u'parents', u'paris', u'park', u'parker', u'parody', u'part', u'particular', u'particularly', u'partner', u'parts', u'party', u'pass', u'passed', u'passing', u'passion', u'past', u'path', u'pathetic', u'patrick', u'paul', u'pay', u'paying', u'peace', u'people', u\"people's\", u'perfect', u'perfectly', u'performance', u'performances', u'performed', u'perhaps', u'period', u'person', u'personal', u'personalities', u'personality', u'personally', u'perspective', u'pet', u'peter', u'phone', u'photography', u'physical', u'pick', u'picked', u'picks', u'picture', u'pictures', u'piece', u'pieces', u'pile', u'pilot', u'pitt', u'pity', u'place', u'placed', u'places', u'plain', u'plan', u'plane', u'planet', u'plans', u'play', u'played', u'player', u'players', u'playing', u'plays', u'pleasant', u'please', u'pleasure', u'plenty', u'plot', u'plots', u'plus', u'poignant', u'point', u'pointless', u'points', u'police', u'political', u'politics', u'poor', u'poorly', u'pop', u'popular', u'porn', u'portray', u'portrayal', u'portrayed', u'portraying', u'portrays', u'position', u'positive', u'possible', u'possibly', u'post', u'potential', u'powell', u'power', u'powerful', u'powers', u'practically', u'praise', u'predictable', u'prefer', u'pregnant', u'premise', u'prepared', u'presence', u'present', u'presentation', u'presented', u'presents', u'president', u'pretentious', u'pretty', u'previous', u'previously', u'price', u'priest', u'prime', u'prince', u'princess', u'print', u'prior', u'prison', u'private', u'probably', u'problem', u'problems', u'process', u'produce', u'produced', u'producer', u'producers', u'product', u'production', u'productions', u'professional', u'professor', u'program', u'project', u'promise', u'promising', u'propaganda', u'proper', u'properly', u'protagonist', u'protect', u'proud', u'prove', u'proved', u'proves', u'provide', u'provided', u'provides', u'psycho', u'psychological', u'public', u'pull', u'pulled', u'pulls', u'punch', u'pure', u'purely', u'purpose', u'put', u'puts', u'putting', u'qualities', u'quality', u'queen', u'quest', u'question', u'questions', u'quick', u'quickly', u'quiet', u'quirky', u'quite', u'r', u'race', u'rachel', u'racist', u'radio', u'rain', u'raise', u'raised', u'ran', u'random', u'range', u'rape', u'rare', u'rarely', u'rate', u'rated', u'rather', u'rating', u'ratings', u'raw', u'ray', u'reach', u'reaction', u'read', u'reading', u'ready', u'real', u'realism', u'realistic', u'reality', u'realize', u'realized', u'realizes', u'really', u'reason', u'reasons', u'recall', u'received', u'recent', u'recently', u'recognize', u'recommend', u'recommended', u'record', u'red', u'redeeming', u'reference', u'references', u'refreshing', u'regard', u'regarding', u'regret', u'regular', u'relate', u'related', u'relationship', u'relationships', u'relatively', u'release', u'released', u'relief', u'religion', u'religious', u'remain', u'remains', u'remake', u'remarkable', u'remember', u'remembered', u'remind', u'reminded', u'reminds', u'reminiscent', u'remote', u'remotely', u'rent', u'rental', u'rented', u'renting', u'repeated', u'replaced', u'reporter', u'reputation', u'required', u'rescue', u'research', u'respect', u'responsible', u'rest', u'result', u'results', u'retarded', u'return', u'returns', u'reveal', u'revealed', u'reveals', u'revenge', u'review', u'reviewer', u'reviewers', u'reviews', u'revolution', u'rich', u'richard', u'ride', u'ridiculous', u'right', u'rights', u'ring', u'rings', u'rip', u'rise', u'risk', u'rival', u'river', u'road', u'rob', u'robert', u'robin', u'robot', u'rock', u'roger', u'rogers', u'role', u'roles', u'roll', u'rolling', u'romance', u'romantic', u'ron', u'room', u'rose', u'rough', u'round', u'routine', u'roy', u'rubbish', u'ruin', u'ruined', u'rule', u'rules', u'run', u'running', u'runs', u'russell', u'russian', u'ryan', u's', u'sad', u'sadly', u'safe', u'said', u'sake', u'sam', u'same', u'san', u'santa', u'sarah', u'sat', u'satire', u'satisfying', u'saturday', u'save', u'saved', u'saving', u'saw', u'say', u'saying', u'says', u'scale', u'scare', u'scared', u'scares', u'scary', u'scenario', u'scene', u'scenery', u'scenes', u'school', u'sci-fi', u'science', u'scientist', u'score', u'scott', u'scream', u'screaming', u'screen', u'screening', u'screenplay', u'script', u'sea', u'sean', u'search', u'season', u'seasons', u'seat', u'second', u'seconds', u'secret', u'section', u'security', u'see', u'seeing', u'seek', u'seem', u'seemed', u'seemingly', u'seems', u'seen', u'sees', u'segment', u'self', u'sell', u'send', u'sense', u'sensitive', u'sent', u'sequel', u'sequels', u'sequence', u'sequences', u'serial', u'series', u'serious', u'seriously', u'serve', u'served', u'serves', u'service', u'set', u'sets', u'setting', u'settings', u'seven', u'several', u'sex', u'sexual', u'sexy', u'shadow', u'shakespeare', u'shallow', u'shame', u'share', u'sharp', u'she', u\"she's\", u'sheer', u'sheriff', u'ship', u'shock', u'shocked', u'shocking', u'shoot', u'shooting', u'shop', u'short', u'shot', u'shots', u'should', u\"shouldn't\", u'show', u'showed', u'shower', u'showing', u'shown', u'shows', u'shut', u'sick', u'side', u'sides', u'sidney', u'sight', u'sign', u'significant', u'silent', u'silly', u'similar', u'simon', u'simple', u'simply', u'sinatra', u'since', u'sing', u'singer', u'singing', u'single', u'sinister', u'sir', u'sister', u'sisters', u'sit', u'sitcom', u'site', u'sitting', u'situation', u'situations', u'six', u'skill', u'skills', u'skin', u'skip', u'sky', u'slapstick', u'slasher', u'sleazy', u'sleep', u'sleeping', u'slightly', u'slow', u'slowly', u'small', u'smart', u'smile', u'smith', u'so', u'so-called', u'soap', u'social', u'society', u'soft', u'sold', u'soldier', u'soldiers', u'solid', u'some', u'somebody', u'somehow', u'someone', u'something', u'sometimes', u'somewhat', u'somewhere', u'son', u'song', u'songs', u'soon', u'sorry', u'sort', u'sorts', u'soul', u'sound', u'sounded', u'sounds', u'soundtrack', u'source', u'south', u'southern', u'space', u'spanish', u'speak', u'speaking', u'speaks', u'special', u'spectacular', u'speech', u'speed', u'spend', u'spends', u'spent', u'spirit', u'spite', u'spoil', u'spoiler', u'spoilers', u'spoof', u'sports', u'spot', u'spy', u'stage', u'stand', u'standard', u'standards', u'standing', u'stands', u'stanley', u'star', u'starred', u'starring', u'stars', u'start', u'started', u'starting', u'starts', u'state', u'statement', u'states', u'station', u'status', u'stay', u'stayed', u'stays', u'steal', u'steals', u'step', u'stephen', u'stereotypes', u'stereotypical', u'steve', u'steven', u'stewart', u'stick', u'still', u'stock', u'stolen', u'stomach', u'stone', u'stop', u'stopped', u'stops', u'store', u'stories', u'story', u'storyline', u'storytelling', u'straight', u'strange', u'strangely', u'street', u'streets', u'strength', u'strong', u'strongly', u'structure', u'struggle', u'struggling', u'stuck', u'student', u'students', u'studio', u'studios', u'study', u'stuff', u'stunning', u'stupid', u'stupidity', u'style', u'stylish', u'subject', u'substance', u'subtitles', u'subtle', u'succeeds', u'success', u'successful', u'successfully', u'such', u'suck', u'sucked', u'sucks', u'sudden', u'suddenly', u'suffer', u'suffering', u'suffers', u'suggest', u'suicide', u'suit', u'summary', u'summer', u'sun', u'sunday', u'super', u'superb', u'superior', u'superman', u'supernatural', u'support', u'supporting', u'suppose', u'supposed', u'supposedly', u'sure', u'surely', u'surface', u'surprise', u'surprised', u'surprises', u'surprising', u'surprisingly', u'surreal', u'survive', u'susan', u'suspect', u'suspects', u'suspense', u'suspenseful', u'sweet', u'sword', u'sympathetic', u'sympathy', u'system', u't', u'table', u'take', u'taken', u'takes', u'taking', u'tale', u'talent', u'talented', u'talents', u'tales', u'talk', u'talking', u'talks', u'tape', u'target', u'tarzan', u'task', u'taste', u'taylor', u'teacher', u'team', u'tears', u'technical', u'technically', u'technology', u'ted', u'tedious', u'teen', u'teenage', u'teenager', u'teenagers', u'teens', u'teeth', u'television', u'tell', u'telling', u'tells', u'ten', u'tend', u'tension', u'term', u'terms', u'terrible', u'terribly', u'terrific', u'terror', u'test', u'texas', u'than', u'thank', u'thankfully', u'thanks', u'that', u\"that's\", u'thats', u'the', u'theater', u'theaters', u'theatre', u'theatrical', u'their', u'them', u'theme', u'themes', u'themselves', u'then', u'theory', u'there', u\"there's\", u'therefore', u'these', u'they', u\"they're\", u\"they've\", u'thin', u'thing', u'things', u'think', u'thinking', u'thinks', u'third', u'this', u'thomas', u'thoroughly', u'those', u'though', u'thought', u'thoughts', u'three', u'thriller', u'thrilling', u'through', u'throughout', u'throw', u'throwing', u'thrown', u'throws', u'thus', u'tight', u'till', u'tim', u'time', u'times', u'timing', u'tiny', u'tired', u'titanic', u'title', u'titles', u'to', u'today', u\"today's\", u'together', u'told', u'tom', u'tone', u'tony', u'too', u'took', u'top', u'topic', u'torture', u'total', u'totally', u'touch', u'touched', u'touches', u'touching', u'tough', u'toward', u'towards', u'town', u'track', u'tradition', u'traditional', u'tragedy', u'tragic', u'trailer', u'train', u'training', u'trapped', u'trash', u'travel', u'treasure', u'treat', u'treated', u'treatment', u'tree', u'trek', u'trick', u'tried', u'tries', u'trilogy', u'trip', u'trouble', u'truck', u'true', u'truly', u'trust', u'truth', u'try', u'trying', u'turkey', u'turn', u'turned', u'turning', u'turns', u'tv', u'twenty', u'twice', u'twist', u'twisted', u'twists', u'two', u'type', u'types', u'typical', u'u', u'ugly', u'uk', u'ultimate', u'ultimately', u'unable', u'unbelievable', u'uncle', u'unconvincing', u'under', u'underground', u'underrated', u'understand', u'understanding', u'understood', u'unexpected', u'unfortunate', u'unfortunately', u'unfunny', u'uninteresting', u'unique', u'united', u'universal', u'universe', u'unknown', u'unless', u'unlike', u'unlikely', u'unnecessary', u'unrealistic', u'until', u'unusual', u'up', u'upon', u'urban', u'us', u'usa', u'use', u'used', u'uses', u'using', u'usual', u'usually', u'utter', u'utterly', u'v', u'vacation', u'value', u'values', u'vampire', u'vampires', u'van', u'variety', u'various', u'vehicle', u'version', u'versions', u'very', u'veteran', u'vhs', u'via', u'victim', u'victims', u'victor', u'victoria', u'video', u'vietnam', u'view', u'viewed', u'viewer', u'viewers', u'viewing', u'views', u'village', u'villain', u'villains', u'violence', u'violent', u'virtually', u'vision', u'visit', u'visual', u'visually', u'visuals', u'voice', u'voices', u'von', u'vote', u'vs', u'w', u'wait', u'waiting', u'walk', u'walked', u'walking', u'walks', u'wall', u'walter', u'want', u'wanted', u'wanting', u'wants', u'war', u'warm', u'warned', u'warner', u'warning', u'wars', u'was', u'washington', u\"wasn't\", u'waste', u'wasted', u'watch', u'watchable', u'watched', u'watching', u'water', u'wave', u'way', u'wayne', u'ways', u'we', u\"we're\", u\"we've\", u'weak', u'weapons', u'wear', u'wearing', u'wears', u'wedding', u'week', u'weekend', u'weeks', u'weird', u'welcome', u'well', u'welles', u'went', u'were', u\"weren't\", u'werewolf', u'west', u'western', u'westerns', u'what', u\"what's\", u'whatever', u'whatsoever', u'when', u'whenever', u'where', u'whether', u'which', u'while', u'whilst', u'white', u'who', u\"who's\", u'whoever', u'whole', u'whom', u'whose', u'why', u'wide', u'wife', u'wild', u'will', u'william', u'williams', u'willing', u'wilson', u'win', u'wind', u'window', u'winner', u'winning', u'wins', u'wise', u'wish', u'wit', u'witch', u'with', u'within', u'without', u'witness', u'witty', u'woman', u'women', u'won', u\"won't\", u'wonder', u'wonderful', u'wonderfully', u'wondering', u'wood', u'wooden', u'woods', u'woody', u'word', u'words', u'work', u'worked', u'working', u'works', u'world', u'worse', u'worst', u'worth', u'worthwhile', u'worthy', u'would', u\"would've\", u\"wouldn't\", u'wow', u'write', u'writer', u'writers', u'writing', u'written', u'wrong', u'wrote', u'x', u'yeah', u'year', u'year-old', u'years', u'yes', u'yet', u'york', u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'young', u'younger', u'your', u'yourself', u'youth', u'zero', u'zombie', u'zombies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.array(mat[0])\n",
    "print np.array(temp_rowshit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_rownames = ['hello', 'test', 'pie', 'dirty', 'bad', 'good']\n",
    "#(mat, lda_vocab)\n",
    "so = semantic_orientation(mat=np.array(mat[0]), rownames=mat[1])\n",
    "so[:5]\n",
    "so[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so = semantic_orientation(mat=np.array(mat[0]), rownames=mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert from list to easily searchable hashmap\n",
    "word_scores = dict()\n",
    "for tup in so:\n",
    "    word_scores[tup[0]] = tup[1]\n",
    "    \n",
    "def get_semantic_score(word_list):\n",
    "    score = 0\n",
    "    for word in word_list:\n",
    "        if word in word_scores:\n",
    "            score += word_scores[word]\n",
    "        else:\n",
    "            print 'not in vocab'\n",
    "    return score\n",
    "\n",
    "get_semantic_score(['good', 'bad', 'john'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"你好\".encode('utf-8')\n",
    "encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent of\n",
    "\n",
    "\"你好\".decode().encode('utf-8')\n",
    "But the decode fails because the string isn't valid ascii. That's why you get a complaint about not being able to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR/AND\n",
    "Takes in a dict of corpus:list of words and returns a dict of corpus:XOR words and dict of corpus:AND words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyList = ['black', 'block', 'beer']\n",
    "\n",
    "def XOR(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first ^ second\n",
    "def AND(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first & second\n",
    "\n",
    "print 'XOR'\n",
    "print XOR(toyList, neighbors_word_list)\n",
    "print 'AND'\n",
    "print AND(toyList, neighbors_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "Takes in a matrix M and correlation list L. Using t-sne, produces a word cloud which represents correlation between all terms. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix: array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold='nan')\n",
    "\n",
    "def word_cloud_preprocessing(words, matrix=mat_ppmi):\n",
    "    output = []\n",
    "    for word in words:\n",
    "        ind = matrix[1].index(word)\n",
    "        output.append(matrix[0][ind])\n",
    "    return output\n",
    "processed_mat = word_cloud_preprocessing(neighbors_word_list)\n",
    "print processed_mat\n",
    "\n",
    "def word_cloud(corr_list): #i think its processed_mat / didn't tsne take in a vector of labels as well?\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    tsne_matrix = model.fit_transform(corr_list)\n",
    "    \n",
    "word_cloud(processed_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modeling, currently using dummy data from lda.datasets\n",
    "\n",
    "NOTE: rerunning can cause relabeling, which means that topic 0 in the first run might now be topic 15 in the next run, so don't be worried if the topic numbers change from run to run\n",
    "\n",
    "run this on the command line first: pip install --user lda\n",
    "\n",
    "https://pypi.python.org/pypi/lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vec = CountVectorizer(stop_words='english')\n",
    "# data = vec.fit_transform(wdmat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #imports\n",
    "\n",
    "# from __future__ import division, print_function\n",
    "\n",
    "# np.set_printoptions(threshold='nan')\n",
    "\n",
    "# #use pip show lda to find the path of where it's installed for you and modify the path append line below with your location\n",
    "# import sys\n",
    "# sys.path.append('/Users/theodorachu/.local/lib/python2.7/site-packages')\n",
    "\n",
    "import lda\n",
    "# import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X): <type 'numpy.ndarray'>\n",
      "shape: (1000, 38772)\n",
      "\n",
      "type(vocab): <type 'tuple'>\n",
      "len(vocab): 38772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document-term matrix\n",
    "X = mat.transpose()\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# the vocab\n",
    "vocab = tuple(lda_vocab)\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "\n",
    "# titles for each story\n",
    "# titles = lda.datasets.load_reuters_titles()\n",
    "# print(\"type(titles): {}\".format(type(titles)))\n",
    "# print(\"len(titles): {}\\n\".format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(rel_X): <type 'numpy.ndarray'>\n",
      "shape: (120, 38772)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input is the equivalence set\n",
    "# output is doc-term matrix of just relevant docs\n",
    "def find_word(input):\n",
    "    indices = [i for i, x in enumerate(lda_vocab) if x in input]\n",
    "    mod_mat = (X.transpose()[indices]).transpose()\n",
    "    mod_mat_indices = mod_mat.sum(axis = 1) != 0\n",
    "    return X[mod_mat_indices]\n",
    "rel_X = find_word(equivalence_set)\n",
    "print(\"type(rel_X): {}\".format(type(rel_X)))\n",
    "print(\"shape: {}\\n\".format(rel_X.shape))\n",
    "X = rel_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc id: 0 word id: 1\n",
      "-- count: 0\n",
      "-- word : -\n"
     ]
    }
   ],
   "source": [
    "#example print statements\n",
    "#gets word 3117 from document 0\n",
    "\n",
    "doc_id = 0\n",
    "word_id = 1\n",
    "\n",
    "print(\"doc id: {} word id: {}\".format(doc_id, word_id))\n",
    "print(\"-- count: {}\".format(X[doc_id, word_id]))\n",
    "print(\"-- word : {}\".format(vocab[word_id]))\n",
    "#print(\"-- doc  : {}\".format(titles[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.35389610e-01,   1.19047619e-03,   1.08225108e-04, ...,\n",
       "          2.39177489e-02,   3.28030303e-01,   3.35497835e-03],\n",
       "       [  8.07502842e-02,   7.99545282e-03,   1.55740811e-02, ...,\n",
       "          6.10079576e-03,   3.42591891e-01,   1.84956423e-01],\n",
       "       [  3.70300752e-02,   6.26566416e-05,   2.32456140e-02, ...,\n",
       "          6.26566416e-05,   3.13972431e-01,   6.26566416e-05],\n",
       "       ..., \n",
       "       [  3.97590361e-02,   1.72117040e-04,   1.72117040e-04, ...,\n",
       "          1.89328744e-03,   4.08089501e-01,   1.72117040e-04],\n",
       "       [  1.02798507e-01,   1.51119403e-02,   3.91791045e-03, ...,\n",
       "          1.86567164e-04,   3.45335821e-01,   1.86567164e-04],\n",
       "       [  1.53894737e-01,   2.10526316e-04,   2.31578947e-03, ...,\n",
       "          8.63157895e-03,   2.90736842e-01,   2.31578947e-03]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(topic_word): <type 'numpy.ndarray'>\n",
      "shape: (20, 38772)\n"
     ]
    }
   ],
   "source": [
    "#topic-word probabilities\n",
    "#shape: (num topics, num words)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "print(\"shape: {}\".format(topic_word.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0 sum: 1.0\n",
      "topic: 1 sum: 1.0\n",
      "topic: 2 sum: 1.0\n",
      "topic: 3 sum: 1.0\n",
      "topic: 4 sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    sum_pr = sum(topic_word[n,:])\n",
    "    print(\"topic: {} sum: {}\".format(n, sum_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Topic 0\n",
      "- one -- two also people last years work still first\n",
      "*Topic 1\n",
      "- said police team game football brown carroll c fans coach\n",
      "*Topic 2\n",
      "- said mr ms room christmas york hotel square sticky apartment\n",
      "*Topic 3\n",
      "- city -- ny1 new mayor first giants york world game\n",
      "*Topic 4\n",
      "- list lists magazine bar ms fashion lunch top hot like\n",
      "*Topic 5\n",
      "- school ms schools said mother dr children johnson family choir\n",
      "*Topic 6\n",
      "- mr years died president state first law court department member\n",
      "*Topic 7\n",
      "- book black collection life museum norris photography books history writing\n",
      "*Topic 8\n",
      "- ms judge alito said party nelson house nicolas republican estate\n",
      "*Topic 9\n",
      "- p -- street 5 10 203 7 jan 1 saturday\n",
      "*Topic 10\n",
      "- -- one another good make less many place among something\n",
      "*Topic 11\n",
      "- cheese restaurant chicken food restaurants wine dishes sauce 1 street\n",
      "*Topic 12\n",
      "- oil water tv percent use 99 7 home set money\n",
      "*Topic 13\n",
      "- women world people cultural men human culture bark find century\n",
      "*Topic 14\n",
      "- quagga rau skin cute zebra animal project plains zebras like\n",
      "*Topic 15\n",
      "- said rights united nations human patients commission study council countries\n",
      "*Topic 16\n",
      "- ukraine tymoshenko yushchenko israeli minister prime russia israel political revolution\n",
      "*Topic 17\n",
      "- mr music theater bloom opera grier play director cash prison\n",
      "*Topic 18\n",
      "-  said new like would time black even made year\n",
      "*Topic 19\n",
      "- hartford connecticut world perth australia barrier city river north voorhees\n"
     ]
    }
   ],
   "source": [
    "#spits out top n words for each topic by probability\n",
    "\n",
    "n = 10\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(doc_topic): <type 'numpy.ndarray'>\n",
      "shape: (120, 20)\n"
     ]
    }
   ],
   "source": [
    "#document-topic probabilities\n",
    "#shape: (num documents, num topics)\n",
    "\n",
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3, 3: 4, 4: 1, 6: 3, 8: 1, 11: 5, 12: 3, 13: 1, 15: 5, 18: 88, 19: 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = {}\n",
    "for i in range(115):\n",
    "    topic = doc_topic[i].argmax()\n",
    "    if topic in dist:\n",
    "        dist[topic] += 1\n",
    "    else:\n",
    "        dist[topic] = 1\n",
    "    #print(\"{} (top topic: {})\".format(i, doc_topic[i].argmax()))\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualizing the inference - matlab setup/imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now the plots don't print? it just throws the notebook into busy mode for a very long time so not sure if something is off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theodorachu/anaconda2/lib/python2.7/site-packages/matplotlib/tight_layout.py:222: UserWarning: tight_layout : falling back to Agg renderer\n",
      "  warnings.warn(\"tight_layout : falling back to Agg renderer\")\n"
     ]
    }
   ],
   "source": [
    "#stem plots - height of each stem reflects the probability of the word in the focus topic\n",
    "\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,4350)\n",
    "    ax[i].set_ylim(0, 0.08)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.draw()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topic distribution - probability of each of the 20 topics for every document\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([1, 3, 4, 8, 9]): #only plotting these specified topics\n",
    "    ax[i].stem(doc_topic[k,:], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[i].set_xlim(-1, 21)\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"Document {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"Topic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deprecated Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseTextFile(filename):\n",
    "    text = open('cor-por-a/' + filename, 'r')\n",
    "    for i in range(0, 10):\n",
    "        print text.readline()\n",
    "    text_parse = text.read().split()\n",
    "    #print text_parse\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "#     print lancaster.stem('maximum') \n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return text_parse\n",
    "#     print porter.stem('maximum')    \n",
    "\n",
    "#parseTextFile('TomSawyer.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes fucking forever\n",
    "def createMatrix(): ###DEPRECATED###\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parseTextFile('TomSawyer.txt')];\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(vocab_size)]\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    for i in range(0, len(u_vec)-1):\n",
    "        term_one = u_vec[i];\n",
    "        term_two = u_vec[i+1];\n",
    "        index_one = vocab_vec.index(term_one)\n",
    "        index_two = vocab_vec.index(term_two)\n",
    "        mat[index_one][index_one] += 1;\n",
    "        mat[index_one][index_two] += 1;\n",
    "        mat[index_two][index_one] += 1;\n",
    "\n",
    "    last_term = u_vec[len(u_vec)-1]\n",
    "    last_term_index = vocab_vec.index(last_term)\n",
    "    mat[last_term_index][last_term_index] += 1\n",
    "    return (mat, vocab_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def pmi(mat, rownames=None, positive=True):  \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return (p, rownames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlateds(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], w[i]) for i in range(len(mat))]\n",
    "    #print dists\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    # print sorted_dists\n",
    "    return sorted_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correlation list returns an ordered list of (word, correlation_score) tuples, where higher correlation_score\n",
    "# means the word is more correlated. The correlation list includes all words in the vocabulary, so you can\n",
    "# selectively take the first n elements if you want to use them.\n",
    "def correlationList(mat_ppmi):\n",
    "    return correlateds(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###DEPRECATED###\n",
    "# neighbors_list = neighbors(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)[: 50]\n",
    "# print neighbors_list\n",
    "\n",
    "# def retrieve_words(tuple_list):\n",
    "#     words = list()\n",
    "#     for _tuple in tuple_list:\n",
    "#         words.append(_tuple[0])\n",
    "#     return words\n",
    "\n",
    "# neighbors_word_list = retrieve_words(neighbors_list)\n",
    "# print neighbors_word_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
