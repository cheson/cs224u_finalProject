{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Theodora Chu, Josh Cohen, Jason Chen\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info for creating VSM data\n",
    "vsmdata_home = \"vsmdata\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input\n",
    "Takes in a text file and returns a list of ordered unigrams U. \n",
    "It should also consider stemming and other relevant pre-processing. Josh's note: parse \"African American\" as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseTextFile(filename):\n",
    "    text = open('cor-por-a/' + filename, 'r')\n",
    "    for i in range(0, 10):\n",
    "        print text.readline()\n",
    "    text_parse = text.read().split()\n",
    "    #print text_parse\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "#     print lancaster.stem('maximum') \n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return text_parse\n",
    "#     print porter.stem('maximum')    \n",
    "\n",
    "#parseTextFile('TomSawyer.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "1. Parse U to create a word-word frequency matrix M, where each row represents a word and each entry x(i,j) represents the number of times word i co-occurs with word j.\n",
    "2. Convert M to a new matrix M’ with some sort of correlation operation. We could use PMI, Occai (see Josh’s paper), CSA, or some other correlation structure.\n",
    "3. Let row a represent the unigram “African American”. Take in that row, and output an ordered list of (this_unigram, correlation_score) pairs which represent the correlation score of this_unigram with the term “African American”\n",
    "4. Produce a list L of the top 100 correlated words with the term “African American”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This takes fucking forever\n",
    "def createMatrix():\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parseTextFile('TomSawyer.txt')];\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(vocab_size)]\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    for i in range(0, len(u_vec)-1):\n",
    "        term_one = u_vec[i];\n",
    "        term_two = u_vec[i+1];\n",
    "        index_one = vocab_vec.index(term_one)\n",
    "        index_two = vocab_vec.index(term_two)\n",
    "        mat[index_one][index_one] += 1;\n",
    "        mat[index_one][index_two] += 1;\n",
    "        mat[index_two][index_one] += 1;\n",
    "\n",
    "    last_term = u_vec[len(u_vec)-1]\n",
    "    last_term_index = vocab_vec.index(last_term)\n",
    "    mat[last_term_index][last_term_index] += 1\n",
    "    return (mat, vocab_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbors(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def pmi(mat, rownames=None, positive=True):  \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    val = 0.0\n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val,0.0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlateds(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], w[i]) for i in range(len(mat))]\n",
    "    #print dists\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    # print sorted_dists\n",
    "    return sorted_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correlation list returns an ordered list of (word, correlation_score) tuples, where h\n",
    "def correlationList(mat_ppmi):\n",
    "    return correlateds(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete by\r\n",
      "\n",
      "Mark Twain (Samuel Clemens)\r\n",
      "\n",
      "\r\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\r\n",
      "\n",
      "no restrictions whatsoever. You may copy it, give it away or re-use\r\n",
      "\n",
      "it under the terms of the Project Gutenberg License included with this\r\n",
      "\n",
      "eBook or online at www.gutenberg.net\r\n",
      "\n",
      "\r\n",
      "\n",
      "Title: The Adventures of Tom Sawyer, Complete\r\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('colored', 9.4169795839905763),\n",
       " ('frontispiece--a', 9.4169795839905763),\n",
       " ('boy,', 6.2389257536426292),\n",
       " ('small', 5.8616315225011615),\n",
       " ('and', 1.3983541189448454)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mat[0] refers to the matrix, mat[1] is a vector of rownames. To get the vector which corresponds to a given word,\n",
    "# call mat[0][mat[1].index('my_word')]\n",
    "mat = createMatrix()\n",
    "mat_ppmi = pmi(mat=mat[0], rownames=mat[1], positive=True)\n",
    "\n",
    "# The output of this shows the format of \n",
    "correlationList(mat_ppmi)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "Takes in a list V of words and returns the average sentiment score across all terms in V as determined by freebase. Note to Jason: consider other sentiment databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4\n"
     ]
    }
   ],
   "source": [
    "def getSentiment(word):\n",
    "    #score = nltk.sentiment.vader.polarity_scores(word)\n",
    "    #replace this with score from freebase\n",
    "    return len(word)\n",
    "\n",
    "V = ['good', 'bad', 'great', 'worrisome', 'stupid']\n",
    "def generate_sentiment(wordList):\n",
    "    totalSentiment = 0.0;\n",
    "    for word in wordList:\n",
    "        totalSentiment += getSentiment(word)\n",
    "    averageSentiment = totalSentiment/len(wordList)\n",
    "    return averageSentiment\n",
    "\n",
    "print generate_sentiment(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR/AND\n",
    "Takes in a dict of corpus:list of words and returns a dict of corpus:XOR words and dict of corpus:AND words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def XOR(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first ^ second\n",
    "def AND(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first & second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "Takes in a matrix M and correlation list L. Using t-sne, produces a word cloud which represents correlation between all terms. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix: array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def word_cloud(corr_list):\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    tsne_matrix = model.fit_transform(corr_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
