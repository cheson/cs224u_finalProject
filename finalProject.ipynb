{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__author__ = \"Theodora Chu, Josh Cohen, Jason Chen\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info for creating VSM data\n",
    "vsmdata_home = \"vsmdata\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "equivalence_set = ['African American', 'African-American', 'black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed_set2 = ['african-american', 'black', 'african', 'happy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input\n",
    "Takes in a text file and returns a list of ordered unigrams U. \n",
    "It should also consider stemming and other relevant pre-processing. Josh's note: parse \"African American\" as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseTextFile(filename):\n",
    "    text = open('cor-por-a/' + filename, 'r')\n",
    "    for i in range(0, 10):\n",
    "        print text.readline()\n",
    "    text_parse = text.read().split()\n",
    "    #print text_parse\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "#     print lancaster.stem('maximum') \n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return text_parse\n",
    "#     print porter.stem('maximum')    \n",
    "\n",
    "#parseTextFile('TomSawyer.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_seedword(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):                \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    return overallCorpus\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                #print article_file\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    overallCorpus += ' ';\n",
    "                    overallCorpus += article_text;\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i);\n",
    "    print 'hey!'\n",
    "    return overallCorpus;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_NYT_articles_worddoc(num_files, root_directory='cor-por-a/2006/'):\n",
    "    overallCorpus = [];\n",
    "    file_list = [];\n",
    "    i = 0;\n",
    "    for dirname_1 in os.listdir(root_directory):\n",
    "        if (dirname_1 == '.DS_Store'):\n",
    "            continue;\n",
    "        print \"parsing outer file directory \" + dirname_1;\n",
    "        for dirname in os.listdir(root_directory + dirname_1 + '/'):\n",
    "            if (dirname == '.DS_Store'):\n",
    "                continue;\n",
    "            print \"parsing directory \" + root_directory + dirname_1 + '/' + dirname;\n",
    "            for filename in os.listdir(root_directory + dirname_1 + '/' + dirname + '/'):      \n",
    "                if (i >= num_files):\n",
    "                    print 'Num files: ' + str(i);\n",
    "                    return (overallCorpus, file_list)\n",
    "                if (filename == '.DS_Store'):\n",
    "                    continue;\n",
    "                article_file = root_directory + dirname_1 + '/' + dirname + '/' + filename;\n",
    "                file_list.append(filename)\n",
    "                article_rep = parse_NYT_article(article_file);\n",
    "                if (article_rep):\n",
    "                    article_text = remove_punctuation(article_rep[1]).split(\" \");\n",
    "                    for word in article_text:\n",
    "                        overallCorpus.append((word, filename))\n",
    "                i = i+1;\n",
    "    print \"num files: \" + str(i)\n",
    "    return (overallCorpus, file_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "1. Parse U to create a word-word frequency matrix M, where each row represents a word and each entry x(i,j) represents the number of times word i co-occurs with word j.\n",
    "2. Convert M to a new matrix M’ with some sort of correlation operation. We could use PMI, Occai (see Josh’s paper), CSA, or some other correlation structure.\n",
    "3. Let row a represent the unigram “African American”. Take in that row, and output an ordered list of (this_unigram, correlation_score) pairs which represent the correlation score of this_unigram with the term “African American”\n",
    "4. Produce a list L of the top 100 correlated words with the term “African American”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns matrix object where mat_obj[0] refers to the seed_word matrix, where mat[1] refers\n",
    "# to the vocab list, where mat[2] refers to a frequency list,\n",
    "# where mat_obj[0][0] refers to the vector representing co-occurrence for first word in seed\n",
    "# set, and where mat_obj[0][len(seed_set)] refers to a vector of overall counts for each term \n",
    "def createSeedWordMatrixNYT(num_files):\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parse_NYT_articles_seedword(num_files)];\n",
    "    num_terms = len(u_vec);\n",
    "    print 'num terms in corpus: ' + str(num_terms);\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    print 'vocab size: ' + str(vocab_size);\n",
    "    print 'matrix dimensions: ' + str(len(seed_set2)) + ' x ' + str(len(vocab_vec));\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(len(seed_set2)+1)]\n",
    "    frequency_vec = [0 for x in range(vocab_size)]\n",
    "\n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i;\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    term = u_vec[0];\n",
    "    term_neighbor_r = u_vec[1];\n",
    "    \n",
    "    # CHANGED\n",
    "    #index_term = vocab_vec.index(term)\n",
    "    index_term = index_dict[term];\n",
    "    \n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_r = vocab_vec.index(term_neighbor_r);\n",
    "            index_neighbor_r = index_dict[term_neighbor_r];\n",
    "            \n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    for i in range(1, len(u_vec)-1):\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(num_terms) + ' terms'\n",
    "        term = u_vec[i];\n",
    "        term_neighbor_l = u_vec[i-1];\n",
    "        term_neighbor_r = u_vec[i+1];\n",
    "        \n",
    "        # CHANGED\n",
    "        #index_term = vocab_vec.index(term)\n",
    "        index_term = index_dict[term]\n",
    "        \n",
    "        frequency_vec[index_term] += 1;\n",
    "        if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_l = vocab_vec.index(term_neighbor_l);\n",
    "            #index_neighbor_r = vocab_vec.index(term_neighbor_r);\n",
    "            index_neighbor_l = index_dict[term_neighbor_l]\n",
    "            index_neighbor_r = index_dict[term_neighbor_r]\n",
    "            \n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_neighbor_r] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    term = u_vec[len(u_vec)-1];\n",
    "    term_neighbor_l = u_vec[len(u_vec)-2];\n",
    "    \n",
    "    # CHANGED\n",
    "    #index_term = vocab_vec.index(term)\n",
    "    index_term = index_dict[term]\n",
    "    \n",
    "    frequency_vec[index_term] += 1;\n",
    "    if (any(seed_word == term for seed_word in seed_set2)):\n",
    "            index_seed = seed_set2.index(term);\n",
    "            \n",
    "            # CHANGED\n",
    "            #index_neighbor_l = vocab_vec.index(term_neighbor_l);\n",
    "            index_neighbor_l = index_dict[term];\n",
    "            \n",
    "            mat[index_seed][index_neighbor_l] += 1;\n",
    "            mat[index_seed][index_term] += 1;\n",
    "    print 'Parsed ' + str(num_terms) + '/' + str(num_terms) + ' terms';\n",
    "    return (mat, vocab_vec, frequency_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns a word, correlation list tuple for each seed in seed_set2\n",
    "def getCorrelationLists(mat_obj):\n",
    "    tupleArr = []\n",
    "    # Word lists for each word\n",
    "    for j in range(len(seed_set2)):\n",
    "        w = mat_obj[0][j]\n",
    "        dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "        sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "        print \"PMI list for word: \" + seed_set2[j] + \"; \" + str(sorted_dists[:5])\n",
    "        tupleArr.append((seed_set2[j], sorted_dists));\n",
    "    \n",
    "    #frequency list for each word:\n",
    "    w = mat_obj[2]\n",
    "    dists = [(mat_obj[1][i], w[i]) for i in range(len(w))]\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    print \"Frequency list: \" + str(sorted_dists[:5])\n",
    "    tupleArr.append((seed_set2[j], sorted_dists));\n",
    "    \n",
    "    return tupleArr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This takes fucking forever\n",
    "def createMatrix():\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parseTextFile('TomSawyer.txt')];\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(vocab_size)]\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    for i in range(0, len(u_vec)-1):\n",
    "        term_one = u_vec[i];\n",
    "        term_two = u_vec[i+1];\n",
    "        index_one = vocab_vec.index(term_one)\n",
    "        index_two = vocab_vec.index(term_two)\n",
    "        mat[index_one][index_one] += 1;\n",
    "        mat[index_one][index_two] += 1;\n",
    "        mat[index_two][index_one] += 1;\n",
    "\n",
    "    last_term = u_vec[len(u_vec)-1]\n",
    "    last_term_index = vocab_vec.index(last_term)\n",
    "    mat[last_term_index][last_term_index] += 1\n",
    "    return (mat, vocab_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbors(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# PMI log: p(x, y)/ p(x)p(y)\n",
    "def pmi_seed(mat_obj, rownames=None, positive=True):  \n",
    "    rownames = mat_obj[1];\n",
    "    frequencies = mat_obj[2];\n",
    "    word_count = np.sum(frequencies, axis=None)\n",
    "    \n",
    "    # Joint probability table:\n",
    "    p = mat_obj[0] / word_count;\n",
    "    colprobs = frequencies/word_count;\n",
    "    sum_of_colprobs = np.sum(colprobs)\n",
    "    \n",
    "    \n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))    \n",
    "    mat_ppmi = [];\n",
    "    for row in p:\n",
    "        if np.sum(row) > 0:\n",
    "            mat_ppmi.append(np_pmi_log(row / (np.sum(row)*colprobs)));\n",
    "        else:\n",
    "            mat_ppmi.append([0 for x in row])\n",
    "    return (mat_ppmi, rownames, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def pmi(mat, rownames=None, positive=True):  \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    val = 0.0\n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val,0.0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlateds(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], w[i]) for i in range(len(mat))]\n",
    "    #print dists\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    # print sorted_dists\n",
    "    return sorted_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correlation list returns an ordered list of (word, correlation_score) tuples, where higher correlation_score\n",
    "# means the word is more correlated. The correlation list includes all words in the vocabulary, so you can\n",
    "# selectively take the first n elements if you want to use them.\n",
    "def correlationList(mat_ppmi):\n",
    "    return correlateds(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools to save result of mat calculations ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6082d91c5fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#recreated using loadtxt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mat_features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mat_labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mat_labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mat' is not defined"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# if not os.path.exists('my_file'): numpy.savetxt('my_file', my_array)\n",
    "\n",
    "#this will save the result of our matrix into a human-readable text file, and the original array is easily\n",
    "#recreated using loadtxt.\n",
    "\n",
    "np.savetxt(\"mat_features\", mat[0])\n",
    "np.savetxt(\"mat_labels\", mat[1])\n",
    "np.loadtxt(\"mat_labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### tool to remove punctuation from a text ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With Punctuation\n"
     ]
    }
   ],
   "source": [
    "s = \"string. With. Punctuation?\" # Sample string\n",
    "def remove_punctuation(text):\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,\"\")\n",
    "    return text\n",
    "\n",
    "print(remove_punctuation(s))\n",
    "\n",
    "\n",
    "#period, question mark, exclamation point, comma, semicolon, colon, dash, \n",
    "#hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools to parse out text from ntif/xml document for NYT articles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2007',\n",
       " 'A doctor who works at a clinic in Jamaica has been charged with insurance fraud, accused of billing insurance companies for tests that were never performed on victims of motor vehicle accidents, prosecutors said yesterday. The doctor, Alexander Israeli, 53, of Middle Village, was arraigned in Criminal Court on Monday night on charges of grand larceny and insurance fraud, said Richard A. Brown, the Queens district attorney. Mr. Brown said that Dr. Israeli billed insurance companies last year for $21,000 worth of neurological tests that were not performed. He faces loss of his medical license and up to seven years in prison if convicted, prosecutors said.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://docs.python-guide.org/en/latest/scenarios/xml/\n",
    "# http://stackoverflow.com/questions/1912434/how-do-i-parse-xml-in-python\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_NYT_article(xmlFile): \n",
    "    tree = ET.parse(xmlFile)\n",
    "    root = tree.getroot()\n",
    "    year = ''\n",
    "    article_text = '';\n",
    "    for child in root:\n",
    "        if child.tag == 'head':\n",
    "            for subchild in child:\n",
    "                if 'name' in subchild.attrib:\n",
    "                    if subchild.attrib['name'] == 'publication_year':\n",
    "                        year = subchild.attrib['content']\n",
    "        if child.tag == 'body':\n",
    "            body = child\n",
    "    for child in body:\n",
    "        if child.tag == 'body.content':\n",
    "            content = child\n",
    "    for child in content:\n",
    "        if child.attrib == {'class': 'full_text'}:\n",
    "            for paragraph in child:\n",
    "                article_text += paragraph.text\n",
    "            return (year, article_text)\n",
    "                \n",
    "parse_NYT_article('nyt_sample_2.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mat_ppmi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ac0d676286da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneighbors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'colored'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat_ppmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat_ppmi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mneighbors_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mat_ppmi' is not defined"
     ]
    }
   ],
   "source": [
    "neighbors_list = neighbors(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)[: 50]\n",
    "print neighbors_list\n",
    "\n",
    "def retrieve_words(tuple_list):\n",
    "    words = list()\n",
    "    for _tuple in tuple_list:\n",
    "        words.append(_tuple[0])\n",
    "    return words\n",
    "\n",
    "neighbors_word_list = retrieve_words(neighbors_list)\n",
    "print neighbors_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Correlation Lists\n",
    "Using the functions above, creates seed-word matrix with a user-specified number of files, performs pmi on that matrix, and computes a resulting correlation list for each seed word.\n",
    "\n",
    "In order to use more files, update the num_files variable. In order to update the seed set, update the seed_set2 global variable to include more words.\n",
    "\n",
    "Note: Creating these correlation lists at scale is very slow. Start off by processing about 10 files, and scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "parsing directory cor-por-a/2006/01/02\n",
      "parsing directory cor-por-a/2006/01/03\n",
      "parsing directory cor-por-a/2006/01/04\n",
      "parsing directory cor-por-a/2006/01/05\n",
      "Num files: 1000\n",
      "num terms in corpus: 594391\n",
      "vocab size: 50181\n",
      "matrix dimensions: 4 x 50181\n",
      "index_dict created!\n",
      "parsed 1000/594391 terms\n",
      "parsed 2000/594391 terms\n",
      "parsed 3000/594391 terms\n",
      "parsed 4000/594391 terms\n",
      "parsed 5000/594391 terms\n",
      "parsed 6000/594391 terms\n",
      "parsed 7000/594391 terms\n",
      "parsed 8000/594391 terms\n",
      "parsed 9000/594391 terms\n",
      "parsed 10000/594391 terms\n",
      "parsed 11000/594391 terms\n",
      "parsed 12000/594391 terms\n",
      "parsed 13000/594391 terms\n",
      "parsed 14000/594391 terms\n",
      "parsed 15000/594391 terms\n",
      "parsed 16000/594391 terms\n",
      "parsed 17000/594391 terms\n",
      "parsed 18000/594391 terms\n",
      "parsed 19000/594391 terms\n",
      "parsed 20000/594391 terms\n",
      "parsed 21000/594391 terms\n",
      "parsed 22000/594391 terms\n",
      "parsed 23000/594391 terms\n",
      "parsed 24000/594391 terms\n",
      "parsed 25000/594391 terms\n",
      "parsed 26000/594391 terms\n",
      "parsed 27000/594391 terms\n",
      "parsed 28000/594391 terms\n",
      "parsed 29000/594391 terms\n",
      "parsed 30000/594391 terms\n",
      "parsed 31000/594391 terms\n",
      "parsed 32000/594391 terms\n",
      "parsed 33000/594391 terms\n",
      "parsed 34000/594391 terms\n",
      "parsed 35000/594391 terms\n",
      "parsed 36000/594391 terms\n",
      "parsed 37000/594391 terms\n",
      "parsed 38000/594391 terms\n",
      "parsed 39000/594391 terms\n",
      "parsed 40000/594391 terms\n",
      "parsed 41000/594391 terms\n",
      "parsed 42000/594391 terms\n",
      "parsed 43000/594391 terms\n",
      "parsed 44000/594391 terms\n",
      "parsed 45000/594391 terms\n",
      "parsed 46000/594391 terms\n",
      "parsed 47000/594391 terms\n",
      "parsed 48000/594391 terms\n",
      "parsed 49000/594391 terms\n",
      "parsed 50000/594391 terms\n",
      "parsed 51000/594391 terms\n",
      "parsed 52000/594391 terms\n",
      "parsed 53000/594391 terms\n",
      "parsed 54000/594391 terms\n",
      "parsed 55000/594391 terms\n",
      "parsed 56000/594391 terms\n",
      "parsed 57000/594391 terms\n",
      "parsed 58000/594391 terms\n",
      "parsed 59000/594391 terms\n",
      "parsed 60000/594391 terms\n",
      "parsed 61000/594391 terms\n",
      "parsed 62000/594391 terms\n",
      "parsed 63000/594391 terms\n",
      "parsed 64000/594391 terms\n",
      "parsed 65000/594391 terms\n",
      "parsed 66000/594391 terms\n",
      "parsed 67000/594391 terms\n",
      "parsed 68000/594391 terms\n",
      "parsed 69000/594391 terms\n",
      "parsed 70000/594391 terms\n",
      "parsed 71000/594391 terms\n",
      "parsed 72000/594391 terms\n",
      "parsed 73000/594391 terms\n",
      "parsed 74000/594391 terms\n",
      "parsed 75000/594391 terms\n",
      "parsed 76000/594391 terms\n",
      "parsed 77000/594391 terms\n",
      "parsed 78000/594391 terms\n",
      "parsed 79000/594391 terms\n",
      "parsed 80000/594391 terms\n",
      "parsed 81000/594391 terms\n",
      "parsed 82000/594391 terms\n",
      "parsed 83000/594391 terms\n",
      "parsed 84000/594391 terms\n",
      "parsed 85000/594391 terms\n",
      "parsed 86000/594391 terms\n",
      "parsed 87000/594391 terms\n",
      "parsed 88000/594391 terms\n",
      "parsed 89000/594391 terms\n",
      "parsed 90000/594391 terms\n",
      "parsed 91000/594391 terms\n",
      "parsed 92000/594391 terms\n",
      "parsed 93000/594391 terms\n",
      "parsed 94000/594391 terms\n",
      "parsed 95000/594391 terms\n",
      "parsed 96000/594391 terms\n",
      "parsed 97000/594391 terms\n",
      "parsed 98000/594391 terms\n",
      "parsed 99000/594391 terms\n",
      "parsed 100000/594391 terms\n",
      "parsed 101000/594391 terms\n",
      "parsed 102000/594391 terms\n",
      "parsed 103000/594391 terms\n",
      "parsed 104000/594391 terms\n",
      "parsed 105000/594391 terms\n",
      "parsed 106000/594391 terms\n",
      "parsed 107000/594391 terms\n",
      "parsed 108000/594391 terms\n",
      "parsed 109000/594391 terms\n",
      "parsed 110000/594391 terms\n",
      "parsed 111000/594391 terms\n",
      "parsed 112000/594391 terms\n",
      "parsed 113000/594391 terms\n",
      "parsed 114000/594391 terms\n",
      "parsed 115000/594391 terms\n",
      "parsed 116000/594391 terms\n",
      "parsed 117000/594391 terms\n",
      "parsed 118000/594391 terms\n",
      "parsed 119000/594391 terms\n",
      "parsed 120000/594391 terms\n",
      "parsed 121000/594391 terms\n",
      "parsed 122000/594391 terms\n",
      "parsed 123000/594391 terms\n",
      "parsed 124000/594391 terms\n",
      "parsed 125000/594391 terms\n",
      "parsed 126000/594391 terms\n",
      "parsed 127000/594391 terms\n",
      "parsed 128000/594391 terms\n",
      "parsed 129000/594391 terms\n",
      "parsed 130000/594391 terms\n",
      "parsed 131000/594391 terms\n",
      "parsed 132000/594391 terms\n",
      "parsed 133000/594391 terms\n",
      "parsed 134000/594391 terms\n",
      "parsed 135000/594391 terms\n",
      "parsed 136000/594391 terms\n",
      "parsed 137000/594391 terms\n",
      "parsed 138000/594391 terms\n",
      "parsed 139000/594391 terms\n",
      "parsed 140000/594391 terms\n",
      "parsed 141000/594391 terms\n",
      "parsed 142000/594391 terms\n",
      "parsed 143000/594391 terms\n",
      "parsed 144000/594391 terms\n",
      "parsed 145000/594391 terms\n",
      "parsed 146000/594391 terms\n",
      "parsed 147000/594391 terms\n",
      "parsed 148000/594391 terms\n",
      "parsed 149000/594391 terms\n",
      "parsed 150000/594391 terms\n",
      "parsed 151000/594391 terms\n",
      "parsed 152000/594391 terms\n",
      "parsed 153000/594391 terms\n",
      "parsed 154000/594391 terms\n",
      "parsed 155000/594391 terms\n",
      "parsed 156000/594391 terms\n",
      "parsed 157000/594391 terms\n",
      "parsed 158000/594391 terms\n",
      "parsed 159000/594391 terms\n",
      "parsed 160000/594391 terms\n",
      "parsed 161000/594391 terms\n",
      "parsed 162000/594391 terms\n",
      "parsed 163000/594391 terms\n",
      "parsed 164000/594391 terms\n",
      "parsed 165000/594391 terms\n",
      "parsed 166000/594391 terms\n",
      "parsed 167000/594391 terms\n",
      "parsed 168000/594391 terms\n",
      "parsed 169000/594391 terms\n",
      "parsed 170000/594391 terms\n",
      "parsed 171000/594391 terms\n",
      "parsed 172000/594391 terms\n",
      "parsed 173000/594391 terms\n",
      "parsed 174000/594391 terms\n",
      "parsed 175000/594391 terms\n",
      "parsed 176000/594391 terms\n",
      "parsed 177000/594391 terms\n",
      "parsed 178000/594391 terms\n",
      "parsed 179000/594391 terms\n",
      "parsed 180000/594391 terms\n",
      "parsed 181000/594391 terms\n",
      "parsed 182000/594391 terms\n",
      "parsed 183000/594391 terms\n",
      "parsed 184000/594391 terms\n",
      "parsed 185000/594391 terms\n",
      "parsed 186000/594391 terms\n",
      "parsed 187000/594391 terms\n",
      "parsed 188000/594391 terms\n",
      "parsed 189000/594391 terms\n",
      "parsed 190000/594391 terms\n",
      "parsed 191000/594391 terms\n",
      "parsed 192000/594391 terms\n",
      "parsed 193000/594391 terms\n",
      "parsed 194000/594391 terms\n",
      "parsed 195000/594391 terms\n",
      "parsed 196000/594391 terms\n",
      "parsed 197000/594391 terms\n",
      "parsed 198000/594391 terms\n",
      "parsed 199000/594391 terms\n",
      "parsed 200000/594391 terms\n",
      "parsed 201000/594391 terms\n",
      "parsed 202000/594391 terms\n",
      "parsed 203000/594391 terms\n",
      "parsed 204000/594391 terms\n",
      "parsed 205000/594391 terms\n",
      "parsed 206000/594391 terms\n",
      "parsed 207000/594391 terms\n",
      "parsed 208000/594391 terms\n",
      "parsed 209000/594391 terms\n",
      "parsed 210000/594391 terms\n",
      "parsed 211000/594391 terms\n",
      "parsed 212000/594391 terms\n",
      "parsed 213000/594391 terms\n",
      "parsed 214000/594391 terms\n",
      "parsed 215000/594391 terms\n",
      "parsed 216000/594391 terms\n",
      "parsed 217000/594391 terms\n",
      "parsed 218000/594391 terms\n",
      "parsed 219000/594391 terms\n",
      "parsed 220000/594391 terms\n",
      "parsed 221000/594391 terms\n",
      "parsed 222000/594391 terms\n",
      "parsed 223000/594391 terms\n",
      "parsed 224000/594391 terms\n",
      "parsed 225000/594391 terms\n",
      "parsed 226000/594391 terms\n",
      "parsed 227000/594391 terms\n",
      "parsed 228000/594391 terms\n",
      "parsed 229000/594391 terms\n",
      "parsed 230000/594391 terms\n",
      "parsed 231000/594391 terms\n",
      "parsed 232000/594391 terms\n",
      "parsed 233000/594391 terms\n",
      "parsed 234000/594391 terms\n",
      "parsed 235000/594391 terms\n",
      "parsed 236000/594391 terms\n",
      "parsed 237000/594391 terms\n",
      "parsed 238000/594391 terms\n",
      "parsed 239000/594391 terms\n",
      "parsed 240000/594391 terms\n",
      "parsed 241000/594391 terms\n",
      "parsed 242000/594391 terms\n",
      "parsed 243000/594391 terms\n",
      "parsed 244000/594391 terms\n",
      "parsed 245000/594391 terms\n",
      "parsed 246000/594391 terms\n",
      "parsed 247000/594391 terms\n",
      "parsed 248000/594391 terms\n",
      "parsed 249000/594391 terms\n",
      "parsed 250000/594391 terms\n",
      "parsed 251000/594391 terms\n",
      "parsed 252000/594391 terms\n",
      "parsed 253000/594391 terms\n",
      "parsed 254000/594391 terms\n",
      "parsed 255000/594391 terms\n",
      "parsed 256000/594391 terms\n",
      "parsed 257000/594391 terms\n",
      "parsed 258000/594391 terms\n",
      "parsed 259000/594391 terms\n",
      "parsed 260000/594391 terms\n",
      "parsed 261000/594391 terms\n",
      "parsed 262000/594391 terms\n",
      "parsed 263000/594391 terms\n",
      "parsed 264000/594391 terms\n",
      "parsed 265000/594391 terms\n",
      "parsed 266000/594391 terms\n",
      "parsed 267000/594391 terms\n",
      "parsed 268000/594391 terms\n",
      "parsed 269000/594391 terms\n",
      "parsed 270000/594391 terms\n",
      "parsed 271000/594391 terms\n",
      "parsed 272000/594391 terms\n",
      "parsed 273000/594391 terms\n",
      "parsed 274000/594391 terms\n",
      "parsed 275000/594391 terms\n",
      "parsed 276000/594391 terms\n",
      "parsed 277000/594391 terms\n",
      "parsed 278000/594391 terms\n",
      "parsed 279000/594391 terms\n",
      "parsed 280000/594391 terms\n",
      "parsed 281000/594391 terms\n",
      "parsed 282000/594391 terms\n",
      "parsed 283000/594391 terms\n",
      "parsed 284000/594391 terms\n",
      "parsed 285000/594391 terms\n",
      "parsed 286000/594391 terms\n",
      "parsed 287000/594391 terms\n",
      "parsed 288000/594391 terms\n",
      "parsed 289000/594391 terms\n",
      "parsed 290000/594391 terms\n",
      "parsed 291000/594391 terms\n",
      "parsed 292000/594391 terms\n",
      "parsed 293000/594391 terms\n",
      "parsed 294000/594391 terms\n",
      "parsed 295000/594391 terms\n",
      "parsed 296000/594391 terms\n",
      "parsed 297000/594391 terms\n",
      "parsed 298000/594391 terms\n",
      "parsed 299000/594391 terms\n",
      "parsed 300000/594391 terms\n",
      "parsed 301000/594391 terms\n",
      "parsed 302000/594391 terms\n",
      "parsed 303000/594391 terms\n",
      "parsed 304000/594391 terms\n",
      "parsed 305000/594391 terms\n",
      "parsed 306000/594391 terms\n",
      "parsed 307000/594391 terms\n",
      "parsed 308000/594391 terms\n",
      "parsed 309000/594391 terms\n",
      "parsed 310000/594391 terms\n",
      "parsed 311000/594391 terms\n",
      "parsed 312000/594391 terms\n",
      "parsed 313000/594391 terms\n",
      "parsed 314000/594391 terms\n",
      "parsed 315000/594391 terms\n",
      "parsed 316000/594391 terms\n",
      "parsed 317000/594391 terms\n",
      "parsed 318000/594391 terms\n",
      "parsed 319000/594391 terms\n",
      "parsed 320000/594391 terms\n",
      "parsed 321000/594391 terms\n",
      "parsed 322000/594391 terms\n",
      "parsed 323000/594391 terms\n",
      "parsed 324000/594391 terms\n",
      "parsed 325000/594391 terms\n",
      "parsed 326000/594391 terms\n",
      "parsed 327000/594391 terms\n",
      "parsed 328000/594391 terms\n",
      "parsed 329000/594391 terms\n",
      "parsed 330000/594391 terms\n",
      "parsed 331000/594391 terms\n",
      "parsed 332000/594391 terms\n",
      "parsed 333000/594391 terms\n",
      "parsed 334000/594391 terms\n",
      "parsed 335000/594391 terms\n",
      "parsed 336000/594391 terms\n",
      "parsed 337000/594391 terms\n",
      "parsed 338000/594391 terms\n",
      "parsed 339000/594391 terms\n",
      "parsed 340000/594391 terms\n",
      "parsed 341000/594391 terms\n",
      "parsed 342000/594391 terms\n",
      "parsed 343000/594391 terms\n",
      "parsed 344000/594391 terms\n",
      "parsed 345000/594391 terms\n",
      "parsed 346000/594391 terms\n",
      "parsed 347000/594391 terms\n",
      "parsed 348000/594391 terms\n",
      "parsed 349000/594391 terms\n",
      "parsed 350000/594391 terms\n",
      "parsed 351000/594391 terms\n",
      "parsed 352000/594391 terms\n",
      "parsed 353000/594391 terms\n",
      "parsed 354000/594391 terms\n",
      "parsed 355000/594391 terms\n",
      "parsed 356000/594391 terms\n",
      "parsed 357000/594391 terms\n",
      "parsed 358000/594391 terms\n",
      "parsed 359000/594391 terms\n",
      "parsed 360000/594391 terms\n",
      "parsed 361000/594391 terms\n",
      "parsed 362000/594391 terms\n",
      "parsed 363000/594391 terms\n",
      "parsed 364000/594391 terms\n",
      "parsed 365000/594391 terms\n",
      "parsed 366000/594391 terms\n",
      "parsed 367000/594391 terms\n",
      "parsed 368000/594391 terms\n",
      "parsed 369000/594391 terms\n",
      "parsed 370000/594391 terms\n",
      "parsed 371000/594391 terms\n",
      "parsed 372000/594391 terms\n",
      "parsed 373000/594391 terms\n",
      "parsed 374000/594391 terms\n",
      "parsed 375000/594391 terms\n",
      "parsed 376000/594391 terms\n",
      "parsed 377000/594391 terms\n",
      "parsed 378000/594391 terms\n",
      "parsed 379000/594391 terms\n",
      "parsed 380000/594391 terms\n",
      "parsed 381000/594391 terms\n",
      "parsed 382000/594391 terms\n",
      "parsed 383000/594391 terms\n",
      "parsed 384000/594391 terms\n",
      "parsed 385000/594391 terms\n",
      "parsed 386000/594391 terms\n",
      "parsed 387000/594391 terms\n",
      "parsed 388000/594391 terms\n",
      "parsed 389000/594391 terms\n",
      "parsed 390000/594391 terms\n",
      "parsed 391000/594391 terms\n",
      "parsed 392000/594391 terms\n",
      "parsed 393000/594391 terms\n",
      "parsed 394000/594391 terms\n",
      "parsed 395000/594391 terms\n",
      "parsed 396000/594391 terms\n",
      "parsed 397000/594391 terms\n",
      "parsed 398000/594391 terms\n",
      "parsed 399000/594391 terms\n",
      "parsed 400000/594391 terms\n",
      "parsed 401000/594391 terms\n",
      "parsed 402000/594391 terms\n",
      "parsed 403000/594391 terms\n",
      "parsed 404000/594391 terms\n",
      "parsed 405000/594391 terms\n",
      "parsed 406000/594391 terms\n",
      "parsed 407000/594391 terms\n",
      "parsed 408000/594391 terms\n",
      "parsed 409000/594391 terms\n",
      "parsed 410000/594391 terms\n",
      "parsed 411000/594391 terms\n",
      "parsed 412000/594391 terms\n",
      "parsed 413000/594391 terms\n",
      "parsed 414000/594391 terms\n",
      "parsed 415000/594391 terms\n",
      "parsed 416000/594391 terms\n",
      "parsed 417000/594391 terms\n",
      "parsed 418000/594391 terms\n",
      "parsed 419000/594391 terms\n",
      "parsed 420000/594391 terms\n",
      "parsed 421000/594391 terms\n",
      "parsed 422000/594391 terms\n",
      "parsed 423000/594391 terms\n",
      "parsed 424000/594391 terms\n",
      "parsed 425000/594391 terms\n",
      "parsed 426000/594391 terms\n",
      "parsed 427000/594391 terms\n",
      "parsed 428000/594391 terms\n",
      "parsed 429000/594391 terms\n",
      "parsed 430000/594391 terms\n",
      "parsed 431000/594391 terms\n",
      "parsed 432000/594391 terms\n",
      "parsed 433000/594391 terms\n",
      "parsed 434000/594391 terms\n",
      "parsed 435000/594391 terms\n",
      "parsed 436000/594391 terms\n",
      "parsed 437000/594391 terms\n",
      "parsed 438000/594391 terms\n",
      "parsed 439000/594391 terms\n",
      "parsed 440000/594391 terms\n",
      "parsed 441000/594391 terms\n",
      "parsed 442000/594391 terms\n",
      "parsed 443000/594391 terms\n",
      "parsed 444000/594391 terms\n",
      "parsed 445000/594391 terms\n",
      "parsed 446000/594391 terms\n",
      "parsed 447000/594391 terms\n",
      "parsed 448000/594391 terms\n",
      "parsed 449000/594391 terms\n",
      "parsed 450000/594391 terms\n",
      "parsed 451000/594391 terms\n",
      "parsed 452000/594391 terms\n",
      "parsed 453000/594391 terms\n",
      "parsed 454000/594391 terms\n",
      "parsed 455000/594391 terms\n",
      "parsed 456000/594391 terms\n",
      "parsed 457000/594391 terms\n",
      "parsed 458000/594391 terms\n",
      "parsed 459000/594391 terms\n",
      "parsed 460000/594391 terms\n",
      "parsed 461000/594391 terms\n",
      "parsed 462000/594391 terms\n",
      "parsed 463000/594391 terms\n",
      "parsed 464000/594391 terms\n",
      "parsed 465000/594391 terms\n",
      "parsed 466000/594391 terms\n",
      "parsed 467000/594391 terms\n",
      "parsed 468000/594391 terms\n",
      "parsed 469000/594391 terms\n",
      "parsed 470000/594391 terms\n",
      "parsed 471000/594391 terms\n",
      "parsed 472000/594391 terms\n",
      "parsed 473000/594391 terms\n",
      "parsed 474000/594391 terms\n",
      "parsed 475000/594391 terms\n",
      "parsed 476000/594391 terms\n",
      "parsed 477000/594391 terms\n",
      "parsed 478000/594391 terms\n",
      "parsed 479000/594391 terms\n",
      "parsed 480000/594391 terms\n",
      "parsed 481000/594391 terms\n",
      "parsed 482000/594391 terms\n",
      "parsed 483000/594391 terms\n",
      "parsed 484000/594391 terms\n",
      "parsed 485000/594391 terms\n",
      "parsed 486000/594391 terms\n",
      "parsed 487000/594391 terms\n",
      "parsed 488000/594391 terms\n",
      "parsed 489000/594391 terms\n",
      "parsed 490000/594391 terms\n",
      "parsed 491000/594391 terms\n",
      "parsed 492000/594391 terms\n",
      "parsed 493000/594391 terms\n",
      "parsed 494000/594391 terms\n",
      "parsed 495000/594391 terms\n",
      "parsed 496000/594391 terms\n",
      "parsed 497000/594391 terms\n",
      "parsed 498000/594391 terms\n",
      "parsed 499000/594391 terms\n",
      "parsed 500000/594391 terms\n",
      "parsed 501000/594391 terms\n",
      "parsed 502000/594391 terms\n",
      "parsed 503000/594391 terms\n",
      "parsed 504000/594391 terms\n",
      "parsed 505000/594391 terms\n",
      "parsed 506000/594391 terms\n",
      "parsed 507000/594391 terms\n",
      "parsed 508000/594391 terms\n",
      "parsed 509000/594391 terms\n",
      "parsed 510000/594391 terms\n",
      "parsed 511000/594391 terms\n",
      "parsed 512000/594391 terms\n",
      "parsed 513000/594391 terms\n",
      "parsed 514000/594391 terms\n",
      "parsed 515000/594391 terms\n",
      "parsed 516000/594391 terms\n",
      "parsed 517000/594391 terms\n",
      "parsed 518000/594391 terms\n",
      "parsed 519000/594391 terms\n",
      "parsed 520000/594391 terms\n",
      "parsed 521000/594391 terms\n",
      "parsed 522000/594391 terms\n",
      "parsed 523000/594391 terms\n",
      "parsed 524000/594391 terms\n",
      "parsed 525000/594391 terms\n",
      "parsed 526000/594391 terms\n",
      "parsed 527000/594391 terms\n",
      "parsed 528000/594391 terms\n",
      "parsed 529000/594391 terms\n",
      "parsed 530000/594391 terms\n",
      "parsed 531000/594391 terms\n",
      "parsed 532000/594391 terms\n",
      "parsed 533000/594391 terms\n",
      "parsed 534000/594391 terms\n",
      "parsed 535000/594391 terms\n",
      "parsed 536000/594391 terms\n",
      "parsed 537000/594391 terms\n",
      "parsed 538000/594391 terms\n",
      "parsed 539000/594391 terms\n",
      "parsed 540000/594391 terms\n",
      "parsed 541000/594391 terms\n",
      "parsed 542000/594391 terms\n",
      "parsed 543000/594391 terms\n",
      "parsed 544000/594391 terms\n",
      "parsed 545000/594391 terms\n",
      "parsed 546000/594391 terms\n",
      "parsed 547000/594391 terms\n",
      "parsed 548000/594391 terms\n",
      "parsed 549000/594391 terms\n",
      "parsed 550000/594391 terms\n",
      "parsed 551000/594391 terms\n",
      "parsed 552000/594391 terms\n",
      "parsed 553000/594391 terms\n",
      "parsed 554000/594391 terms\n",
      "parsed 555000/594391 terms\n",
      "parsed 556000/594391 terms\n",
      "parsed 557000/594391 terms\n",
      "parsed 558000/594391 terms\n",
      "parsed 559000/594391 terms\n",
      "parsed 560000/594391 terms\n",
      "parsed 561000/594391 terms\n",
      "parsed 562000/594391 terms\n",
      "parsed 563000/594391 terms\n",
      "parsed 564000/594391 terms\n",
      "parsed 565000/594391 terms\n",
      "parsed 566000/594391 terms\n",
      "parsed 567000/594391 terms\n",
      "parsed 568000/594391 terms\n",
      "parsed 569000/594391 terms\n",
      "parsed 570000/594391 terms\n",
      "parsed 571000/594391 terms\n",
      "parsed 572000/594391 terms\n",
      "parsed 573000/594391 terms\n",
      "parsed 574000/594391 terms\n",
      "parsed 575000/594391 terms\n",
      "parsed 576000/594391 terms\n",
      "parsed 577000/594391 terms\n",
      "parsed 578000/594391 terms\n",
      "parsed 579000/594391 terms\n",
      "parsed 580000/594391 terms\n",
      "parsed 581000/594391 terms\n",
      "parsed 582000/594391 terms\n",
      "parsed 583000/594391 terms\n",
      "parsed 584000/594391 terms\n",
      "parsed 585000/594391 terms\n",
      "parsed 586000/594391 terms\n",
      "parsed 587000/594391 terms\n",
      "parsed 588000/594391 terms\n",
      "parsed 589000/594391 terms\n",
      "parsed 590000/594391 terms\n",
      "parsed 591000/594391 terms\n",
      "parsed 592000/594391 terms\n",
      "parsed 593000/594391 terms\n",
      "parsed 594000/594391 terms\n",
      "Parsed 594391/594391 terms\n"
     ]
    }
   ],
   "source": [
    "mat_obj = createSeedWordMatrixNYT(num_files=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_obj_ppmi = pmi_seed(mat_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI list for word: african-american; [(u'', 0), (u' ', 0), (u'0', 0), (u'00', 0), (u'007', 0)]\n",
      "PMI list for word: black; [(u'abacus', 6.7940029604116896), (u'black', 6.7940029604116896), (u'adaptable', 6.7940029604116887), (u'amex', 6.7940029604116887), (u'beansbut', 6.7940029604116887)]\n",
      "PMI list for word: african; [(u'african', 8.6131614038278581), (u'fauna', 8.6131614038278581), (u'yam', 8.6131614038278581), (u'antelope', 8.3254793313760782), (u'apartheid', 7.514549115159749)]\n",
      "PMI list for word: happy; [(u'2006granted', 7.9340004652426535), (u'35th', 7.9340004652426535), (u'endingms', 7.9340004652426535), (u'happy', 7.9340004652426535), (u'memoriesthe', 7.9340004652426535)]\n",
      "Frequency list: [(u'the', 34376), (u'of', 16569), (u'a', 15669), (u'and', 15245), (u'to', 15180)]\n"
     ]
    }
   ],
   "source": [
    "lists = getCorrelationLists(mat_obj_ppmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Document Matrix\n",
    "Creates a word document matrix for use by Theo and her LDA work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createWordDocumentMatrixNYT(num_files):\n",
    "    result = parse_NYT_articles_worddoc(num_files);\n",
    "    word_file_vec = [(x[0].lower(),x[1]) for x in result[0]];\n",
    "    word_vec = [x[0] for x in word_file_vec];\n",
    "    vocab_vec = np.unique(word_vec).tolist()\n",
    "    file_vec = result[1];\n",
    "    print 'num terms in corpus: ' + str(len(word_vec))\n",
    "    print 'vocab size: ' + str(len(vocab_vec))\n",
    "    print 'matrix dimensions: ' + str(len(vocab_vec)) + ' x ' + str(len(file_vec));\n",
    "    mat = [[0 for x in range(len(file_vec))] for y in range(len(vocab_vec))]\n",
    "    \n",
    "    index_dict = {};\n",
    "    for i in range (0, len(vocab_vec)):\n",
    "        index_dict[vocab_vec[i]] = i;\n",
    "    print 'index_dict created!'\n",
    "    \n",
    "    file_index_dict = {};\n",
    "    for i in range (0, len(file_vec)):\n",
    "        file_index_dict[file_vec[i]] = i;\n",
    "    print 'file_index_dict created!'\n",
    "    \n",
    "    i = 0;\n",
    "    for word_file_tuple in word_file_vec:\n",
    "        if (i % 1000 == 0):\n",
    "            print 'parsed ' + str(i) + '/' + str(len(word_vec)) + ' terms'\n",
    "        word = word_file_tuple[0]\n",
    "        file_name = word_file_tuple[1]\n",
    "        \n",
    "        # CHANGED\n",
    "        #index_word = vocab_vec.index(word);\n",
    "        index_word = index_dict[word]  \n",
    "        #index_file = file_vec.index(file_name);\n",
    "        index_file = file_index_dict[file_name];\n",
    "        \n",
    "        mat[index_word][index_file] +=1;\n",
    "        i = i+1;\n",
    "    print 'Parsed all terms'\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing outer file directory 01\n",
      "parsing directory cor-por-a/2006/01/01\n",
      "Num files: 100\n",
      "num terms in corpus: 83029\n",
      "vocab size: 15218\n",
      "matrix dimensions: 15218 x 100\n",
      "index_dict created!\n",
      "file_index_dict created!\n",
      "parsed 0/83029 terms\n",
      "parsed 1000/83029 terms\n",
      "parsed 2000/83029 terms\n",
      "parsed 3000/83029 terms\n",
      "parsed 4000/83029 terms\n",
      "parsed 5000/83029 terms\n",
      "parsed 6000/83029 terms\n",
      "parsed 7000/83029 terms\n",
      "parsed 8000/83029 terms\n",
      "parsed 9000/83029 terms\n",
      "parsed 10000/83029 terms\n",
      "parsed 11000/83029 terms\n",
      "parsed 12000/83029 terms\n",
      "parsed 13000/83029 terms\n",
      "parsed 14000/83029 terms\n",
      "parsed 15000/83029 terms\n",
      "parsed 16000/83029 terms\n",
      "parsed 17000/83029 terms\n",
      "parsed 18000/83029 terms\n",
      "parsed 19000/83029 terms\n",
      "parsed 20000/83029 terms\n",
      "parsed 21000/83029 terms\n",
      "parsed 22000/83029 terms\n",
      "parsed 23000/83029 terms\n",
      "parsed 24000/83029 terms\n",
      "parsed 25000/83029 terms\n",
      "parsed 26000/83029 terms\n",
      "parsed 27000/83029 terms\n",
      "parsed 28000/83029 terms\n",
      "parsed 29000/83029 terms\n",
      "parsed 30000/83029 terms\n",
      "parsed 31000/83029 terms\n",
      "parsed 32000/83029 terms\n",
      "parsed 33000/83029 terms\n",
      "parsed 34000/83029 terms\n",
      "parsed 35000/83029 terms\n",
      "parsed 36000/83029 terms\n",
      "parsed 37000/83029 terms\n",
      "parsed 38000/83029 terms\n",
      "parsed 39000/83029 terms\n",
      "parsed 40000/83029 terms\n",
      "parsed 41000/83029 terms\n",
      "parsed 42000/83029 terms\n",
      "parsed 43000/83029 terms\n",
      "parsed 44000/83029 terms\n",
      "parsed 45000/83029 terms\n",
      "parsed 46000/83029 terms\n",
      "parsed 47000/83029 terms\n",
      "parsed 48000/83029 terms\n",
      "parsed 49000/83029 terms\n",
      "parsed 50000/83029 terms\n",
      "parsed 51000/83029 terms\n",
      "parsed 52000/83029 terms\n",
      "parsed 53000/83029 terms\n",
      "parsed 54000/83029 terms\n",
      "parsed 55000/83029 terms\n",
      "parsed 56000/83029 terms\n",
      "parsed 57000/83029 terms\n",
      "parsed 58000/83029 terms\n",
      "parsed 59000/83029 terms\n",
      "parsed 60000/83029 terms\n",
      "parsed 61000/83029 terms\n",
      "parsed 62000/83029 terms\n",
      "parsed 63000/83029 terms\n",
      "parsed 64000/83029 terms\n",
      "parsed 65000/83029 terms\n",
      "parsed 66000/83029 terms\n",
      "parsed 67000/83029 terms\n",
      "parsed 68000/83029 terms\n",
      "parsed 69000/83029 terms\n",
      "parsed 70000/83029 terms\n",
      "parsed 71000/83029 terms\n",
      "parsed 72000/83029 terms\n",
      "parsed 73000/83029 terms\n",
      "parsed 74000/83029 terms\n",
      "parsed 75000/83029 terms\n",
      "parsed 76000/83029 terms\n",
      "parsed 77000/83029 terms\n",
      "parsed 78000/83029 terms\n",
      "parsed 79000/83029 terms\n",
      "parsed 80000/83029 terms\n",
      "parsed 81000/83029 terms\n",
      "parsed 82000/83029 terms\n",
      "parsed 83000/83029 terms\n",
      "Parsed all terms\n"
     ]
    }
   ],
   "source": [
    "mat = createWordDocumentMatrixNYT(num_files=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "Takes in a list V of words and returns the average sentiment score across all terms in V as determined by freebase. Note to Jason: consider other sentiment databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def getSentiment(word):\n",
    "    synset = list(swn.senti_synsets(word))\n",
    "    if len(synset) > 0: #if a synset exists for this word\n",
    "        synset = synset[0]\n",
    "        return(synset.pos_score(), synset.neg_score(), synset.obj_score())\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "V = ['good', 'bad', 'great', 'awesome', 'amazing', 'holy', 'beautiful', 'worrisome', 'stupid']\n",
    "def generate_sentiment(wordList):\n",
    "    totalSentiment = 0.0;\n",
    "    for word in wordList:\n",
    "        if is_ascii(word): #see note below for rationale\n",
    "            sentiment = getSentiment(word)\n",
    "            if sentiment == None:\n",
    "                sentiment = 0.0\n",
    "            if type(sentiment) is float: #why does this happen\n",
    "                print \"n/a\"\n",
    "            else:  \n",
    "                totalSentiment += (sentiment[0] - sentiment[1]) \n",
    "                print (sentiment[0] - sentiment[1])\n",
    "        #sentiwordnet generates tuples of pos, neg, and neu. currently naively choosing to consider only sum of pos and neg. \n",
    "    averageSentiment = totalSentiment/len(wordList)\n",
    "    return averageSentiment\n",
    "\n",
    "def generate_sentiment_2(wordTupleList):\n",
    "    reader = csv.reader(open('sentiment_words.txt', 'rb'))\n",
    "    sentiment_words = dict(reader)\n",
    "    sentiment_score = 0\n",
    "    for wordTuple in wordTupleList:\n",
    "        word = wordTuple[0]\n",
    "        score = 1/wordTuple[1] #inverse of distance\n",
    "        if word in sentiment_words:\n",
    "            if sentiment_words[word] == 'pos':\n",
    "                print word + \" +\" + str(score)\n",
    "                sentiment_score += score\n",
    "            if sentiment_words[word] == 'neg':\n",
    "                print word + \" -\" + str(score)\n",
    "                sentiment_score -= score\n",
    "    return sentiment_score\n",
    "\n",
    "print generate_sentiment_2(neighbors_list)\n",
    "#print generate_sentiment_2(correlated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    \"\"\"Cosine distance between 1d np.arrays `u` and `v`, which must have \n",
    "    the same dimensionality. Returns a float.\"\"\"\n",
    "    # Use scipy's method:\n",
    "    return scipy.spatial.distance.cosine(u, v)\n",
    "    # Or define it yourself:\n",
    "    # return 1.0 - (np.dot(u, v) / (vector_length(u) * vector_length(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semantic_orientation(\n",
    "        mat, \n",
    "        rownames,\n",
    "        seeds1=('bad', 'nasty', 'poor', 'negative', 'unfortunate', 'wrong', 'inferior'),\n",
    "        seeds2=('good', 'nice', 'excellent', 'positive', 'fortunate', 'correct', 'superior'),\n",
    "        distfunc=cosine):    \n",
    "    \"\"\"No frills implementation of the semantic Orientation (SO) method of \n",
    "    Turney and Littman. seeds1 and seeds2 should be representative members \n",
    "    of two intutively opposing semantic classes. The method will then try \n",
    "    to rank the vocabulary by its relative association with each seed set.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : 2d np.array\n",
    "        The matrix used to derive the SO ranking.\n",
    "        \n",
    "    rownames : list of str\n",
    "        The names of the rows of `mat` (the vocabulary).\n",
    "        \n",
    "    seeds1 : tuple of str\n",
    "        The default is the negative seed set of Turney and Littman.\n",
    "        \n",
    "    seeds2 : tuple of str\n",
    "        The default is the positive seed set of Turney and Littman.\n",
    "        \n",
    "    distfunc : function mapping vector pairs to floats (default: `cosine`)\n",
    "        The measure of distance between vectors. Can also be `euclidean`, \n",
    "        `matching`, `jaccard`, as well as any other distance measure \n",
    "        between 1d vectors. \n",
    "    \n",
    "    Returns\n",
    "    -------    \n",
    "    list of tuples\n",
    "        The vocabulary ranked according to the SO method, with words \n",
    "        closest to `seeds1` at the top and words closest to `seeds2` at the \n",
    "        bottom. Each member of the list is a (word, score) pair.\n",
    "    \n",
    "    \"\"\"    \n",
    "    sm1 = _so_seed_matrix(seeds1, mat, rownames)\n",
    "    sm2 = _so_seed_matrix(seeds2, mat, rownames)\n",
    "    scores = [(rownames[i], _so_row_func(mat[i], sm1, sm2, distfunc)) for i in xrange(len(mat))]\n",
    "    return sorted(scores, key=itemgetter(1), reverse=False)\n",
    "\n",
    "def _so_seed_matrix(seeds, mat, rownames):\n",
    "    indices = [rownames.index(word) for word in seeds if word in rownames]\n",
    "    if not indices:\n",
    "        raise ValueError('The matrix contains no members of the seed set: %s' % \",\".join(seeds))\n",
    "    print indices\n",
    "    print np.array(indices)\n",
    "    return mat[np.array(indices)]\n",
    "    \n",
    "def _so_row_func(row, sm1, sm2, distfunc):\n",
    "    val1 = np.sum([distfunc(row, srow) for srow in sm1])\n",
    "    val2 = np.sum([distfunc(row, srow) for srow in sm2])\n",
    "    return val1 - val2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we tokenize ignoring punctuation?\n",
    "\n",
    "Solution: for each word, look at last letter, if it is in a set of punctuation, remove that punctuation. Or, just strip away punctuation from the entire text in the very beginning. We're losing some degree of information but it is essentially a way of \"normalizing\" the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mat[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_rowshit = [u'!', u'):', u');', u'1', u'1/10', u'1/2', u'10', u'10/10', u'100', u'11', u'12', u'13', u'14', u'15', u'17', u'1950', u'1950s', u'1970', u'1980', u'2', u'20', u'2000', u'25', u'3', u'3/10', u'30', u'4', u'4/10', u'40', u'5', u'50', u'6', u'60', u'60s', u'7', u'7/10', u'70', u'70s', u'8', u'8/', u'80', u'80s', u'9', u'90', u':)', u'?', u'a', u'abandoned', u'ability', u'able', u'about', u'above', u'absolute', u'absolutely', u'absurd', u'abuse', u'academy', u'accent', u'accents', u'accept', u'accident', u'accidentally', u'according', u'account', u'accurate', u'achieve', u'across', u'act', u'acted', u'acting', u'action', u'actions', u'actor', u'actors', u'actress', u'actresses', u'acts', u'actual', u'actually', u'adam', u'adaptation', u'add', u'added', u'adding', u'addition', u'adds', u'admit', u'adult', u'adults', u'adventure', u'adventures', u'advice', u'affair', u'afraid', u'africa', u'african', u'after', u'afternoon', u'again', u'against', u'age', u'agent', u'ages', u'ago', u'agree', u'ahead', u\"ain't\", u'air', u'aka', u'al', u'alan', u'alas', u'albert', u'alex', u'alice', u'alien', u'aliens', u'alive', u'all', u'allen', u'allow', u'allowed', u'allows', u'almost', u'alone', u'along', u'already', u'alright', u'also', u'although', u'always', u'am', u'amateur', u'amateurish', u'amazed', u'amazing', u'amazingly', u'america', u'american', u'americans', u'among', u'amongst', u'amount', u'amusing', u'an', u'ancient', u'and', u'anderson', u'andy', u'angel', u'angels', u'anger', u'angle', u'angles', u'angry', u'animal', u'animals', u'animated', u'animation', u'anime', u'ann', u'anna', u'anne', u'annoying', u'another', u'answer', u'answers', u'anthony', u'any', u'anybody', u'anymore', u'anyone', u'anything', u'anyway', u'anywhere', u'apart', u'apartment', u'apparent', u'apparently', u'appeal', u'appealing', u'appear', u'appearance', u'appeared', u'appears', u'appreciate', u'appreciated', u'approach', u'appropriate', u'are', u'area', u\"aren't\", u'arms', u'army', u'around', u'arrives', u'art', u'arthur', u'artist', u'artistic', u'artists', u'arts', u'as', u'ashamed', u'asian', u'aside', u'ask', u'asked', u'asking', u'asks', u'asleep', u'aspect', u'aspects', u'ass', u'assume', u'at', u'atmosphere', u'atrocious', u'attack', u'attacked', u'attacks', u'attempt', u'attempting', u'attempts', u'attention', u'attitude', u'attractive', u'audience', u'audiences', u'aunt', u'australian', u'authentic', u'author', u'available', u'average', u'avoid', u'award', u'awards', u'aware', u'away', u'awesome', u'awful', u'awkward', u'b', u'b-movie', u'baby', u'back', u'background', u'bad', u'badly', u'balance', u'ball', u'band', u'bank', u'bar', u'barbara', u'barely', u'base', u'baseball', u'based', u'basic', u'basically', u'basis', u'batman', u'battle', u'bbc', u'be', u'beach', u'bear', u'beast', u'beat', u'beautiful', u'beautifully', u'beauty', u'became', u'because', u'become', u'becomes', u'becoming', u'bed', u'been', u'before', u'began', u'begin', u'beginning', u'begins', u'behavior', u'behind', u'being', u'belief', u'believable', u'believe', u'believed', u'believes', u'beloved', u'below', u'ben', u'besides', u'best', u'bet', u'better', u'between', u'beyond', u'big', u'bigger', u'biggest', u'bill', u'billy', u'birth', u'bit', u'bits', u'bizarre', u'black', u'blah', u'blair', u'blame', u'bland', u'blind', u'blockbuster', u'blonde', u'blood', u'bloody', u'blow', u'blown', u'blue', u'board', u'boat', u'bob', u'bodies', u'body', u'bollywood', u'bomb', u'bond', u'book', u'books', u'bore', u'bored', u'boring', u'born', u'boss', u'both', u'bother', u'bothered', u'bottom', u'bought', u'bourne', u'box', u'boy', u'boyfriend', u'boys', u'brad', u'brain', u'brave', u'break', u'breaking', u'breaks', u'breath', u'breathtaking', u'brian', u'brief', u'bright', u'brilliant', u'brilliantly', u'bring', u'bringing', u'brings', u'british', u'broadway', u'broken', u'brooks', u'brother', u'brothers', u'brought', u'brown', u'bruce', u'brutal', u'buddy', u'budget', u'build', u'building', u'built', u'bunch', u'burns', u'burt', u'bus', u'business', u'busy', u'but', u'buy', u'buying', u'by', u'c', u'cabin', u'cable', u'cage', u'caine', u'california', u'call', u'called', u'calling', u'calls', u'came', u'cameo', u'camera', u'camp', u'campy', u'can', u\"can't\", u'canadian', u'candy', u'cannot', u'cant', u'capable', u'captain', u'capture', u'captured', u'captures', u'car', u'care', u'career', u'cares', u'caring', u'carried', u'carries', u'carry', u'carrying', u'cars', u'cartoon', u'cartoons', u'case', u'cases', u'cash', u'cast', u'casting', u'castle', u'cat', u'catch', u'category', u'caught', u'cause', u'caused', u'causes', u'cell', u'center', u'central', u'century', u'certain', u'certainly', u'cgi', u'challenge', u'chance', u'change', u'changed', u'changes', u'changing', u'channel', u'character', u\"character's\", u'characters', u'charge', u'charles', u'charlie', u'charm', u'charming', u'chase', u'che', u'cheap', u'check', u'cheesy', u'chemistry', u'chick', u'chief', u'child', u'childhood', u'children', u\"children's\", u'chilling', u'china', u'chinese', u'choice', u'choices', u'choose', u'chose', u'chosen', u'chris', u'christian', u'christmas', u'christopher', u'church', u'cinderella', u'cinema', u'cinematic', u'cinematography', u'circumstances', u'city', u'claim', u'claims', u'claire', u'clark', u'class', u'classic', u'classics', u'clean', u'clear', u'clearly', u'clever', u'clich', u'climax', u'clips', u'close', u'closer', u'closing', u'clothes', u'club', u'clue', u'code', u'cold', u'collection', u'college', u'color', u'colors', u'columbo', u'combination', u'combined', u'come', u'comedic', u'comedies', u'comedy', u'comes', u'comic', u'comical', u'coming', u'comment', u'commentary', u'comments', u'commercial', u'committed', u'common', u'community', u'company', u'compare', u'compared', u'comparison', u'compelling', u'complete', u'completely', u'complex', u'complicated', u'computer', u'concept', u'concerned', u'conclusion', u'conflict', u'confused', u'confusing', u'confusion', u'connection', u'consider', u'considered', u'considering', u'constant', u'constantly', u'contain', u'contains', u'contemporary', u'content', u'context', u'continue', u'continues', u'continuity', u'contrast', u'contrived', u'control', u'conversation', u'convey', u'convince', u'convinced', u'convincing', u'cool', u'cop', u'cops', u'copy', u'core', u'corny', u'correct', u'cost', u'costs', u'costume', u'costumes', u'could', u\"could've\", u\"couldn't\", u'count', u'country', u'couple', u'course', u'court', u'cover', u'covered', u'cowboy', u'crap', u'crappy', u'crash', u'crazy', u'create', u'created', u'creates', u'creating', u'creative', u'creature', u'creatures', u'credit', u'credits', u'creepy', u'crew', u'crime', u'criminal', u'criminals', u'critical', u'criticism', u'critics', u'cross', u'crowd', u'crude', u'cruel', u'cry', u'crying', u'cult', u'cultural', u'culture', u'curious', u'current', u'cut', u'cute', u'cuts', u'cutting', u'd', u'dad', u'daily', u'damn', u'dan', u'dance', u'dancing', u'danger', u'dangerous', u'daniel', u'danny', u'dark', u'darkness', u'date', u'dated', u'daughter', u'daughters', u'david', u'davis', u'day', u'days', u'de', u'dead', u'deadly', u'deal', u'dealing', u'deals', u'dean', u'death', u'deaths', u'debut', u'decade', u'decades', u'decent', u'decide', u'decided', u'decides', u'decision', u'deep', u'deeper', u'deeply', u'definitely', u'degree', u'delight', u'delightful', u'deliver', u'delivered', u'delivers', u'delivery', u'demon', u'demons', u'dennis', u'department', u'depicted', u'depiction', u'depressing', u'depth', u'describe', u'described', u'description', u'desert', u'deserve', u'deserved', u'deserves', u'design', u'designed', u'desire', u'desperate', u'desperately', u'despite', u'destroy', u'destroyed', u'detail', u'details', u'detective', u'determined', u'develop', u'developed', u'development', u'device', u'devil', u'dialog', u'dialogue', u'dick', u'did', u\"didn't\", u'die', u'died', u'dies', u'difference', u'different', u'difficult', u'direct', u'directed', u'directing', u'direction', u'directly', u'director', u\"director's\", u'directors', u'dirty', u'disappointed', u'disappointing', u'disappointment', u'disaster', u'disbelief', u'discover', u'discovered', u'discovers', u'disgusting', u'disney', u'display', u'disturbing', u'do', u'doctor', u'documentary', u'does', u\"doesn't\", u'dog', u'dogs', u'doing', u'dollar', u'dollars', u'don', u\"don't\", u'donald', u'done', u'door', u'double', u'doubt', u'douglas', u'down', u'downright', u'dozen', u'dr', u'drag', u'dragon', u'drama', u'dramatic', u'draw', u'drawn', u'dreadful', u'dream', u'dreams', u'dress', u'dressed', u'drew', u'drinking', u'drive', u'driven', u'driver', u'driving', u'drop', u'drug', u'drugs', u'drunk', u'dry', u'dubbed', u'dude', u'due', u'dull', u'dumb', u'during', u'dvd', u'dying', u'e', u'each', u'earlier', u'early', u'earth', u'easily', u'easy', u'eat', u'eating', u'ed', u'eddie', u'edge', u'edited', u'editing', u'edward', u'effect', u'effective', u'effectively', u'effects', u'effort', u'efforts', u'eight', u'either', u'element', u'elements', u'elizabeth', u'else', u'embarrassed', u'embarrassing', u'emma', u'emotion', u'emotional', u'emotionally', u'emotions', u'empty', u'encounter', u'end', u'ended', u'ending', u'endless', u'ends', u'enemy', u'energy', u'engaging', u'england', u'english', u'enjoy', u'enjoyable', u'enjoyed', u'enjoying', u'enough', u'enter', u'entertain', u'entertained', u'entertaining', u'entertainment', u'entire', u'entirely', u'environment', u'epic', u'episode', u'episodes', u'equally', u'era', u'eric', u'erotic', u'escape', u'escapes', u'especially', u'essential', u'essentially', u'established', u'etc', u'europe', u'european', u'even', u'evening', u'event', u'events', u'eventually', u'ever', u'every', u'everybody', u'everyday', u'everyone', u'everything', u'everywhere', u'evidence', u'evil', u'exact', u'exactly', u'example', u'examples', u'excellent', u'except', u'exception', u'excited', u'excitement', u'exciting', u'excuse', u'executed', u'execution', u'exist', u'existence', u'exists', u'expect', u'expectations', u'expected', u'expecting', u'experience', u'experienced', u'experiences', u'experiment', u'expert', u'explain', u'explained', u'explains', u'explanation', u'exploitation', u'express', u'expression', u'extent', u'extra', u'extraordinary', u'extras', u'extreme', u'extremely', u'eye', u'eyes', u'f', u'fabulous', u'face', u'faces', u'facial', u'fact', u'factor', u'facts', u'fail', u'failed', u'fails', u'failure', u'fair', u'fairly', u'fairy', u'faith', u'faithful', u'fake', u'fall', u'fallen', u'falling', u'falls', u'false', u'fame', u'familiar', u'families', u'family', u'famous', u'fan', u'fans', u'fantastic', u'fantasy', u'far', u'fare', u'fascinating', u'fashion', u'fast', u'fat', u'fate', u'father', u\"father's\", u'fault', u'favor', u'favorite', u'favorites', u'favourite', u'fear', u'feature', u'featured', u'features', u'featuring', u'feel', u'feeling', u'feelings', u'feels', u'feet', u'fell', u'fellow', u'felt', u'female', u'festival', u'few', u'fiction', u'fictional', u'field', u'fight', u'fighting', u'fights', u'figure', u'figured', u'figures', u'fill', u'filled', u'film', u\"film's\", u'film-making', u'filmed', u'filming', u'filmmaker', u'filmmakers', u'films', u'final', u'finale', u'finally', u'find', u'finding', u'finds', u'fine', u'finest', u'finish', u'finished', u'fire', u'first', u'fit', u'fits', u'five', u'flashback', u'flashbacks', u'flat', u'flaws', u'flesh', u'flick', u'flicks', u'flight', u'floor', u'flow', u'fly', u'flying', u'focus', u'focused', u'focuses', u'folks', u'follow', u'followed', u'following', u'follows', u'food', u'fool', u'foot', u'footage', u'football', u'for', u'force', u'forced', u'forces', u'ford', u'foreign', u'forest', u'forever', u'forget', u'forgettable', u'forgot', u'forgotten', u'form', u'format', u'former', u'formula', u'forth', u'fortunately', u'forward', u'foster', u'found', u'four', u'fourth', u'fox', u'frame', u'france', u'frank', u'frankly', u'fred', u'freddy', u'free', u'freedom', u'freeman', u'french', u'frequently', u'fresh', u'friday', u'friend', u'friendly', u'friends', u'friendship', u'frightening', u'from', u'front', u'fu', u'full', u'fully', u'fun', u'funnier', u'funniest', u'funny', u'further', u'future', u'g', u'gags', u'game', u'games', u'gang', u'gangster', u'garbage', u'gary', u'gas', u'gave', u'gay', u'gem', u'gene', u'general', u'generally', u'generation', u'genius', u'genre', u'genuine', u'genuinely', u'george', u'german', u'germany', u'get', u'gets', u'getting', u'ghost', u'ghosts', u'giant', u'girl', u'girlfriend', u'girls', u'give', u'given', u'gives', u'giving', u'glad', u'go', u'god', u'goes', u'going', u'gold', u'golden', u'gone', u'gonna', u'good', u'goofy', u'gordon', u'gore', u'gorgeous', u'gory', u'got', u'gotten', u'government', u'grace', u'grade', u'grand', u'grant', u'granted', u'graphic', u'graphics', u'gratuitous', u'grave', u'great', u'greater', u'greatest', u'green', u'grew', u'grim', u'gritty', u'ground', u'group', u'grow', u'growing', u'grown', u'gruesome', u'guard', u'guess', u'guilty', u'gun', u'guns', u'guy', u'guys', u'h', u'ha', u'had', u\"hadn't\", u'hair', u'half', u'halfway', u'hall', u'halloween', u'hand', u'handle', u'handled', u'hands', u'handsome', u'hanging', u'happen', u'happened', u'happening', u'happens', u'happiness', u'happy', u'hard', u'hardly', u'hardy', u'harris', u'harry', u'harsh', u'has', u\"hasn't\", u'hat', u'hate', u'hated', u'haunted', u'haunting', u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he's\", u'head', u'heads', u'hear', u'heard', u'hearing', u'heart', u'heaven', u'heavily', u'heavy', u'heck', u'held', u'hell', u'help', u'helped', u'helping', u'helps', u'henry', u'her', u'here', u\"here's\", u'hero', u'heroes', u'heroine', u'herself', u'hey', u'hidden', u'hide', u'high', u'higher', u'highlight', u'highly', u'hilarious', u'hill', u'him', u'himself', u'hired', u'his', u'historical', u'history', u'hit', u'hitchcock', u'hitler', u'hits', u'hoffman', u'hold', u'holding', u'holds', u'holes', u'hollywood', u'home', u'honest', u'honestly', u'hong', u'honor', u'hope', u'hopefully', u'hopes', u'hoping', u'horrible', u'horribly', u'horrific', u'horror', u'horse', u'hospital', u'hot', u'hotel', u'hour', u'hours', u'house', u'how', u'howard', u'however', u'huge', u'human', u'humanity', u'humans', u'humor', u'humorous', u'humour', u'hunt', u'hunter', u'hurt', u'husband', u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'ice', u'idea', u'ideas', u'identity', u'idiot', u'if', u'ignore', u'ii', u'ill', u'image', u'imagery', u'images', u'imagination', u'imagine', u'imdb', u'immediately', u'impact', u'important', u'impossible', u'impressed', u'impression', u'impressive', u'in', u'include', u'included', u'includes', u'including', u'incredible', u'incredibly', u'indeed', u'independent', u'india', u'indian', u'indie', u'individual', u'industry', u'inept', u'influence', u'information', u'initial', u'initially', u'inner', u'innocent', u'insane', u'inside', u'insight', u'inspector', u'inspiration', u'inspired', u'instance', u'instead', u'insult', u'intellectual', u'intelligence', u'intelligent', u'intended', u'intense', u'intensity', u'intentions', u'interest', u'interested', u'interesting', u'international', u'interpretation', u'interview', u'interviews', u'into', u'intriguing', u'introduced', u'introduction', u'invisible', u'involved', u'involves', u'involving', u'irish', u'ironic', u'irritating', u'is', u'island', u\"isn't\", u'issue', u'issues', u'it', u\"it's\", u'italian', u'its', u'itself', u'j', u'jack', u'jackie', u'jackson', u'jail', u'james', u'jane', u'japan', u'japanese', u'jason', u'jean', u'jeff', u'jennifer', u'jerry', u'jessica', u'jesus', u'jewish', u'jim', u'jimmy', u'joan', u'job', u'jobs', u'joe', u'john', u'johnny', u'johnson', u'join', u'joke', u'jokes', u'jon', u'jones', u'joseph', u'journey', u'joy', u'jr', u'judge', u'julia', u'julie', u'jump', u'jumps', u'jungle', u'junk', u'just', u'justice', u'k', u'kate', u'keaton', u'keep', u'keeping', u'keeps', u'kelly', u'kept', u'kevin', u'key', u'kick', u'kid', u'kids', u'kill', u'killed', u'killer', u'killers', u'killing', u'kills', u'kim', u'kind', u'kinda', u'kinds', u'king', u'kiss', u'knew', u'know', u'knowing', u'knowledge', u'known', u'knows', u'kong', u'kung', u'l', u'la', u'lack', u'lacking', u'lacks', u'ladies', u'lady', u'lake', u'lame', u'land', u'lane', u'language', u'large', u'largely', u'larry', u'last', u'late', u'later', u'latest', u'latter', u'laugh', u'laughable', u'laughed', u'laughing', u'laughs', u'laughter', u'laura', u'law', u'lawyer', u'lazy', u'lead', u'leader', u'leading', u'leads', u'league', u'learn', u'learned', u'learning', u'learns', u'least', u'leave', u'leaves', u'leaving', u'led', u'lee', u'left', u'legend', u'legendary', u'legs', u'length', u'lesbian', u'leslie', u'less', u'lesson', u'let', u\"let's\", u'lets', u'level', u'levels', u'lewis', u'lie', u'lies', u'life', u'lifetime', u'light', u'lighting', u'lights', u'likable', u'like', u'liked', u'likely', u'likes', u'limited', u'line', u'lines', u'lisa', u'list', u'listen', u'listening', u'literally', u'little', u'live', u'lived', u'lives', u'living', u'local', u'location', u'locations', u'locked', u'logic', u'london', u'lonely', u'long', u'longer', u'look', u'looked', u'looking', u'looks', u'loose', u'lord', u'lose', u'loses', u'losing', u'loss', u'lost', u'lot', u'lots', u'loud', u'louis', u'lousy', u'love', u'loved', u'lovely', u'lover', u'lovers', u'loves', u'loving', u'low', u'low-budget', u'lower', u'luck', u'lucky', u'ludicrous', u'lugosi', u'luke', u'lynch', u'm', u'machine', u'mad', u'made', u'madness', u'magic', u'magical', u'magnificent', u'main', u'mainly', u'mainstream', u'major', u'majority', u'make', u'make-up', u'makers', u'makes', u'makeup', u'making', u'male', u'man', u\"man's\", u'manage', u'managed', u'manager', u'manages', u'manner', u'mansion', u'many', u'maria', u'marie', u'mark', u'market', u'marriage', u'married', u'marry', u'martial', u'martin', u'marvelous', u'mary', u'mask', u'massive', u'master', u'masterpiece', u'match', u'material', u'matrix', u'matt', u'matter', u'matters', u'mature', u'max', u'may', u'maybe', u'me', u'mean', u'meaning', u'means', u'meant', u'meanwhile', u'media', u'mediocre', u'meet', u'meeting', u'meets', u'melodrama', u'member', u'members', u'memorable', u'memories', u'memory', u'men', u'mental', u'mention', u'mentioned', u'mere', u'merely', u'mess', u'message', u'met', u'metal', u'mexican', u'mexico', u'mgm', u'michael', u'michelle', u'mid', u'middle', u'midnight', u'might', u'mike', u'mildly', u'miles', u'military', u'million', u'mind', u'minds', u'mine', u'minor', u'minute', u'minutes', u'mirror', u'miss', u'missed', u'missing', u'mission', u'mistake', u'mistakes', u'mix', u'mixed', u'model', u'modern', u'mom', u'moment', u'moments', u'money', u'monster', u'monsters', u'months', u'mood', u'moon', u'moore', u'moral', u'more', u'morgan', u'morning', u'most', u'mostly', u'mother', u'motion', u'mountain', u'mouth', u'move', u'moved', u'movement', u'moves', u'movie', u\"movie's\", u'movies', u'moving', u'mr', u'mrs', u'ms', u'mst', u'much', u'multiple', u'murder', u'murdered', u'murderer', u'murders', u'murphy', u'music', u'musical', u'musicals', u'must', u'my', u'myself', u'mysterious', u'mystery', u'n', u'naive', u'naked', u'name', u'named', u'names', u'nancy', u'narration', u'narrative', u'nasty', u'nation', u'national', u'native', u'natural', u'naturally', u'nature', u'navy', u'near', u'nearly', u'necessarily', u'necessary', u'ned', u'need', u'needed', u'needless', u'needs', u'negative', u'neither', u'network', u'never', u'nevertheless', u'new', u'news', u'next', u'nice', u'nicely', u'nick', u'night', u'nightmare', u'no', u'nobody', u'noir', u'nominated', u'none', u'nonetheless', u'nonsense', u'nor', u'normal', u'normally', u'north', u'not', u'notable', u'note', u'nothing', u'notice', u'noticed', u'notorious', u'novel', u'novels', u'now', u'nowadays', u'nowhere', u'nude', u'nudity', u'number', u'numbers', u'numerous', u'o', u'obnoxious', u'obsessed', u'obsession', u'obvious', u'obviously', u'occasional', u'occasionally', u'odd', u'oddly', u'of', u'off', u'offensive', u'offer', u'offered', u'offers', u'office', u'officer', u'often', u'oh', u'ok', u'okay', u'old', u'older', u'oliver', u'on', u'once', u'one', u\"one's\", u'ones', u'only', u'onto', u'open', u'opening', u'opens', u'opera', u'opinion', u'opportunity', u'opposite', u'or', u'order', u'ordinary', u'original', u'originality', u'originally', u'oscar', u'other', u'others', u'otherwise', u'our', u'out', u'outside', u'outstanding', u'over', u'over-the-top', u'overall', u'overly', u'own', u'owner', u'p', u'pace', u'paced', u'pacing', u'pacino', u'page', u'paid', u'pain', u'painful', u'painfully', u'paint', u'pair', u'paper', u'par', u'parents', u'paris', u'park', u'parker', u'parody', u'part', u'particular', u'particularly', u'partner', u'parts', u'party', u'pass', u'passed', u'passing', u'passion', u'past', u'path', u'pathetic', u'patrick', u'paul', u'pay', u'paying', u'peace', u'people', u\"people's\", u'perfect', u'perfectly', u'performance', u'performances', u'performed', u'perhaps', u'period', u'person', u'personal', u'personalities', u'personality', u'personally', u'perspective', u'pet', u'peter', u'phone', u'photography', u'physical', u'pick', u'picked', u'picks', u'picture', u'pictures', u'piece', u'pieces', u'pile', u'pilot', u'pitt', u'pity', u'place', u'placed', u'places', u'plain', u'plan', u'plane', u'planet', u'plans', u'play', u'played', u'player', u'players', u'playing', u'plays', u'pleasant', u'please', u'pleasure', u'plenty', u'plot', u'plots', u'plus', u'poignant', u'point', u'pointless', u'points', u'police', u'political', u'politics', u'poor', u'poorly', u'pop', u'popular', u'porn', u'portray', u'portrayal', u'portrayed', u'portraying', u'portrays', u'position', u'positive', u'possible', u'possibly', u'post', u'potential', u'powell', u'power', u'powerful', u'powers', u'practically', u'praise', u'predictable', u'prefer', u'pregnant', u'premise', u'prepared', u'presence', u'present', u'presentation', u'presented', u'presents', u'president', u'pretentious', u'pretty', u'previous', u'previously', u'price', u'priest', u'prime', u'prince', u'princess', u'print', u'prior', u'prison', u'private', u'probably', u'problem', u'problems', u'process', u'produce', u'produced', u'producer', u'producers', u'product', u'production', u'productions', u'professional', u'professor', u'program', u'project', u'promise', u'promising', u'propaganda', u'proper', u'properly', u'protagonist', u'protect', u'proud', u'prove', u'proved', u'proves', u'provide', u'provided', u'provides', u'psycho', u'psychological', u'public', u'pull', u'pulled', u'pulls', u'punch', u'pure', u'purely', u'purpose', u'put', u'puts', u'putting', u'qualities', u'quality', u'queen', u'quest', u'question', u'questions', u'quick', u'quickly', u'quiet', u'quirky', u'quite', u'r', u'race', u'rachel', u'racist', u'radio', u'rain', u'raise', u'raised', u'ran', u'random', u'range', u'rape', u'rare', u'rarely', u'rate', u'rated', u'rather', u'rating', u'ratings', u'raw', u'ray', u'reach', u'reaction', u'read', u'reading', u'ready', u'real', u'realism', u'realistic', u'reality', u'realize', u'realized', u'realizes', u'really', u'reason', u'reasons', u'recall', u'received', u'recent', u'recently', u'recognize', u'recommend', u'recommended', u'record', u'red', u'redeeming', u'reference', u'references', u'refreshing', u'regard', u'regarding', u'regret', u'regular', u'relate', u'related', u'relationship', u'relationships', u'relatively', u'release', u'released', u'relief', u'religion', u'religious', u'remain', u'remains', u'remake', u'remarkable', u'remember', u'remembered', u'remind', u'reminded', u'reminds', u'reminiscent', u'remote', u'remotely', u'rent', u'rental', u'rented', u'renting', u'repeated', u'replaced', u'reporter', u'reputation', u'required', u'rescue', u'research', u'respect', u'responsible', u'rest', u'result', u'results', u'retarded', u'return', u'returns', u'reveal', u'revealed', u'reveals', u'revenge', u'review', u'reviewer', u'reviewers', u'reviews', u'revolution', u'rich', u'richard', u'ride', u'ridiculous', u'right', u'rights', u'ring', u'rings', u'rip', u'rise', u'risk', u'rival', u'river', u'road', u'rob', u'robert', u'robin', u'robot', u'rock', u'roger', u'rogers', u'role', u'roles', u'roll', u'rolling', u'romance', u'romantic', u'ron', u'room', u'rose', u'rough', u'round', u'routine', u'roy', u'rubbish', u'ruin', u'ruined', u'rule', u'rules', u'run', u'running', u'runs', u'russell', u'russian', u'ryan', u's', u'sad', u'sadly', u'safe', u'said', u'sake', u'sam', u'same', u'san', u'santa', u'sarah', u'sat', u'satire', u'satisfying', u'saturday', u'save', u'saved', u'saving', u'saw', u'say', u'saying', u'says', u'scale', u'scare', u'scared', u'scares', u'scary', u'scenario', u'scene', u'scenery', u'scenes', u'school', u'sci-fi', u'science', u'scientist', u'score', u'scott', u'scream', u'screaming', u'screen', u'screening', u'screenplay', u'script', u'sea', u'sean', u'search', u'season', u'seasons', u'seat', u'second', u'seconds', u'secret', u'section', u'security', u'see', u'seeing', u'seek', u'seem', u'seemed', u'seemingly', u'seems', u'seen', u'sees', u'segment', u'self', u'sell', u'send', u'sense', u'sensitive', u'sent', u'sequel', u'sequels', u'sequence', u'sequences', u'serial', u'series', u'serious', u'seriously', u'serve', u'served', u'serves', u'service', u'set', u'sets', u'setting', u'settings', u'seven', u'several', u'sex', u'sexual', u'sexy', u'shadow', u'shakespeare', u'shallow', u'shame', u'share', u'sharp', u'she', u\"she's\", u'sheer', u'sheriff', u'ship', u'shock', u'shocked', u'shocking', u'shoot', u'shooting', u'shop', u'short', u'shot', u'shots', u'should', u\"shouldn't\", u'show', u'showed', u'shower', u'showing', u'shown', u'shows', u'shut', u'sick', u'side', u'sides', u'sidney', u'sight', u'sign', u'significant', u'silent', u'silly', u'similar', u'simon', u'simple', u'simply', u'sinatra', u'since', u'sing', u'singer', u'singing', u'single', u'sinister', u'sir', u'sister', u'sisters', u'sit', u'sitcom', u'site', u'sitting', u'situation', u'situations', u'six', u'skill', u'skills', u'skin', u'skip', u'sky', u'slapstick', u'slasher', u'sleazy', u'sleep', u'sleeping', u'slightly', u'slow', u'slowly', u'small', u'smart', u'smile', u'smith', u'so', u'so-called', u'soap', u'social', u'society', u'soft', u'sold', u'soldier', u'soldiers', u'solid', u'some', u'somebody', u'somehow', u'someone', u'something', u'sometimes', u'somewhat', u'somewhere', u'son', u'song', u'songs', u'soon', u'sorry', u'sort', u'sorts', u'soul', u'sound', u'sounded', u'sounds', u'soundtrack', u'source', u'south', u'southern', u'space', u'spanish', u'speak', u'speaking', u'speaks', u'special', u'spectacular', u'speech', u'speed', u'spend', u'spends', u'spent', u'spirit', u'spite', u'spoil', u'spoiler', u'spoilers', u'spoof', u'sports', u'spot', u'spy', u'stage', u'stand', u'standard', u'standards', u'standing', u'stands', u'stanley', u'star', u'starred', u'starring', u'stars', u'start', u'started', u'starting', u'starts', u'state', u'statement', u'states', u'station', u'status', u'stay', u'stayed', u'stays', u'steal', u'steals', u'step', u'stephen', u'stereotypes', u'stereotypical', u'steve', u'steven', u'stewart', u'stick', u'still', u'stock', u'stolen', u'stomach', u'stone', u'stop', u'stopped', u'stops', u'store', u'stories', u'story', u'storyline', u'storytelling', u'straight', u'strange', u'strangely', u'street', u'streets', u'strength', u'strong', u'strongly', u'structure', u'struggle', u'struggling', u'stuck', u'student', u'students', u'studio', u'studios', u'study', u'stuff', u'stunning', u'stupid', u'stupidity', u'style', u'stylish', u'subject', u'substance', u'subtitles', u'subtle', u'succeeds', u'success', u'successful', u'successfully', u'such', u'suck', u'sucked', u'sucks', u'sudden', u'suddenly', u'suffer', u'suffering', u'suffers', u'suggest', u'suicide', u'suit', u'summary', u'summer', u'sun', u'sunday', u'super', u'superb', u'superior', u'superman', u'supernatural', u'support', u'supporting', u'suppose', u'supposed', u'supposedly', u'sure', u'surely', u'surface', u'surprise', u'surprised', u'surprises', u'surprising', u'surprisingly', u'surreal', u'survive', u'susan', u'suspect', u'suspects', u'suspense', u'suspenseful', u'sweet', u'sword', u'sympathetic', u'sympathy', u'system', u't', u'table', u'take', u'taken', u'takes', u'taking', u'tale', u'talent', u'talented', u'talents', u'tales', u'talk', u'talking', u'talks', u'tape', u'target', u'tarzan', u'task', u'taste', u'taylor', u'teacher', u'team', u'tears', u'technical', u'technically', u'technology', u'ted', u'tedious', u'teen', u'teenage', u'teenager', u'teenagers', u'teens', u'teeth', u'television', u'tell', u'telling', u'tells', u'ten', u'tend', u'tension', u'term', u'terms', u'terrible', u'terribly', u'terrific', u'terror', u'test', u'texas', u'than', u'thank', u'thankfully', u'thanks', u'that', u\"that's\", u'thats', u'the', u'theater', u'theaters', u'theatre', u'theatrical', u'their', u'them', u'theme', u'themes', u'themselves', u'then', u'theory', u'there', u\"there's\", u'therefore', u'these', u'they', u\"they're\", u\"they've\", u'thin', u'thing', u'things', u'think', u'thinking', u'thinks', u'third', u'this', u'thomas', u'thoroughly', u'those', u'though', u'thought', u'thoughts', u'three', u'thriller', u'thrilling', u'through', u'throughout', u'throw', u'throwing', u'thrown', u'throws', u'thus', u'tight', u'till', u'tim', u'time', u'times', u'timing', u'tiny', u'tired', u'titanic', u'title', u'titles', u'to', u'today', u\"today's\", u'together', u'told', u'tom', u'tone', u'tony', u'too', u'took', u'top', u'topic', u'torture', u'total', u'totally', u'touch', u'touched', u'touches', u'touching', u'tough', u'toward', u'towards', u'town', u'track', u'tradition', u'traditional', u'tragedy', u'tragic', u'trailer', u'train', u'training', u'trapped', u'trash', u'travel', u'treasure', u'treat', u'treated', u'treatment', u'tree', u'trek', u'trick', u'tried', u'tries', u'trilogy', u'trip', u'trouble', u'truck', u'true', u'truly', u'trust', u'truth', u'try', u'trying', u'turkey', u'turn', u'turned', u'turning', u'turns', u'tv', u'twenty', u'twice', u'twist', u'twisted', u'twists', u'two', u'type', u'types', u'typical', u'u', u'ugly', u'uk', u'ultimate', u'ultimately', u'unable', u'unbelievable', u'uncle', u'unconvincing', u'under', u'underground', u'underrated', u'understand', u'understanding', u'understood', u'unexpected', u'unfortunate', u'unfortunately', u'unfunny', u'uninteresting', u'unique', u'united', u'universal', u'universe', u'unknown', u'unless', u'unlike', u'unlikely', u'unnecessary', u'unrealistic', u'until', u'unusual', u'up', u'upon', u'urban', u'us', u'usa', u'use', u'used', u'uses', u'using', u'usual', u'usually', u'utter', u'utterly', u'v', u'vacation', u'value', u'values', u'vampire', u'vampires', u'van', u'variety', u'various', u'vehicle', u'version', u'versions', u'very', u'veteran', u'vhs', u'via', u'victim', u'victims', u'victor', u'victoria', u'video', u'vietnam', u'view', u'viewed', u'viewer', u'viewers', u'viewing', u'views', u'village', u'villain', u'villains', u'violence', u'violent', u'virtually', u'vision', u'visit', u'visual', u'visually', u'visuals', u'voice', u'voices', u'von', u'vote', u'vs', u'w', u'wait', u'waiting', u'walk', u'walked', u'walking', u'walks', u'wall', u'walter', u'want', u'wanted', u'wanting', u'wants', u'war', u'warm', u'warned', u'warner', u'warning', u'wars', u'was', u'washington', u\"wasn't\", u'waste', u'wasted', u'watch', u'watchable', u'watched', u'watching', u'water', u'wave', u'way', u'wayne', u'ways', u'we', u\"we're\", u\"we've\", u'weak', u'weapons', u'wear', u'wearing', u'wears', u'wedding', u'week', u'weekend', u'weeks', u'weird', u'welcome', u'well', u'welles', u'went', u'were', u\"weren't\", u'werewolf', u'west', u'western', u'westerns', u'what', u\"what's\", u'whatever', u'whatsoever', u'when', u'whenever', u'where', u'whether', u'which', u'while', u'whilst', u'white', u'who', u\"who's\", u'whoever', u'whole', u'whom', u'whose', u'why', u'wide', u'wife', u'wild', u'will', u'william', u'williams', u'willing', u'wilson', u'win', u'wind', u'window', u'winner', u'winning', u'wins', u'wise', u'wish', u'wit', u'witch', u'with', u'within', u'without', u'witness', u'witty', u'woman', u'women', u'won', u\"won't\", u'wonder', u'wonderful', u'wonderfully', u'wondering', u'wood', u'wooden', u'woods', u'woody', u'word', u'words', u'work', u'worked', u'working', u'works', u'world', u'worse', u'worst', u'worth', u'worthwhile', u'worthy', u'would', u\"would've\", u\"wouldn't\", u'wow', u'write', u'writer', u'writers', u'writing', u'written', u'wrong', u'wrote', u'x', u'yeah', u'year', u'year-old', u'years', u'yes', u'yet', u'york', u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'young', u'younger', u'your', u'yourself', u'youth', u'zero', u'zombie', u'zombies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.array(mat[0])\n",
    "print np.array(temp_rowshit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_rownames = ['hello', 'test', 'pie', 'dirty', 'bad', 'good']\n",
    "so = semantic_orientation(mat=np.array(mat[0]), rownames=mat[1])\n",
    "so[:5]\n",
    "so[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert from list to easily searchable hashmap\n",
    "word_scores = dict()\n",
    "for tup in so:\n",
    "    word_scores[tup[0]] = tup[1]\n",
    "    \n",
    "def get_semantic_score(word_list):\n",
    "    score = 0\n",
    "    for word in word_list:\n",
    "        if word in word_scores:\n",
    "            score += word_scores[word]\n",
    "        else:\n",
    "            print 'not in vocab'\n",
    "    return score\n",
    "\n",
    "get_semantic_score(['good', 'bad', 'john'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"你好\".encode('utf-8')\n",
    "encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent of\n",
    "\n",
    "\"你好\".decode().encode('utf-8')\n",
    "But the decode fails because the string isn't valid ascii. That's why you get a complaint about not being able to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR/AND\n",
    "Takes in a dict of corpus:list of words and returns a dict of corpus:XOR words and dict of corpus:AND words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyList = ['black', 'block', 'beer']\n",
    "\n",
    "def XOR(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first ^ second\n",
    "def AND(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first & second\n",
    "\n",
    "print 'XOR'\n",
    "print XOR(toyList, neighbors_word_list)\n",
    "print 'AND'\n",
    "print AND(toyList, neighbors_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "Takes in a matrix M and correlation list L. Using t-sne, produces a word cloud which represents correlation between all terms. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix: array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold='nan')\n",
    "\n",
    "def word_cloud_preprocessing(words, matrix=mat_ppmi):\n",
    "    output = []\n",
    "    for word in words:\n",
    "        ind = matrix[1].index(word)\n",
    "        output.append(matrix[0][ind])\n",
    "    return output\n",
    "processed_mat = word_cloud_preprocessing(neighbors_word_list)\n",
    "print processed_mat\n",
    "\n",
    "def word_cloud(corr_list): #i think its processed_mat / didn't tsne take in a vector of labels as well?\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    tsne_matrix = model.fit_transform(corr_list)\n",
    "    \n",
    "word_cloud(processed_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic modeling, currently using dummy data from lda.datasets\n",
    "\n",
    "NOTE: rerunning can cause relabeling, which means that topic 0 in the first run might now be topic 15 in the next run, so don't be worried if the topic numbers change from run to run\n",
    "\n",
    "run this on the command line first: pip install --user lda\n",
    "\n",
    "https://pypi.python.org/pypi/lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#use pip show lda to find the path of where it's installed for you and modify the path append line below with your location\n",
    "import sys\n",
    "sys.path.append('/Users/theodorachu/.local/lib/python2.7/site-packages')\n",
    "\n",
    "import lda\n",
    "import lda.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document-term matrix\n",
    "X = lda.datasets.load_reuters()\n",
    "print(\"type(X): {}\".format(type(X)))\n",
    "print(\"shape: {}\\n\".format(X.shape))\n",
    "\n",
    "# the vocab\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "print(\"type(vocab): {}\".format(type(vocab)))\n",
    "print(\"len(vocab): {}\\n\".format(len(vocab)))\n",
    "\n",
    "# titles for each story\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "print(\"type(titles): {}\".format(type(titles)))\n",
    "print(\"len(titles): {}\\n\".format(len(titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example print statements\n",
    "#gets word 3117 from document 0\n",
    "\n",
    "doc_id = 0\n",
    "word_id = 3117\n",
    "\n",
    "print(\"doc id: {} word id: {}\".format(doc_id, word_id))\n",
    "print(\"-- count: {}\".format(X[doc_id, word_id]))\n",
    "print(\"-- word : {}\".format(vocab[word_id]))\n",
    "print(\"-- doc  : {}\".format(titles[doc_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "\n",
    "model = lda.LDA(n_topics=20, n_iter=500, random_state=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#topic-word probabilities\n",
    "#shape: (num topics, num words)\n",
    "\n",
    "topic_word = model.topic_word_\n",
    "print(\"type(topic_word): {}\".format(type(topic_word)))\n",
    "print(\"shape: {}\".format(topic_word.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spits out top n words for each topic by probability\n",
    "\n",
    "n = 5\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
    "    print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#document-topic probabilities\n",
    "#shape: (num documents, num topics)\n",
    "\n",
    "doc_topic = model.doc_topic_\n",
    "print(\"type(doc_topic): {}\".format(type(doc_topic)))\n",
    "print(\"shape: {}\".format(doc_topic.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualizing the inference - matlab setup/imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use matplotlib style sheet\n",
    "try:\n",
    "    plt.style.use('ggplot')\n",
    "except:\n",
    "    # version of matplotlib might not be recent\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "right now the plots don't print? it just throws the notebook into busy mode for a very long time so not sure if something is off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stem plots - height of each stem reflects the probability of the word in the focus topic\n",
    "\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([0, 5, 9, 14, 19]):\n",
    "    ax[i].stem(topic_word[k,:], linefmt='b-',\n",
    "               markerfmt='bo', basefmt='w-')\n",
    "    ax[i].set_xlim(-50,4350)\n",
    "    ax[i].set_ylim(0, 0.08)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"topic {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"word\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topic distribution - probability of each of the 20 topics for every document\n",
    "f, ax= plt.subplots(5, 1, figsize=(8, 6), sharex=True)\n",
    "for i, k in enumerate([1, 3, 4, 8, 9]): #only plotting these specified topics\n",
    "    ax[i].stem(doc_topic[k,:], linefmt='r-',\n",
    "               markerfmt='ro', basefmt='w-')\n",
    "    ax[i].set_xlim(-1, 21)\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel(\"Prob\")\n",
    "    ax[i].set_title(\"Document {}\".format(k))\n",
    "\n",
    "ax[4].set_xlabel(\"Topic\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
