{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Theodora Chu, Josh Cohen, Jason Chen\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2016 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Info for creating VSM data\n",
    "vsmdata_home = \"vsmdata\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.spatial.distance\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input\n",
    "Takes in a text file and returns a list of ordered unigrams U. \n",
    "It should also consider stemming and other relevant pre-processing. Josh's note: parse \"African American\" as a unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseTextFile(filename):\n",
    "    text = open('cor-por-a/' + filename, 'r')\n",
    "    for i in range(0, 10):\n",
    "        print text.readline()\n",
    "    text_parse = text.read().split()\n",
    "    #print text_parse\n",
    "\n",
    "    lancaster = LancasterStemmer()\n",
    "#     print lancaster.stem('maximum') \n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    return text_parse\n",
    "#     print porter.stem('maximum')    \n",
    "\n",
    "#parseTextFile('TomSawyer.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "1. Parse U to create a word-word frequency matrix M, where each row represents a word and each entry x(i,j) represents the number of times word i co-occurs with word j.\n",
    "2. Convert M to a new matrix M’ with some sort of correlation operation. We could use PMI, Occai (see Josh’s paper), CSA, or some other correlation structure.\n",
    "3. Let row a represent the unigram “African American”. Take in that row, and output an ordered list of (this_unigram, correlation_score) pairs which represent the correlation score of this_unigram with the term “African American”\n",
    "4. Produce a list L of the top 100 correlated words with the term “African American”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This takes fucking forever\n",
    "def createMatrix():\n",
    "    # Initializes vector of terms\n",
    "    u_vec = [x.lower() for x in parseTextFile('TomSawyer.txt')];\n",
    "    vocab_vec = np.unique(u_vec).tolist()\n",
    "    vocab_size = len(vocab_vec)\n",
    "    mat = [[0 for x in range(vocab_size)] for y in range(vocab_size)]\n",
    "    \n",
    "    # Updates matrix, using bigrams\n",
    "    for i in range(0, len(u_vec)-1):\n",
    "        term_one = u_vec[i];\n",
    "        term_two = u_vec[i+1];\n",
    "        index_one = vocab_vec.index(term_one)\n",
    "        index_two = vocab_vec.index(term_two)\n",
    "        mat[index_one][index_one] += 1;\n",
    "        mat[index_one][index_two] += 1;\n",
    "        mat[index_two][index_one] += 1;\n",
    "\n",
    "    last_term = u_vec[len(u_vec)-1]\n",
    "    last_term_index = vocab_vec.index(last_term)\n",
    "    mat[last_term_index][last_term_index] += 1\n",
    "    return (mat, vocab_vec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):        \n",
    "    return scipy.spatial.distance.cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbors(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], distfunc(w, mat[i])) for i in range(len(mat))]\n",
    "    return sorted(dists, key=itemgetter(1), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def pmi(mat, rownames=None, positive=True):  \n",
    "    # Joint probability table:\n",
    "    p = mat / np.sum(mat, axis=None)\n",
    "    # Pre-compute column sums:\n",
    "    colprobs = np.sum(p, axis=0)\n",
    "    # Vectorize this function so that it can be applied rowwise:\n",
    "    np_pmi_log = np.vectorize((lambda x : _pmi_log(x, positive=positive)))\n",
    "    p = np.array([np_pmi_log(row / (np.sum(row)*colprobs)) for row in p])   \n",
    "    return (p, rownames)\n",
    "\n",
    "def _pmi_log(x, positive=True):\n",
    "    val = 0.0\n",
    "    if x > 0.0:\n",
    "        val = np.log(x)\n",
    "    if positive:\n",
    "        val = max([val,0.0])\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlateds(word, mat, rownames, distfunc=cosine):\n",
    "    if word not in rownames:\n",
    "        raise ValueError('%s is not in this VSM' % word)\n",
    "    w = mat[rownames.index(word)]\n",
    "    dists = [(rownames[i], w[i]) for i in range(len(mat))]\n",
    "    #print dists\n",
    "    sorted_dists = sorted(dists, key=itemgetter(1), reverse=True)\n",
    "    # print sorted_dists\n",
    "    return sorted_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correlation list returns an ordered list of (word, correlation_score) tuples, where higher correlation_score\n",
    "# means the word is more correlated. The correlation list includes all words in the vocabulary, so you can\n",
    "# selectively take the first n elements if you want to use them.\n",
    "def correlationList(mat_ppmi):\n",
    "    return correlateds(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\n",
      "The Project Gutenberg EBook of The Adventures of Tom Sawyer, Complete by\r\n",
      "\n",
      "Mark Twain (Samuel Clemens)\r\n",
      "\n",
      "\r\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost\r\n",
      "\n",
      "no restrictions whatsoever. You may copy it, give it away or re-use\r\n",
      "\n",
      "it under the terms of the Project Gutenberg License included with this\r\n",
      "\n",
      "eBook or online at www.gutenberg.net\r\n",
      "\n",
      "\r\n",
      "\n",
      "Title: The Adventures of Tom Sawyer, Complete\r\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('colored', 9.4169795839905763),\n",
       " ('frontispiece--a', 9.4169795839905763),\n",
       " ('boy,', 6.2389257536426292),\n",
       " ('small', 5.8616315225011615),\n",
       " ('and', 1.3983541189448454)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mat[0] refers to the matrix, mat[1] is a vector of rownames. To get the vector which corresponds to a given word,\n",
    "# call mat[0][mat[1].index('my_word')]\n",
    "mat = createMatrix()\n",
    "mat_ppmi = pmi(mat=mat[0], rownames=mat[1], positive=True)\n",
    "\n",
    "# The output of this shows the format of the correlation list. You \n",
    "correlationList(mat_ppmi)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('colored', 1.1102230246251565e-16), ('frontispiece--a', 0.26683418138331849), ('boy,', 0.72281068148519168), ('scoldings', 0.7872725066119548), (\"sister's\", 0.80244142688351794), ('newcomer', 0.80692312982705072), ('reward--in', 0.80692312982705072), (\"signpainter's\", 0.81145390861254185), ('small', 0.8140836541431572), ('sacks', 0.81565158584186703), ('comforts', 0.81817882660553809), ('catfish--provisions', 0.82358249463107236), ('willie', 0.82874528710588957), ('\\xa0i', 0.82874528710588957), ('hole,', 0.83175120585574069), ('\"branch\"', 0.83229509399182877), ('staff.', 0.83304774972930862), ('human', 0.83419264599984588), ('friendless', 0.8353984098379047), ('skiff', 0.8416621778249993), ('chamber,', 0.8419171116430535), ('watcher', 0.84761191403940617), ('cavern', 0.85188353729408162), ('recess', 0.85808672433960165), ('bluff', 0.86197070485873517), ('tick,', 0.8657556134858555), ('model', 0.86992459670831856), ('strain', 0.87014618555865109), ('\"my', 0.88014593355008408), ('trust', 0.88730174622963431), ('log', 0.89249740244666309), ('sandy', 0.89366004329832505), ('quiet', 0.89872062743413439), ('raft', 0.9031659277321743), ('placed', 0.90509117570330799), ('comfort', 0.90588927443037015), ('trifle', 0.9103885993229125), ('presently,', 0.91295254187295849), ('body', 0.92099690109094612), ('church', 0.93082917349414673), ('black', 0.9322952196033083), ('donations', 0.93532630185227128), ('saw', 0.93982830784894666), ('hand', 0.94533410652098493), ('too.', 0.94929020414134857), ('blue', 0.94934022748329583), ('told', 0.95016968506152488), ('boys,', 0.95100585859129694), ('poor', 0.95351470880740152), (\"can't\", 0.95556470714892283)]\n",
      "['colored', 'frontispiece--a', 'boy,', 'scoldings', \"sister's\", 'newcomer', 'reward--in', \"signpainter's\", 'small', 'sacks', 'comforts', 'catfish--provisions', 'willie', '\\xa0i', 'hole,', '\"branch\"', 'staff.', 'human', 'friendless', 'skiff', 'chamber,', 'watcher', 'cavern', 'recess', 'bluff', 'tick,', 'model', 'strain', '\"my', 'trust', 'log', 'sandy', 'quiet', 'raft', 'placed', 'comfort', 'trifle', 'presently,', 'body', 'church', 'black', 'donations', 'saw', 'hand', 'too.', 'blue', 'told', 'boys,', 'poor', \"can't\"]\n"
     ]
    }
   ],
   "source": [
    "neighbors_list = neighbors(word='colored', mat=mat_ppmi[0], rownames=mat_ppmi[1], distfunc=cosine)[: 50]\n",
    "print neighbors_list\n",
    "\n",
    "def retrieve_words(tuple_list):\n",
    "    words = list()\n",
    "    for _tuple in tuple_list:\n",
    "        words.append(_tuple[0])\n",
    "    return words\n",
    "\n",
    "neighbors_word_list = retrieve_words(neighbors_list)\n",
    "print neighbors_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "Takes in a list V of words and returns the average sentiment score across all terms in V as determined by freebase. Note to Jason: consider other sentiment databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "n/a\n",
      "n/a\n",
      "-0.125\n",
      "n/a\n",
      "0.0\n",
      "n/a\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "n/a\n",
      "n/a\n",
      "n/a\n",
      "n/a\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "n/a\n",
      "0.25\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.5\n",
      "0.0\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "-0.375\n",
      "0.0\n",
      "0.0\n",
      "0.5\n",
      "n/a\n",
      "0.0\n",
      "0.0\n",
      "n/a\n",
      "0.0\n",
      "n/a\n",
      "-0.005\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def getSentiment(word):\n",
    "    synset = list(swn.senti_synsets(word))\n",
    "    if len(synset) > 0: #if a synset exists for this word\n",
    "        synset = synset[0]\n",
    "        return(synset.pos_score(), synset.neg_score(), synset.obj_score())\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "V = ['good', 'bad', 'great', 'awesome', 'amazing', 'holy', 'beautiful', 'worrisome', 'stupid']\n",
    "def generate_sentiment(wordList):\n",
    "    totalSentiment = 0.0;\n",
    "    for word in wordList:\n",
    "        if is_ascii(word): #see note below for rationale\n",
    "            sentiment = getSentiment(word)\n",
    "            if sentiment == None:\n",
    "                sentiment = 0.0\n",
    "            if type(sentiment) is float: #why does this happen\n",
    "                print \"n/a\"\n",
    "            else:  \n",
    "                totalSentiment += (sentiment[0] - sentiment[1]) \n",
    "                print (sentiment[0] - sentiment[1])\n",
    "        #sentiwordnet generates tuples of pos, neg, and neu. currently naively choosing to consider only sum of pos and neg. \n",
    "    averageSentiment = totalSentiment/len(wordList)\n",
    "    return averageSentiment\n",
    "\n",
    "print generate_sentiment(neighbors_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-3e4f1338aa92>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-3e4f1338aa92>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent of\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"你好\".encode('utf-8')\n",
    "encode converts a unicode object to a string object. But here you have invoked it on a string object (because you don't have the u). So python has to convert the string to a unicode object first. So it does the equivalent of\n",
    "\n",
    "\"你好\".decode().encode('utf-8')\n",
    "But the decode fails because the string isn't valid ascii. That's why you get a complaint about not being able to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR/AND\n",
    "Takes in a dict of corpus:list of words and returns a dict of corpus:XOR words and dict of corpus:AND words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR\n",
      "set(['body', 'recess', 'human', 'too.', '\\xa0i', 'boy,', 'watcher', 'strain', 'frontispiece--a', 'cavern', 'boys,', 'church', \"sister's\", '\"my', 'saw', 'sandy', 'trifle', 'friendless', 'trust', 'log', 'comfort', u'beer', 'hole,', 'scoldings', 'raft', 'willie', 'told', 'poor', 'blue', 'presently,', 'catfish--provisions', 'chamber,', 'tick,', 'hand', 'newcomer', 'bluff', 'reward--in', 'colored', 'staff.', 'sacks', 'quiet', \"signpainter's\", 'placed', 'skiff', 'donations', u'block', 'small', 'model', '\"branch\"', \"can't\", 'comforts'])\n",
      "AND\n",
      "set([u'black'])\n"
     ]
    }
   ],
   "source": [
    "toyList = ['black', 'block', 'beer']\n",
    "\n",
    "def XOR(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first ^ second\n",
    "def AND(corpus1, corpus2):\n",
    "    first = set(corpus1)\n",
    "    second = set(corpus2)\n",
    "    return first & second\n",
    "\n",
    "print 'XOR'\n",
    "print XOR(toyList, neighbors_word_list)\n",
    "print 'AND'\n",
    "print AND(toyList, neighbors_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud\n",
    "Takes in a matrix M and correlation list L. Using t-sne, produces a word cloud which represents correlation between all terms. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matrix: array, shape (n_samples, n_features) or (n_samples, n_samples)\n",
    "If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold='nan')\n",
    "\n",
    "def word_cloud_preprocessing(words, matrix=mat_ppmi):\n",
    "    output = []\n",
    "    for word in words:\n",
    "        ind = matrix[1].index(word)\n",
    "        output.append(matrix[0][ind])\n",
    "    return output\n",
    "processed_mat = word_cloud_preprocessing(neighbors_word_list)\n",
    "print processed_mat\n",
    "\n",
    "def word_cloud(corr_list): #i think its processed_mat / didn't tsne take in a vector of labels as well?\n",
    "    model = TSNE(n_components=2, random_state=0)\n",
    "    tsne_matrix = model.fit_transform(corr_list)\n",
    "    \n",
    "word_cloud(processed_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
